{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcaevIiW3ZHJ"
      },
      "source": [
        "#Loading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCP8K9O_5VDn",
        "outputId": "b6f833d8-c2e6-44b9-cf48-72be5868bdf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "2lh9Q5vm4DaH",
        "outputId": "517ed71d-7b54-45f5-9e20-eb79ef111a60"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>waistline</th>\n",
              "      <th>sight_left</th>\n",
              "      <th>sight_right</th>\n",
              "      <th>hear_left</th>\n",
              "      <th>hear_right</th>\n",
              "      <th>SBP</th>\n",
              "      <th>...</th>\n",
              "      <th>LDL_chole</th>\n",
              "      <th>triglyceride</th>\n",
              "      <th>hemoglobin</th>\n",
              "      <th>urine_protein</th>\n",
              "      <th>serum_creatinine</th>\n",
              "      <th>SGOT_AST</th>\n",
              "      <th>SGOT_ALT</th>\n",
              "      <th>gamma_GTP</th>\n",
              "      <th>SMK_stat_type_cd</th>\n",
              "      <th>DRK_YN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Male</td>\n",
              "      <td>35</td>\n",
              "      <td>170</td>\n",
              "      <td>75</td>\n",
              "      <td>90.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>...</td>\n",
              "      <td>126.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>17.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Male</td>\n",
              "      <td>30</td>\n",
              "      <td>180</td>\n",
              "      <td>80</td>\n",
              "      <td>89.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>...</td>\n",
              "      <td>148.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>20.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Male</td>\n",
              "      <td>40</td>\n",
              "      <td>165</td>\n",
              "      <td>75</td>\n",
              "      <td>91.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>...</td>\n",
              "      <td>74.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>47.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Male</td>\n",
              "      <td>50</td>\n",
              "      <td>175</td>\n",
              "      <td>80</td>\n",
              "      <td>91.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>...</td>\n",
              "      <td>104.0</td>\n",
              "      <td>106.0</td>\n",
              "      <td>17.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>29.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Male</td>\n",
              "      <td>50</td>\n",
              "      <td>165</td>\n",
              "      <td>60</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>...</td>\n",
              "      <td>117.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>13.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>19.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991341</th>\n",
              "      <td>Male</td>\n",
              "      <td>45</td>\n",
              "      <td>175</td>\n",
              "      <td>80</td>\n",
              "      <td>92.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>...</td>\n",
              "      <td>125.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991342</th>\n",
              "      <td>Male</td>\n",
              "      <td>35</td>\n",
              "      <td>170</td>\n",
              "      <td>75</td>\n",
              "      <td>86.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>...</td>\n",
              "      <td>84.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>14.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991343</th>\n",
              "      <td>Female</td>\n",
              "      <td>40</td>\n",
              "      <td>155</td>\n",
              "      <td>50</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>...</td>\n",
              "      <td>77.0</td>\n",
              "      <td>157.0</td>\n",
              "      <td>14.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>30.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991344</th>\n",
              "      <td>Male</td>\n",
              "      <td>25</td>\n",
              "      <td>175</td>\n",
              "      <td>60</td>\n",
              "      <td>72.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>...</td>\n",
              "      <td>73.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>14.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>21.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991345</th>\n",
              "      <td>Male</td>\n",
              "      <td>50</td>\n",
              "      <td>160</td>\n",
              "      <td>70</td>\n",
              "      <td>90.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>...</td>\n",
              "      <td>153.0</td>\n",
              "      <td>163.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>24.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Y</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>991346 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           sex  age  height  weight  waistline  sight_left  sight_right  \\\n",
              "0         Male   35     170      75       90.0         1.0          1.0   \n",
              "1         Male   30     180      80       89.0         0.9          1.2   \n",
              "2         Male   40     165      75       91.0         1.2          1.5   \n",
              "3         Male   50     175      80       91.0         1.5          1.2   \n",
              "4         Male   50     165      60       80.0         1.0          1.2   \n",
              "...        ...  ...     ...     ...        ...         ...          ...   \n",
              "991341    Male   45     175      80       92.1         1.5          1.5   \n",
              "991342    Male   35     170      75       86.0         1.0          1.5   \n",
              "991343  Female   40     155      50       68.0         1.0          0.7   \n",
              "991344    Male   25     175      60       72.0         1.5          1.0   \n",
              "991345    Male   50     160      70       90.5         1.0          1.5   \n",
              "\n",
              "        hear_left  hear_right    SBP  ...  LDL_chole  triglyceride  \\\n",
              "0             1.0         1.0  120.0  ...      126.0          92.0   \n",
              "1             1.0         1.0  130.0  ...      148.0         121.0   \n",
              "2             1.0         1.0  120.0  ...       74.0         104.0   \n",
              "3             1.0         1.0  145.0  ...      104.0         106.0   \n",
              "4             1.0         1.0  138.0  ...      117.0         104.0   \n",
              "...           ...         ...    ...  ...        ...           ...   \n",
              "991341        1.0         1.0  114.0  ...      125.0         132.0   \n",
              "991342        1.0         1.0  119.0  ...       84.0          45.0   \n",
              "991343        1.0         1.0  110.0  ...       77.0         157.0   \n",
              "991344        1.0         1.0  119.0  ...       73.0          53.0   \n",
              "991345        1.0         1.0  133.0  ...      153.0         163.0   \n",
              "\n",
              "        hemoglobin  urine_protein  serum_creatinine  SGOT_AST  SGOT_ALT  \\\n",
              "0             17.1            1.0               1.0      21.0      35.0   \n",
              "1             15.8            1.0               0.9      20.0      36.0   \n",
              "2             15.8            1.0               0.9      47.0      32.0   \n",
              "3             17.6            1.0               1.1      29.0      34.0   \n",
              "4             13.8            1.0               0.8      19.0      12.0   \n",
              "...            ...            ...               ...       ...       ...   \n",
              "991341        15.0            1.0               1.0      26.0      36.0   \n",
              "991342        15.8            1.0               1.1      14.0      17.0   \n",
              "991343        14.3            1.0               0.8      30.0      27.0   \n",
              "991344        14.5            1.0               0.8      21.0      14.0   \n",
              "991345        15.8            1.0               0.9      24.0      43.0   \n",
              "\n",
              "        gamma_GTP  SMK_stat_type_cd  DRK_YN  \n",
              "0            40.0               1.0       Y  \n",
              "1            27.0               3.0       N  \n",
              "2            68.0               1.0       N  \n",
              "3            18.0               1.0       N  \n",
              "4            25.0               1.0       N  \n",
              "...           ...               ...     ...  \n",
              "991341       27.0               1.0       N  \n",
              "991342       15.0               1.0       N  \n",
              "991343       17.0               3.0       Y  \n",
              "991344       17.0               1.0       N  \n",
              "991345       36.0               3.0       Y  \n",
              "\n",
              "[991346 rows x 24 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "# df = pd.read_csv(\"/content/drive/MyDrive/DMT-CS235-Project/smoking_driking_dataset.csv\")\n",
        "df = pd.read_csv('smoking_driking_dataset.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "Dg_TbUXazOHP",
        "outputId": "7d6c118b-b0bb-41df-90a0-6bf882415714"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>waistline</th>\n",
              "      <th>sight_left</th>\n",
              "      <th>sight_right</th>\n",
              "      <th>hear_left</th>\n",
              "      <th>hear_right</th>\n",
              "      <th>SBP</th>\n",
              "      <th>DBP</th>\n",
              "      <th>...</th>\n",
              "      <th>HDL_chole</th>\n",
              "      <th>LDL_chole</th>\n",
              "      <th>triglyceride</th>\n",
              "      <th>hemoglobin</th>\n",
              "      <th>urine_protein</th>\n",
              "      <th>serum_creatinine</th>\n",
              "      <th>SGOT_AST</th>\n",
              "      <th>SGOT_ALT</th>\n",
              "      <th>gamma_GTP</th>\n",
              "      <th>SMK_stat_type_cd</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>47.614491</td>\n",
              "      <td>162.240625</td>\n",
              "      <td>63.284050</td>\n",
              "      <td>81.233358</td>\n",
              "      <td>0.980834</td>\n",
              "      <td>0.978429</td>\n",
              "      <td>1.031495</td>\n",
              "      <td>1.030476</td>\n",
              "      <td>122.432498</td>\n",
              "      <td>76.052627</td>\n",
              "      <td>...</td>\n",
              "      <td>56.936800</td>\n",
              "      <td>113.037692</td>\n",
              "      <td>132.141751</td>\n",
              "      <td>14.229824</td>\n",
              "      <td>1.094224</td>\n",
              "      <td>0.860467</td>\n",
              "      <td>25.989308</td>\n",
              "      <td>25.755051</td>\n",
              "      <td>37.136347</td>\n",
              "      <td>1.608122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>14.181339</td>\n",
              "      <td>9.282957</td>\n",
              "      <td>12.514241</td>\n",
              "      <td>11.850323</td>\n",
              "      <td>0.605949</td>\n",
              "      <td>0.604774</td>\n",
              "      <td>0.174650</td>\n",
              "      <td>0.171892</td>\n",
              "      <td>14.543148</td>\n",
              "      <td>9.889365</td>\n",
              "      <td>...</td>\n",
              "      <td>17.238479</td>\n",
              "      <td>35.842812</td>\n",
              "      <td>102.196985</td>\n",
              "      <td>1.584929</td>\n",
              "      <td>0.437724</td>\n",
              "      <td>0.480530</td>\n",
              "      <td>23.493386</td>\n",
              "      <td>26.308599</td>\n",
              "      <td>50.424153</td>\n",
              "      <td>0.818507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>20.000000</td>\n",
              "      <td>130.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>35.000000</td>\n",
              "      <td>155.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>74.100000</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>70.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>46.000000</td>\n",
              "      <td>89.000000</td>\n",
              "      <td>73.000000</td>\n",
              "      <td>13.200000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>45.000000</td>\n",
              "      <td>160.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>81.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>120.000000</td>\n",
              "      <td>76.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>111.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>14.300000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>60.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>70.000000</td>\n",
              "      <td>87.800000</td>\n",
              "      <td>1.200000</td>\n",
              "      <td>1.200000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>131.000000</td>\n",
              "      <td>82.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>66.000000</td>\n",
              "      <td>135.000000</td>\n",
              "      <td>159.000000</td>\n",
              "      <td>15.400000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>85.000000</td>\n",
              "      <td>190.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>9.900000</td>\n",
              "      <td>9.900000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>273.000000</td>\n",
              "      <td>185.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>8110.000000</td>\n",
              "      <td>5119.000000</td>\n",
              "      <td>9490.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>98.000000</td>\n",
              "      <td>9999.000000</td>\n",
              "      <td>7210.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 22 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 age         height         weight      waistline  \\\n",
              "count  991346.000000  991346.000000  991346.000000  991346.000000   \n",
              "mean       47.614491     162.240625      63.284050      81.233358   \n",
              "std        14.181339       9.282957      12.514241      11.850323   \n",
              "min        20.000000     130.000000      25.000000       8.000000   \n",
              "25%        35.000000     155.000000      55.000000      74.100000   \n",
              "50%        45.000000     160.000000      60.000000      81.000000   \n",
              "75%        60.000000     170.000000      70.000000      87.800000   \n",
              "max        85.000000     190.000000     140.000000     999.000000   \n",
              "\n",
              "          sight_left    sight_right      hear_left     hear_right  \\\n",
              "count  991346.000000  991346.000000  991346.000000  991346.000000   \n",
              "mean        0.980834       0.978429       1.031495       1.030476   \n",
              "std         0.605949       0.604774       0.174650       0.171892   \n",
              "min         0.100000       0.100000       1.000000       1.000000   \n",
              "25%         0.700000       0.700000       1.000000       1.000000   \n",
              "50%         1.000000       1.000000       1.000000       1.000000   \n",
              "75%         1.200000       1.200000       1.000000       1.000000   \n",
              "max         9.900000       9.900000       2.000000       2.000000   \n",
              "\n",
              "                 SBP            DBP  ...      HDL_chole      LDL_chole  \\\n",
              "count  991346.000000  991346.000000  ...  991346.000000  991346.000000   \n",
              "mean      122.432498      76.052627  ...      56.936800     113.037692   \n",
              "std        14.543148       9.889365  ...      17.238479      35.842812   \n",
              "min        67.000000      32.000000  ...       1.000000       1.000000   \n",
              "25%       112.000000      70.000000  ...      46.000000      89.000000   \n",
              "50%       120.000000      76.000000  ...      55.000000     111.000000   \n",
              "75%       131.000000      82.000000  ...      66.000000     135.000000   \n",
              "max       273.000000     185.000000  ...    8110.000000    5119.000000   \n",
              "\n",
              "        triglyceride     hemoglobin  urine_protein  serum_creatinine  \\\n",
              "count  991346.000000  991346.000000  991346.000000     991346.000000   \n",
              "mean      132.141751      14.229824       1.094224          0.860467   \n",
              "std       102.196985       1.584929       0.437724          0.480530   \n",
              "min         1.000000       1.000000       1.000000          0.100000   \n",
              "25%        73.000000      13.200000       1.000000          0.700000   \n",
              "50%       106.000000      14.300000       1.000000          0.800000   \n",
              "75%       159.000000      15.400000       1.000000          1.000000   \n",
              "max      9490.000000      25.000000       6.000000         98.000000   \n",
              "\n",
              "            SGOT_AST       SGOT_ALT      gamma_GTP  SMK_stat_type_cd  \n",
              "count  991346.000000  991346.000000  991346.000000     991346.000000  \n",
              "mean       25.989308      25.755051      37.136347          1.608122  \n",
              "std        23.493386      26.308599      50.424153          0.818507  \n",
              "min         1.000000       1.000000       1.000000          1.000000  \n",
              "25%        19.000000      15.000000      16.000000          1.000000  \n",
              "50%        23.000000      20.000000      23.000000          1.000000  \n",
              "75%        28.000000      29.000000      39.000000          2.000000  \n",
              "max      9999.000000    7210.000000     999.000000          3.000000  \n",
              "\n",
              "[8 rows x 22 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "s46MP_Hs5by-"
      },
      "outputs": [],
      "source": [
        "columns = df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg9gcaPSlTcs"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szQT3HIelXJD",
        "outputId": "c57372dc-e6a0-4253-ac48-c52515bb87d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "sex                 0\n",
              "age                 0\n",
              "height              0\n",
              "weight              0\n",
              "waistline           0\n",
              "sight_left          0\n",
              "sight_right         0\n",
              "hear_left           0\n",
              "hear_right          0\n",
              "SBP                 0\n",
              "DBP                 0\n",
              "BLDS                0\n",
              "tot_chole           0\n",
              "HDL_chole           0\n",
              "LDL_chole           0\n",
              "triglyceride        0\n",
              "hemoglobin          0\n",
              "urine_protein       0\n",
              "serum_creatinine    0\n",
              "SGOT_AST            0\n",
              "SGOT_ALT            0\n",
              "gamma_GTP           0\n",
              "SMK_stat_type_cd    0\n",
              "DRK_YN              0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIOT_jDVlYPT",
        "outputId": "19b051ac-aec1-46ff-8ff4-e3a6d57ec243"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "p3Ot7rYOlaJk",
        "outputId": "d387e89f-74ac-4acc-efa6-270da2548572"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>waistline</th>\n",
              "      <th>sight_left</th>\n",
              "      <th>sight_right</th>\n",
              "      <th>hear_left</th>\n",
              "      <th>hear_right</th>\n",
              "      <th>SBP</th>\n",
              "      <th>...</th>\n",
              "      <th>LDL_chole</th>\n",
              "      <th>triglyceride</th>\n",
              "      <th>hemoglobin</th>\n",
              "      <th>urine_protein</th>\n",
              "      <th>serum_creatinine</th>\n",
              "      <th>SGOT_AST</th>\n",
              "      <th>SGOT_ALT</th>\n",
              "      <th>gamma_GTP</th>\n",
              "      <th>SMK_stat_type_cd</th>\n",
              "      <th>DRK_YN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Male</td>\n",
              "      <td>35</td>\n",
              "      <td>170</td>\n",
              "      <td>75</td>\n",
              "      <td>90.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>...</td>\n",
              "      <td>126.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>17.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Male</td>\n",
              "      <td>30</td>\n",
              "      <td>180</td>\n",
              "      <td>80</td>\n",
              "      <td>89.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>...</td>\n",
              "      <td>148.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>20.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Male</td>\n",
              "      <td>40</td>\n",
              "      <td>165</td>\n",
              "      <td>75</td>\n",
              "      <td>91.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>...</td>\n",
              "      <td>74.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>47.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Male</td>\n",
              "      <td>50</td>\n",
              "      <td>175</td>\n",
              "      <td>80</td>\n",
              "      <td>91.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>...</td>\n",
              "      <td>104.0</td>\n",
              "      <td>106.0</td>\n",
              "      <td>17.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>29.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Male</td>\n",
              "      <td>50</td>\n",
              "      <td>165</td>\n",
              "      <td>60</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>...</td>\n",
              "      <td>117.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>13.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>19.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991341</th>\n",
              "      <td>Male</td>\n",
              "      <td>45</td>\n",
              "      <td>175</td>\n",
              "      <td>80</td>\n",
              "      <td>92.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>...</td>\n",
              "      <td>125.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991342</th>\n",
              "      <td>Male</td>\n",
              "      <td>35</td>\n",
              "      <td>170</td>\n",
              "      <td>75</td>\n",
              "      <td>86.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>...</td>\n",
              "      <td>84.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>14.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991343</th>\n",
              "      <td>Female</td>\n",
              "      <td>40</td>\n",
              "      <td>155</td>\n",
              "      <td>50</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>...</td>\n",
              "      <td>77.0</td>\n",
              "      <td>157.0</td>\n",
              "      <td>14.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>30.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991344</th>\n",
              "      <td>Male</td>\n",
              "      <td>25</td>\n",
              "      <td>175</td>\n",
              "      <td>60</td>\n",
              "      <td>72.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>...</td>\n",
              "      <td>73.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>14.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>21.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991345</th>\n",
              "      <td>Male</td>\n",
              "      <td>50</td>\n",
              "      <td>160</td>\n",
              "      <td>70</td>\n",
              "      <td>90.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>...</td>\n",
              "      <td>153.0</td>\n",
              "      <td>163.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>24.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Y</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>991320 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           sex  age  height  weight  waistline  sight_left  sight_right  \\\n",
              "0         Male   35     170      75       90.0         1.0          1.0   \n",
              "1         Male   30     180      80       89.0         0.9          1.2   \n",
              "2         Male   40     165      75       91.0         1.2          1.5   \n",
              "3         Male   50     175      80       91.0         1.5          1.2   \n",
              "4         Male   50     165      60       80.0         1.0          1.2   \n",
              "...        ...  ...     ...     ...        ...         ...          ...   \n",
              "991341    Male   45     175      80       92.1         1.5          1.5   \n",
              "991342    Male   35     170      75       86.0         1.0          1.5   \n",
              "991343  Female   40     155      50       68.0         1.0          0.7   \n",
              "991344    Male   25     175      60       72.0         1.5          1.0   \n",
              "991345    Male   50     160      70       90.5         1.0          1.5   \n",
              "\n",
              "        hear_left  hear_right    SBP  ...  LDL_chole  triglyceride  \\\n",
              "0             1.0         1.0  120.0  ...      126.0          92.0   \n",
              "1             1.0         1.0  130.0  ...      148.0         121.0   \n",
              "2             1.0         1.0  120.0  ...       74.0         104.0   \n",
              "3             1.0         1.0  145.0  ...      104.0         106.0   \n",
              "4             1.0         1.0  138.0  ...      117.0         104.0   \n",
              "...           ...         ...    ...  ...        ...           ...   \n",
              "991341        1.0         1.0  114.0  ...      125.0         132.0   \n",
              "991342        1.0         1.0  119.0  ...       84.0          45.0   \n",
              "991343        1.0         1.0  110.0  ...       77.0         157.0   \n",
              "991344        1.0         1.0  119.0  ...       73.0          53.0   \n",
              "991345        1.0         1.0  133.0  ...      153.0         163.0   \n",
              "\n",
              "        hemoglobin  urine_protein  serum_creatinine  SGOT_AST  SGOT_ALT  \\\n",
              "0             17.1            1.0               1.0      21.0      35.0   \n",
              "1             15.8            1.0               0.9      20.0      36.0   \n",
              "2             15.8            1.0               0.9      47.0      32.0   \n",
              "3             17.6            1.0               1.1      29.0      34.0   \n",
              "4             13.8            1.0               0.8      19.0      12.0   \n",
              "...            ...            ...               ...       ...       ...   \n",
              "991341        15.0            1.0               1.0      26.0      36.0   \n",
              "991342        15.8            1.0               1.1      14.0      17.0   \n",
              "991343        14.3            1.0               0.8      30.0      27.0   \n",
              "991344        14.5            1.0               0.8      21.0      14.0   \n",
              "991345        15.8            1.0               0.9      24.0      43.0   \n",
              "\n",
              "        gamma_GTP  SMK_stat_type_cd  DRK_YN  \n",
              "0            40.0               1.0       Y  \n",
              "1            27.0               3.0       N  \n",
              "2            68.0               1.0       N  \n",
              "3            18.0               1.0       N  \n",
              "4            25.0               1.0       N  \n",
              "...           ...               ...     ...  \n",
              "991341       27.0               1.0       N  \n",
              "991342       15.0               1.0       N  \n",
              "991343       17.0               3.0       Y  \n",
              "991344       17.0               1.0       N  \n",
              "991345       36.0               3.0       Y  \n",
              "\n",
              "[991320 rows x 24 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "2MQlf5lzTN1f",
        "outputId": "175be87c-8259-4e7c-9f7c-2d262fe63509"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>waistline</th>\n",
              "      <th>sight_left</th>\n",
              "      <th>sight_right</th>\n",
              "      <th>hear_left</th>\n",
              "      <th>hear_right</th>\n",
              "      <th>SBP</th>\n",
              "      <th>...</th>\n",
              "      <th>LDL_chole</th>\n",
              "      <th>triglyceride</th>\n",
              "      <th>hemoglobin</th>\n",
              "      <th>urine_protein</th>\n",
              "      <th>serum_creatinine</th>\n",
              "      <th>SGOT_AST</th>\n",
              "      <th>SGOT_ALT</th>\n",
              "      <th>gamma_GTP</th>\n",
              "      <th>SMK_stat_type_cd</th>\n",
              "      <th>DRK_YN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>35</td>\n",
              "      <td>170</td>\n",
              "      <td>75</td>\n",
              "      <td>90.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>...</td>\n",
              "      <td>126.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>17.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>30</td>\n",
              "      <td>180</td>\n",
              "      <td>80</td>\n",
              "      <td>89.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>...</td>\n",
              "      <td>148.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>20.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>40</td>\n",
              "      <td>165</td>\n",
              "      <td>75</td>\n",
              "      <td>91.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>...</td>\n",
              "      <td>74.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>47.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>175</td>\n",
              "      <td>80</td>\n",
              "      <td>91.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>...</td>\n",
              "      <td>104.0</td>\n",
              "      <td>106.0</td>\n",
              "      <td>17.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>29.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>165</td>\n",
              "      <td>60</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>...</td>\n",
              "      <td>117.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>13.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>19.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991341</th>\n",
              "      <td>1</td>\n",
              "      <td>45</td>\n",
              "      <td>175</td>\n",
              "      <td>80</td>\n",
              "      <td>92.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>...</td>\n",
              "      <td>125.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991342</th>\n",
              "      <td>1</td>\n",
              "      <td>35</td>\n",
              "      <td>170</td>\n",
              "      <td>75</td>\n",
              "      <td>86.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>...</td>\n",
              "      <td>84.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>14.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991343</th>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>155</td>\n",
              "      <td>50</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>...</td>\n",
              "      <td>77.0</td>\n",
              "      <td>157.0</td>\n",
              "      <td>14.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>30.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991344</th>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>175</td>\n",
              "      <td>60</td>\n",
              "      <td>72.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>...</td>\n",
              "      <td>73.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>14.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>21.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991345</th>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>160</td>\n",
              "      <td>70</td>\n",
              "      <td>90.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>...</td>\n",
              "      <td>153.0</td>\n",
              "      <td>163.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>24.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>991346 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        sex  age  height  weight  waistline  sight_left  sight_right  \\\n",
              "0         1   35     170      75       90.0         1.0          1.0   \n",
              "1         1   30     180      80       89.0         0.9          1.2   \n",
              "2         1   40     165      75       91.0         1.2          1.5   \n",
              "3         1   50     175      80       91.0         1.5          1.2   \n",
              "4         1   50     165      60       80.0         1.0          1.2   \n",
              "...     ...  ...     ...     ...        ...         ...          ...   \n",
              "991341    1   45     175      80       92.1         1.5          1.5   \n",
              "991342    1   35     170      75       86.0         1.0          1.5   \n",
              "991343    0   40     155      50       68.0         1.0          0.7   \n",
              "991344    1   25     175      60       72.0         1.5          1.0   \n",
              "991345    1   50     160      70       90.5         1.0          1.5   \n",
              "\n",
              "        hear_left  hear_right    SBP  ...  LDL_chole  triglyceride  \\\n",
              "0             1.0         1.0  120.0  ...      126.0          92.0   \n",
              "1             1.0         1.0  130.0  ...      148.0         121.0   \n",
              "2             1.0         1.0  120.0  ...       74.0         104.0   \n",
              "3             1.0         1.0  145.0  ...      104.0         106.0   \n",
              "4             1.0         1.0  138.0  ...      117.0         104.0   \n",
              "...           ...         ...    ...  ...        ...           ...   \n",
              "991341        1.0         1.0  114.0  ...      125.0         132.0   \n",
              "991342        1.0         1.0  119.0  ...       84.0          45.0   \n",
              "991343        1.0         1.0  110.0  ...       77.0         157.0   \n",
              "991344        1.0         1.0  119.0  ...       73.0          53.0   \n",
              "991345        1.0         1.0  133.0  ...      153.0         163.0   \n",
              "\n",
              "        hemoglobin  urine_protein  serum_creatinine  SGOT_AST  SGOT_ALT  \\\n",
              "0             17.1            1.0               1.0      21.0      35.0   \n",
              "1             15.8            1.0               0.9      20.0      36.0   \n",
              "2             15.8            1.0               0.9      47.0      32.0   \n",
              "3             17.6            1.0               1.1      29.0      34.0   \n",
              "4             13.8            1.0               0.8      19.0      12.0   \n",
              "...            ...            ...               ...       ...       ...   \n",
              "991341        15.0            1.0               1.0      26.0      36.0   \n",
              "991342        15.8            1.0               1.1      14.0      17.0   \n",
              "991343        14.3            1.0               0.8      30.0      27.0   \n",
              "991344        14.5            1.0               0.8      21.0      14.0   \n",
              "991345        15.8            1.0               0.9      24.0      43.0   \n",
              "\n",
              "        gamma_GTP  SMK_stat_type_cd  DRK_YN  \n",
              "0            40.0               1.0       1  \n",
              "1            27.0               3.0       0  \n",
              "2            68.0               1.0       0  \n",
              "3            18.0               1.0       0  \n",
              "4            25.0               1.0       0  \n",
              "...           ...               ...     ...  \n",
              "991341       27.0               1.0       0  \n",
              "991342       15.0               1.0       0  \n",
              "991343       17.0               3.0       1  \n",
              "991344       17.0               1.0       0  \n",
              "991345       36.0               3.0       1  \n",
              "\n",
              "[991346 rows x 24 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "le = LabelEncoder()\n",
        "df[\"sex\"] = le.fit_transform(df[\"sex\"])\n",
        "df[\"DRK_YN\"] = le.fit_transform(df[\"DRK_YN\"])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "v_s--iVCSj9L",
        "outputId": "c50d4392-fc98-4591-80f2-bd8500ca38dd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>waistline</th>\n",
              "      <th>sight_left</th>\n",
              "      <th>sight_right</th>\n",
              "      <th>hear_left</th>\n",
              "      <th>hear_right</th>\n",
              "      <th>SBP</th>\n",
              "      <th>...</th>\n",
              "      <th>LDL_chole</th>\n",
              "      <th>triglyceride</th>\n",
              "      <th>hemoglobin</th>\n",
              "      <th>urine_protein</th>\n",
              "      <th>serum_creatinine</th>\n",
              "      <th>SGOT_AST</th>\n",
              "      <th>SGOT_ALT</th>\n",
              "      <th>gamma_GTP</th>\n",
              "      <th>SMK_stat_type_cd</th>\n",
              "      <th>DRK_YN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>...</td>\n",
              "      <td>126.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>17.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>180.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>...</td>\n",
              "      <td>148.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>20.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>...</td>\n",
              "      <td>74.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>47.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>...</td>\n",
              "      <td>104.0</td>\n",
              "      <td>106.0</td>\n",
              "      <td>17.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>29.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>...</td>\n",
              "      <td>117.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>13.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>19.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991341</th>\n",
              "      <td>1.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>92.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>...</td>\n",
              "      <td>125.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991342</th>\n",
              "      <td>1.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>...</td>\n",
              "      <td>84.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>14.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991343</th>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>155.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>...</td>\n",
              "      <td>77.0</td>\n",
              "      <td>157.0</td>\n",
              "      <td>14.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>30.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991344</th>\n",
              "      <td>1.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>...</td>\n",
              "      <td>73.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>14.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>21.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991345</th>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>90.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>...</td>\n",
              "      <td>153.0</td>\n",
              "      <td>163.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>24.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>991346 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        sex   age  height  weight  waistline  sight_left  sight_right  \\\n",
              "0       1.0  35.0   170.0    75.0       90.0         1.0          1.0   \n",
              "1       1.0  30.0   180.0    80.0       89.0         0.9          1.2   \n",
              "2       1.0  40.0   165.0    75.0       91.0         1.2          1.5   \n",
              "3       1.0  50.0   175.0    80.0       91.0         1.5          1.2   \n",
              "4       1.0  50.0   165.0    60.0       80.0         1.0          1.2   \n",
              "...     ...   ...     ...     ...        ...         ...          ...   \n",
              "991341  1.0  45.0   175.0    80.0       92.1         1.5          1.5   \n",
              "991342  1.0  35.0   170.0    75.0       86.0         1.0          1.5   \n",
              "991343  0.0  40.0   155.0    50.0       68.0         1.0          0.7   \n",
              "991344  1.0  25.0   175.0    60.0       72.0         1.5          1.0   \n",
              "991345  1.0  50.0   160.0    70.0       90.5         1.0          1.5   \n",
              "\n",
              "        hear_left  hear_right    SBP  ...  LDL_chole  triglyceride  \\\n",
              "0             1.0         1.0  120.0  ...      126.0          92.0   \n",
              "1             1.0         1.0  130.0  ...      148.0         121.0   \n",
              "2             1.0         1.0  120.0  ...       74.0         104.0   \n",
              "3             1.0         1.0  145.0  ...      104.0         106.0   \n",
              "4             1.0         1.0  138.0  ...      117.0         104.0   \n",
              "...           ...         ...    ...  ...        ...           ...   \n",
              "991341        1.0         1.0  114.0  ...      125.0         132.0   \n",
              "991342        1.0         1.0  119.0  ...       84.0          45.0   \n",
              "991343        1.0         1.0  110.0  ...       77.0         157.0   \n",
              "991344        1.0         1.0  119.0  ...       73.0          53.0   \n",
              "991345        1.0         1.0  133.0  ...      153.0         163.0   \n",
              "\n",
              "        hemoglobin  urine_protein  serum_creatinine  SGOT_AST  SGOT_ALT  \\\n",
              "0             17.1            1.0               1.0      21.0      35.0   \n",
              "1             15.8            1.0               0.9      20.0      36.0   \n",
              "2             15.8            1.0               0.9      47.0      32.0   \n",
              "3             17.6            1.0               1.1      29.0      34.0   \n",
              "4             13.8            1.0               0.8      19.0      12.0   \n",
              "...            ...            ...               ...       ...       ...   \n",
              "991341        15.0            1.0               1.0      26.0      36.0   \n",
              "991342        15.8            1.0               1.1      14.0      17.0   \n",
              "991343        14.3            1.0               0.8      30.0      27.0   \n",
              "991344        14.5            1.0               0.8      21.0      14.0   \n",
              "991345        15.8            1.0               0.9      24.0      43.0   \n",
              "\n",
              "        gamma_GTP  SMK_stat_type_cd  DRK_YN  \n",
              "0            40.0               1.0     1.0  \n",
              "1            27.0               3.0     0.0  \n",
              "2            68.0               1.0     0.0  \n",
              "3            18.0               1.0     0.0  \n",
              "4            25.0               1.0     0.0  \n",
              "...           ...               ...     ...  \n",
              "991341       27.0               1.0     0.0  \n",
              "991342       15.0               1.0     0.0  \n",
              "991343       17.0               3.0     1.0  \n",
              "991344       17.0               1.0     0.0  \n",
              "991345       36.0               3.0     1.0  \n",
              "\n",
              "[991346 rows x 24 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for i in df.columns:\n",
        "  df[i] = df[i].astype('float64')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "_d_ud0SiSj9L",
        "outputId": "2fc7f469-e192-404e-cc27-6a22a721a934"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>waistline</th>\n",
              "      <th>sight_left</th>\n",
              "      <th>sight_right</th>\n",
              "      <th>hear_left</th>\n",
              "      <th>hear_right</th>\n",
              "      <th>SBP</th>\n",
              "      <th>...</th>\n",
              "      <th>LDL_chole</th>\n",
              "      <th>triglyceride</th>\n",
              "      <th>hemoglobin</th>\n",
              "      <th>urine_protein</th>\n",
              "      <th>serum_creatinine</th>\n",
              "      <th>SGOT_AST</th>\n",
              "      <th>SGOT_ALT</th>\n",
              "      <th>gamma_GTP</th>\n",
              "      <th>SMK_stat_type_cd</th>\n",
              "      <th>DRK_YN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>...</td>\n",
              "      <td>126.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>17.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>180.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>...</td>\n",
              "      <td>148.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>20.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>...</td>\n",
              "      <td>74.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>47.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>...</td>\n",
              "      <td>104.0</td>\n",
              "      <td>106.0</td>\n",
              "      <td>17.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>29.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>...</td>\n",
              "      <td>117.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>13.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>19.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991341</th>\n",
              "      <td>1.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>92.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>...</td>\n",
              "      <td>125.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991342</th>\n",
              "      <td>1.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>...</td>\n",
              "      <td>84.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>14.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991343</th>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>155.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>...</td>\n",
              "      <td>77.0</td>\n",
              "      <td>157.0</td>\n",
              "      <td>14.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>30.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991344</th>\n",
              "      <td>1.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>...</td>\n",
              "      <td>73.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>14.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>21.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991345</th>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>90.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>...</td>\n",
              "      <td>153.0</td>\n",
              "      <td>163.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>24.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>991346 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        sex   age  height  weight  waistline  sight_left  sight_right  \\\n",
              "0       1.0  35.0   170.0    75.0       90.0         1.0          1.0   \n",
              "1       1.0  30.0   180.0    80.0       89.0         0.9          1.2   \n",
              "2       1.0  40.0   165.0    75.0       91.0         1.2          1.5   \n",
              "3       1.0  50.0   175.0    80.0       91.0         1.5          1.2   \n",
              "4       1.0  50.0   165.0    60.0       80.0         1.0          1.2   \n",
              "...     ...   ...     ...     ...        ...         ...          ...   \n",
              "991341  1.0  45.0   175.0    80.0       92.1         1.5          1.5   \n",
              "991342  1.0  35.0   170.0    75.0       86.0         1.0          1.5   \n",
              "991343  0.0  40.0   155.0    50.0       68.0         1.0          0.7   \n",
              "991344  1.0  25.0   175.0    60.0       72.0         1.5          1.0   \n",
              "991345  1.0  50.0   160.0    70.0       90.5         1.0          1.5   \n",
              "\n",
              "        hear_left  hear_right    SBP  ...  LDL_chole  triglyceride  \\\n",
              "0             1.0         1.0  120.0  ...      126.0          92.0   \n",
              "1             1.0         1.0  130.0  ...      148.0         121.0   \n",
              "2             1.0         1.0  120.0  ...       74.0         104.0   \n",
              "3             1.0         1.0  145.0  ...      104.0         106.0   \n",
              "4             1.0         1.0  138.0  ...      117.0         104.0   \n",
              "...           ...         ...    ...  ...        ...           ...   \n",
              "991341        1.0         1.0  114.0  ...      125.0         132.0   \n",
              "991342        1.0         1.0  119.0  ...       84.0          45.0   \n",
              "991343        1.0         1.0  110.0  ...       77.0         157.0   \n",
              "991344        1.0         1.0  119.0  ...       73.0          53.0   \n",
              "991345        1.0         1.0  133.0  ...      153.0         163.0   \n",
              "\n",
              "        hemoglobin  urine_protein  serum_creatinine  SGOT_AST  SGOT_ALT  \\\n",
              "0             17.1            1.0               1.0      21.0      35.0   \n",
              "1             15.8            1.0               0.9      20.0      36.0   \n",
              "2             15.8            1.0               0.9      47.0      32.0   \n",
              "3             17.6            1.0               1.1      29.0      34.0   \n",
              "4             13.8            1.0               0.8      19.0      12.0   \n",
              "...            ...            ...               ...       ...       ...   \n",
              "991341        15.0            1.0               1.0      26.0      36.0   \n",
              "991342        15.8            1.0               1.1      14.0      17.0   \n",
              "991343        14.3            1.0               0.8      30.0      27.0   \n",
              "991344        14.5            1.0               0.8      21.0      14.0   \n",
              "991345        15.8            1.0               0.9      24.0      43.0   \n",
              "\n",
              "        gamma_GTP  SMK_stat_type_cd  DRK_YN  \n",
              "0            40.0               1.0     1.0  \n",
              "1            27.0               0.0     0.0  \n",
              "2            68.0               1.0     0.0  \n",
              "3            18.0               1.0     0.0  \n",
              "4            25.0               1.0     0.0  \n",
              "...           ...               ...     ...  \n",
              "991341       27.0               1.0     0.0  \n",
              "991342       15.0               1.0     0.0  \n",
              "991343       17.0               0.0     1.0  \n",
              "991344       17.0               1.0     0.0  \n",
              "991345       36.0               0.0     1.0  \n",
              "\n",
              "[991346 rows x 24 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.loc[df[\"SMK_stat_type_cd\"] == 3.0, \"SMK_stat_type_cd\"] = 0.0\n",
        "df.loc[df[\"SMK_stat_type_cd\"] == 2.0, \"SMK_stat_type_cd\"] = 0.0\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3dLsnS9fSj9M"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# for i in df.columns:\n",
        "#     plt.figure()\n",
        "#     sns.boxplot(data=df[i])\n",
        "#     plt.title(f'Box Plot of {i}')\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "h5GUm28E4-Iq",
        "outputId": "cf04627d-9b00-4a6f-f4ab-17d87e8529be"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>waistline</th>\n",
              "      <th>sight_left</th>\n",
              "      <th>sight_right</th>\n",
              "      <th>hear_left</th>\n",
              "      <th>hear_right</th>\n",
              "      <th>SBP</th>\n",
              "      <th>...</th>\n",
              "      <th>LDL_chole</th>\n",
              "      <th>triglyceride</th>\n",
              "      <th>hemoglobin</th>\n",
              "      <th>urine_protein</th>\n",
              "      <th>serum_creatinine</th>\n",
              "      <th>SGOT_AST</th>\n",
              "      <th>SGOT_ALT</th>\n",
              "      <th>gamma_GTP</th>\n",
              "      <th>SMK_stat_type_cd</th>\n",
              "      <th>DRK_YN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>1.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>9.9</td>\n",
              "      <td>9.9</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>146.0</td>\n",
              "      <td>...</td>\n",
              "      <td>117.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>15.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>19.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>322</th>\n",
              "      <td>0.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>9.9</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>...</td>\n",
              "      <td>117.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>14.7</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>15.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>656</th>\n",
              "      <td>1.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>111.0</td>\n",
              "      <td>9.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>...</td>\n",
              "      <td>69.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>14.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>17.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1378</th>\n",
              "      <td>1.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>9.9</td>\n",
              "      <td>0.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>...</td>\n",
              "      <td>74.0</td>\n",
              "      <td>164.0</td>\n",
              "      <td>16.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>19.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1441</th>\n",
              "      <td>1.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>9.9</td>\n",
              "      <td>0.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>157.0</td>\n",
              "      <td>...</td>\n",
              "      <td>157.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>23.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>990284</th>\n",
              "      <td>1.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>9.9</td>\n",
              "      <td>9.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>...</td>\n",
              "      <td>86.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>13.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.3</td>\n",
              "      <td>19.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>990841</th>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>83.0</td>\n",
              "      <td>9.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>...</td>\n",
              "      <td>123.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>16.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>24.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>990873</th>\n",
              "      <td>1.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>9.9</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>...</td>\n",
              "      <td>107.0</td>\n",
              "      <td>106.0</td>\n",
              "      <td>13.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991092</th>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>9.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>...</td>\n",
              "      <td>121.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>12.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>25.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991129</th>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>79.1</td>\n",
              "      <td>9.9</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>...</td>\n",
              "      <td>137.0</td>\n",
              "      <td>143.0</td>\n",
              "      <td>13.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>12.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3118 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        sex   age  height  weight  waistline  sight_left  sight_right  \\\n",
              "76      1.0  65.0   165.0    70.0       94.0         9.9          9.9   \n",
              "322     0.0  70.0   150.0    60.0       95.0         9.9          0.2   \n",
              "656     1.0  70.0   165.0    90.0      111.0         9.9          1.0   \n",
              "1378    1.0  65.0   160.0    65.0       91.0         9.9          0.9   \n",
              "1441    1.0  75.0   170.0    70.0       89.0         9.9          0.7   \n",
              "...     ...   ...     ...     ...        ...         ...          ...   \n",
              "990284  1.0  60.0   160.0    70.0       86.0         9.9          9.9   \n",
              "990841  1.0  50.0   170.0    70.0       83.0         9.9          1.0   \n",
              "990873  1.0  75.0   160.0    70.0      100.0         9.9          0.3   \n",
              "991092  0.0  50.0   165.0    50.0       75.0         9.9          1.0   \n",
              "991129  0.0  60.0   150.0    60.0       79.1         9.9          1.2   \n",
              "\n",
              "        hear_left  hear_right    SBP  ...  LDL_chole  triglyceride  \\\n",
              "76            2.0         1.0  146.0  ...      117.0          89.0   \n",
              "322           1.0         1.0  124.0  ...      117.0          85.0   \n",
              "656           1.0         1.0  126.0  ...       69.0          86.0   \n",
              "1378          1.0         1.0  125.0  ...       74.0         164.0   \n",
              "1441          1.0         1.0  157.0  ...      157.0          76.0   \n",
              "...           ...         ...    ...  ...        ...           ...   \n",
              "990284        1.0         1.0  160.0  ...       86.0          90.0   \n",
              "990841        1.0         1.0  150.0  ...      123.0          99.0   \n",
              "990873        1.0         1.0  160.0  ...      107.0         106.0   \n",
              "991092        1.0         1.0  124.0  ...      121.0          79.0   \n",
              "991129        1.0         1.0  124.0  ...      137.0         143.0   \n",
              "\n",
              "        hemoglobin  urine_protein  serum_creatinine  SGOT_AST  SGOT_ALT  \\\n",
              "76            15.6            1.0               0.7      19.0      16.0   \n",
              "322           14.7            5.0               0.9      15.0      17.0   \n",
              "656           14.8            1.0               1.1      17.0      14.0   \n",
              "1378          16.8            1.0               0.8      19.0      19.0   \n",
              "1441          14.0            2.0               1.5      23.0      24.0   \n",
              "...            ...            ...               ...       ...       ...   \n",
              "990284        13.9            3.0               3.3      19.0      17.0   \n",
              "990841        16.5            1.0               1.1      24.0      17.0   \n",
              "990873        13.5            1.0               1.0      19.0      14.0   \n",
              "991092        12.4            1.0               0.6      25.0      20.0   \n",
              "991129        13.3            1.0               0.6      12.0      14.0   \n",
              "\n",
              "        gamma_GTP  SMK_stat_type_cd  DRK_YN  \n",
              "76           29.0               0.0     1.0  \n",
              "322          12.0               1.0     0.0  \n",
              "656          19.0               1.0     1.0  \n",
              "1378         75.0               0.0     1.0  \n",
              "1441         74.0               1.0     0.0  \n",
              "...           ...               ...     ...  \n",
              "990284       33.0               1.0     0.0  \n",
              "990841       19.0               0.0     1.0  \n",
              "990873       19.0               0.0     0.0  \n",
              "991092       60.0               1.0     1.0  \n",
              "991129       21.0               1.0     0.0  \n",
              "\n",
              "[3118 rows x 24 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "filtered_df = df[df['sight_left'] > 9.8]\n",
        "filtered_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "QJdipm_JSj9N",
        "outputId": "cbfa9e84-853c-4194-bcf3-4f63938e9571"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>waistline</th>\n",
              "      <th>sight_left</th>\n",
              "      <th>sight_right</th>\n",
              "      <th>hear_left</th>\n",
              "      <th>hear_right</th>\n",
              "      <th>SBP</th>\n",
              "      <th>...</th>\n",
              "      <th>LDL_chole</th>\n",
              "      <th>triglyceride</th>\n",
              "      <th>hemoglobin</th>\n",
              "      <th>urine_protein</th>\n",
              "      <th>serum_creatinine</th>\n",
              "      <th>SGOT_AST</th>\n",
              "      <th>SGOT_ALT</th>\n",
              "      <th>gamma_GTP</th>\n",
              "      <th>SMK_stat_type_cd</th>\n",
              "      <th>DRK_YN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>9.486833</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>...</td>\n",
              "      <td>11.224972</td>\n",
              "      <td>9.591663</td>\n",
              "      <td>17.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.582576</td>\n",
              "      <td>5.916080</td>\n",
              "      <td>6.324555</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>180.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9.433981</td>\n",
              "      <td>0.948683</td>\n",
              "      <td>1.095445</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>...</td>\n",
              "      <td>12.165525</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.948683</td>\n",
              "      <td>4.472136</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>5.196152</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>9.539392</td>\n",
              "      <td>1.095445</td>\n",
              "      <td>1.224745</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>...</td>\n",
              "      <td>8.602325</td>\n",
              "      <td>10.198039</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.948683</td>\n",
              "      <td>6.855655</td>\n",
              "      <td>5.656854</td>\n",
              "      <td>8.246211</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9.539392</td>\n",
              "      <td>1.224745</td>\n",
              "      <td>1.095445</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>...</td>\n",
              "      <td>10.198039</td>\n",
              "      <td>10.295630</td>\n",
              "      <td>17.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.048809</td>\n",
              "      <td>5.385165</td>\n",
              "      <td>5.830952</td>\n",
              "      <td>4.242641</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>8.944272</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.095445</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>...</td>\n",
              "      <td>10.816654</td>\n",
              "      <td>10.198039</td>\n",
              "      <td>13.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.894427</td>\n",
              "      <td>4.358899</td>\n",
              "      <td>3.464102</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991341</th>\n",
              "      <td>1.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9.596874</td>\n",
              "      <td>1.224745</td>\n",
              "      <td>1.224745</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>...</td>\n",
              "      <td>11.180340</td>\n",
              "      <td>11.489125</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.099020</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>5.196152</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991342</th>\n",
              "      <td>1.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>9.273618</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.224745</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>...</td>\n",
              "      <td>9.165151</td>\n",
              "      <td>6.708204</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.048809</td>\n",
              "      <td>3.741657</td>\n",
              "      <td>4.123106</td>\n",
              "      <td>3.872983</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991343</th>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>155.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>8.246211</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.836660</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>...</td>\n",
              "      <td>8.774964</td>\n",
              "      <td>12.529964</td>\n",
              "      <td>14.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.894427</td>\n",
              "      <td>5.477226</td>\n",
              "      <td>5.196152</td>\n",
              "      <td>4.123106</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991344</th>\n",
              "      <td>1.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>8.485281</td>\n",
              "      <td>1.224745</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>...</td>\n",
              "      <td>8.544004</td>\n",
              "      <td>7.280110</td>\n",
              "      <td>14.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.894427</td>\n",
              "      <td>4.582576</td>\n",
              "      <td>3.741657</td>\n",
              "      <td>4.123106</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991345</th>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>9.513149</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.224745</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>...</td>\n",
              "      <td>12.369317</td>\n",
              "      <td>12.767145</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.948683</td>\n",
              "      <td>4.898979</td>\n",
              "      <td>6.557439</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>991346 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        sex   age  height  weight  waistline  sight_left  sight_right  \\\n",
              "0       1.0  35.0   170.0    75.0   9.486833    1.000000     1.000000   \n",
              "1       1.0  30.0   180.0    80.0   9.433981    0.948683     1.095445   \n",
              "2       1.0  40.0   165.0    75.0   9.539392    1.095445     1.224745   \n",
              "3       1.0  50.0   175.0    80.0   9.539392    1.224745     1.095445   \n",
              "4       1.0  50.0   165.0    60.0   8.944272    1.000000     1.095445   \n",
              "...     ...   ...     ...     ...        ...         ...          ...   \n",
              "991341  1.0  45.0   175.0    80.0   9.596874    1.224745     1.224745   \n",
              "991342  1.0  35.0   170.0    75.0   9.273618    1.000000     1.224745   \n",
              "991343  0.0  40.0   155.0    50.0   8.246211    1.000000     0.836660   \n",
              "991344  1.0  25.0   175.0    60.0   8.485281    1.224745     1.000000   \n",
              "991345  1.0  50.0   160.0    70.0   9.513149    1.000000     1.224745   \n",
              "\n",
              "        hear_left  hear_right    SBP  ...  LDL_chole  triglyceride  \\\n",
              "0             1.0         1.0  120.0  ...  11.224972      9.591663   \n",
              "1             1.0         1.0  130.0  ...  12.165525     11.000000   \n",
              "2             1.0         1.0  120.0  ...   8.602325     10.198039   \n",
              "3             1.0         1.0  145.0  ...  10.198039     10.295630   \n",
              "4             1.0         1.0  138.0  ...  10.816654     10.198039   \n",
              "...           ...         ...    ...  ...        ...           ...   \n",
              "991341        1.0         1.0  114.0  ...  11.180340     11.489125   \n",
              "991342        1.0         1.0  119.0  ...   9.165151      6.708204   \n",
              "991343        1.0         1.0  110.0  ...   8.774964     12.529964   \n",
              "991344        1.0         1.0  119.0  ...   8.544004      7.280110   \n",
              "991345        1.0         1.0  133.0  ...  12.369317     12.767145   \n",
              "\n",
              "        hemoglobin  urine_protein  serum_creatinine  SGOT_AST  SGOT_ALT  \\\n",
              "0             17.1            1.0          1.000000  4.582576  5.916080   \n",
              "1             15.8            1.0          0.948683  4.472136  6.000000   \n",
              "2             15.8            1.0          0.948683  6.855655  5.656854   \n",
              "3             17.6            1.0          1.048809  5.385165  5.830952   \n",
              "4             13.8            1.0          0.894427  4.358899  3.464102   \n",
              "...            ...            ...               ...       ...       ...   \n",
              "991341        15.0            1.0          1.000000  5.099020  6.000000   \n",
              "991342        15.8            1.0          1.048809  3.741657  4.123106   \n",
              "991343        14.3            1.0          0.894427  5.477226  5.196152   \n",
              "991344        14.5            1.0          0.894427  4.582576  3.741657   \n",
              "991345        15.8            1.0          0.948683  4.898979  6.557439   \n",
              "\n",
              "        gamma_GTP  SMK_stat_type_cd  DRK_YN  \n",
              "0        6.324555               1.0     1.0  \n",
              "1        5.196152               0.0     0.0  \n",
              "2        8.246211               1.0     0.0  \n",
              "3        4.242641               1.0     0.0  \n",
              "4        5.000000               1.0     0.0  \n",
              "...           ...               ...     ...  \n",
              "991341   5.196152               1.0     0.0  \n",
              "991342   3.872983               1.0     0.0  \n",
              "991343   4.123106               0.0     1.0  \n",
              "991344   4.123106               1.0     0.0  \n",
              "991345   6.000000               0.0     1.0  \n",
              "\n",
              "[991346 rows x 24 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "columns_tranformed = ['waistline','sight_left','sight_right','hear_left','hear_right','BLDS',\n",
        "                      'tot_chole','HDL_chole','LDL_chole','triglyceride','urine_protein',\n",
        "                      'serum_creatinine','SGOT_AST','SGOT_ALT','gamma_GTP']\n",
        "\n",
        "for i in columns_tranformed:\n",
        "    df[i] = np.sqrt(df[i])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSNUpth2Sj9N",
        "outputId": "68993ce4-14df-442e-da3b-841f6e38385b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "sex                -0.642598\n",
              "age                 0.073802\n",
              "height             -0.491795\n",
              "weight             -0.395614\n",
              "waistline          -0.280184\n",
              "sight_left         -0.095448\n",
              "sight_right        -0.096737\n",
              "hear_left           0.010414\n",
              "hear_right          0.012682\n",
              "SBP                -0.107369\n",
              "DBP                -0.142998\n",
              "BLDS               -0.108436\n",
              "tot_chole          -0.005722\n",
              "HDL_chole           0.196042\n",
              "LDL_chole           0.015089\n",
              "triglyceride       -0.237070\n",
              "hemoglobin         -0.464186\n",
              "urine_protein      -0.018831\n",
              "serum_creatinine   -0.304228\n",
              "SGOT_AST           -0.126406\n",
              "SGOT_ALT           -0.214921\n",
              "gamma_GTP          -0.332368\n",
              "SMK_stat_type_cd    1.000000\n",
              "DRK_YN             -0.362274\n",
              "Name: SMK_stat_type_cd, dtype: float64"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corr = df.corr()\n",
        "corr['SMK_stat_type_cd']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fVMxUuF6Sj9O"
      },
      "outputs": [],
      "source": [
        "columns_to_be_scaled = ['age','height','weight','waistline','sight_left','sight_right','hear_left','hear_right',\n",
        "                        'SBP','DBP','BLDS','tot_chole','HDL_chole','LDL_chole','triglyceride','hemoglobin','urine_protein',\n",
        "                        'serum_creatinine','SGOT_AST','SGOT_ALT','gamma_GTP']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qEMRg5u9Sj9O"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "StandardScaler_object = StandardScaler()\n",
        "df[columns_to_be_scaled] = StandardScaler_object.fit_transform(df[columns_to_be_scaled])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtihaUO2Sj9P",
        "outputId": "9191dc06-4af5-4977-935a-cc901ebbf3dc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({0.0: 388905, 1.0: 388905})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from collections import Counter\n",
        "\n",
        "X = df.drop(\"SMK_stat_type_cd\",axis=1)\n",
        "y = df[\"SMK_stat_type_cd\"]\n",
        "#y = (df[\"SMK_stat_type_cd\"] == 1).astype(int)\n",
        "undersampler = RandomUnderSampler(sampling_strategy=1)\n",
        "X, y = undersampler.fit_resample(X, y)\n",
        "Counter(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOgeTsMRkkrU"
      },
      "source": [
        "# Multi-layer Perceptron from scratch with k fold cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Hwi90dip0WEQ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import regularizers\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "NdkBpHKbNY11"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VbUouea6edug"
      },
      "outputs": [],
      "source": [
        "y_train = np.array(y_train).astype(int)\n",
        "y_test = np.array(y_test).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8kX1Y4e9_Fd",
        "outputId": "a23d4285-23fa-426e-f920-06dcd29a8079"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training the model with scratch implementation\n",
            "Epoch 1/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.7504 - loss: 0.5849 - val_accuracy: 0.8229 - val_loss: 0.4646\n",
            "Epoch 2/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8247 - loss: 0.4585 - val_accuracy: 0.8260 - val_loss: 0.4459\n",
            "Epoch 3/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8283 - loss: 0.4407 - val_accuracy: 0.8266 - val_loss: 0.4372\n",
            "Epoch 4/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8292 - loss: 0.4333 - val_accuracy: 0.8271 - val_loss: 0.4312\n",
            "Epoch 5/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8296 - loss: 0.4282 - val_accuracy: 0.8277 - val_loss: 0.4276\n",
            "Epoch 6/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8296 - loss: 0.4247 - val_accuracy: 0.8277 - val_loss: 0.4251\n",
            "Epoch 7/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8298 - loss: 0.4228 - val_accuracy: 0.8281 - val_loss: 0.4234\n",
            "Epoch 8/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8298 - loss: 0.4207 - val_accuracy: 0.8286 - val_loss: 0.4219\n",
            "Epoch 9/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8299 - loss: 0.4199 - val_accuracy: 0.8282 - val_loss: 0.4203\n",
            "Epoch 10/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8302 - loss: 0.4179 - val_accuracy: 0.8284 - val_loss: 0.4193\n",
            "Epoch 11/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8297 - loss: 0.4175 - val_accuracy: 0.8281 - val_loss: 0.4185\n",
            "Epoch 12/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8299 - loss: 0.4167 - val_accuracy: 0.8286 - val_loss: 0.4184\n",
            "Epoch 13/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8307 - loss: 0.4152 - val_accuracy: 0.8284 - val_loss: 0.4175\n",
            "Epoch 14/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8300 - loss: 0.4158 - val_accuracy: 0.8279 - val_loss: 0.4171\n",
            "Epoch 15/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8313 - loss: 0.4134 - val_accuracy: 0.8283 - val_loss: 0.4163\n",
            "Epoch 16/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8297 - loss: 0.4147 - val_accuracy: 0.8284 - val_loss: 0.4160\n",
            "Epoch 17/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8302 - loss: 0.4143 - val_accuracy: 0.8286 - val_loss: 0.4164\n",
            "Epoch 18/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8305 - loss: 0.4133 - val_accuracy: 0.8282 - val_loss: 0.4152\n",
            "Epoch 19/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8305 - loss: 0.4129 - val_accuracy: 0.8284 - val_loss: 0.4149\n",
            "Epoch 20/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8311 - loss: 0.4128 - val_accuracy: 0.8285 - val_loss: 0.4151\n",
            "Epoch 21/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8304 - loss: 0.4130 - val_accuracy: 0.8283 - val_loss: 0.4146\n",
            "Epoch 22/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8294 - loss: 0.4135 - val_accuracy: 0.8284 - val_loss: 0.4143\n",
            "Epoch 23/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8298 - loss: 0.4134 - val_accuracy: 0.8285 - val_loss: 0.4142\n",
            "Epoch 24/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8307 - loss: 0.4118 - val_accuracy: 0.8286 - val_loss: 0.4147\n",
            "Epoch 25/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8305 - loss: 0.4116 - val_accuracy: 0.8278 - val_loss: 0.4140\n",
            "Epoch 26/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8310 - loss: 0.4113 - val_accuracy: 0.8284 - val_loss: 0.4136\n",
            "Epoch 27/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8296 - loss: 0.4129 - val_accuracy: 0.8287 - val_loss: 0.4144\n",
            "Epoch 28/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8292 - loss: 0.4134 - val_accuracy: 0.8287 - val_loss: 0.4138\n",
            "Epoch 29/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8298 - loss: 0.4124 - val_accuracy: 0.8279 - val_loss: 0.4134\n",
            "Epoch 30/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8304 - loss: 0.4111 - val_accuracy: 0.8286 - val_loss: 0.4133\n",
            "Epoch 31/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8298 - loss: 0.4114 - val_accuracy: 0.8283 - val_loss: 0.4130\n",
            "Epoch 32/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8305 - loss: 0.4109 - val_accuracy: 0.8285 - val_loss: 0.4127\n",
            "Epoch 33/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8308 - loss: 0.4103 - val_accuracy: 0.8287 - val_loss: 0.4126\n",
            "Epoch 34/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8297 - loss: 0.4123 - val_accuracy: 0.8287 - val_loss: 0.4126\n",
            "Epoch 35/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8303 - loss: 0.4102 - val_accuracy: 0.8288 - val_loss: 0.4129\n",
            "Epoch 36/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8305 - loss: 0.4099 - val_accuracy: 0.8285 - val_loss: 0.4125\n",
            "Epoch 37/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8300 - loss: 0.4108 - val_accuracy: 0.8285 - val_loss: 0.4124\n",
            "Epoch 38/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8309 - loss: 0.4098 - val_accuracy: 0.8285 - val_loss: 0.4123\n",
            "Epoch 39/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8304 - loss: 0.4102 - val_accuracy: 0.8279 - val_loss: 0.4126\n",
            "Epoch 40/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8302 - loss: 0.4107 - val_accuracy: 0.8286 - val_loss: 0.4119\n",
            "Epoch 41/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8316 - loss: 0.4089 - val_accuracy: 0.8288 - val_loss: 0.4121\n",
            "Epoch 42/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8302 - loss: 0.4104 - val_accuracy: 0.8288 - val_loss: 0.4119\n",
            "Epoch 43/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8307 - loss: 0.4090 - val_accuracy: 0.8285 - val_loss: 0.4122\n",
            "Epoch 44/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8294 - loss: 0.4117 - val_accuracy: 0.8281 - val_loss: 0.4122\n",
            "Epoch 45/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8305 - loss: 0.4104 - val_accuracy: 0.8285 - val_loss: 0.4119\n",
            "Epoch 46/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8302 - loss: 0.4100 - val_accuracy: 0.8286 - val_loss: 0.4116\n",
            "Epoch 47/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8308 - loss: 0.4095 - val_accuracy: 0.8286 - val_loss: 0.4117\n",
            "Epoch 48/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8293 - loss: 0.4113 - val_accuracy: 0.8285 - val_loss: 0.4120\n",
            "Epoch 49/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8297 - loss: 0.4100 - val_accuracy: 0.8282 - val_loss: 0.4118\n",
            "Epoch 50/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8313 - loss: 0.4090 - val_accuracy: 0.8289 - val_loss: 0.4116\n",
            "\u001b[1m13612/13612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 733us/step\n",
            "\u001b[1m3403/3403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 804us/step\n",
            "\u001b[1m7292/7292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 737us/step\n",
            "Training the model with sklearn implementation\n",
            "Iteration 1, loss = 0.50398777\n",
            "Validation score: 0.819227\n",
            "Iteration 2, loss = 0.42330276\n",
            "Validation score: 0.823899\n",
            "Iteration 3, loss = 0.41558924\n",
            "Validation score: 0.825380\n",
            "Iteration 4, loss = 0.41239690\n",
            "Validation score: 0.826540\n",
            "Iteration 5, loss = 0.41049926\n",
            "Validation score: 0.827228\n",
            "Iteration 6, loss = 0.40920549\n",
            "Validation score: 0.826700\n",
            "Iteration 7, loss = 0.40823151\n",
            "Validation score: 0.827401\n",
            "Iteration 8, loss = 0.40746717\n",
            "Validation score: 0.827297\n",
            "Iteration 9, loss = 0.40688912\n",
            "Validation score: 0.827366\n",
            "Iteration 10, loss = 0.40643811\n",
            "Validation score: 0.827733\n",
            "Iteration 11, loss = 0.40604513\n",
            "Validation score: 0.827538\n",
            "Iteration 12, loss = 0.40577088\n",
            "Validation score: 0.827653\n",
            "Iteration 13, loss = 0.40545525\n",
            "Validation score: 0.828055\n",
            "Iteration 14, loss = 0.40523566\n",
            "Validation score: 0.828422\n",
            "Iteration 15, loss = 0.40503857\n",
            "Validation score: 0.828261\n",
            "Iteration 16, loss = 0.40489024\n",
            "Validation score: 0.828250\n",
            "Iteration 17, loss = 0.40473132\n",
            "Validation score: 0.828606\n",
            "Iteration 18, loss = 0.40457634\n",
            "Validation score: 0.828457\n",
            "Iteration 19, loss = 0.40444588\n",
            "Validation score: 0.828571\n",
            "Iteration 20, loss = 0.40435402\n",
            "Validation score: 0.828755\n",
            "Iteration 21, loss = 0.40425455\n",
            "Validation score: 0.828847\n",
            "Iteration 22, loss = 0.40415696\n",
            "Validation score: 0.828698\n",
            "Iteration 23, loss = 0.40404598\n",
            "Validation score: 0.828858\n",
            "Iteration 24, loss = 0.40397271\n",
            "Validation score: 0.828824\n",
            "Iteration 25, loss = 0.40389921\n",
            "Validation score: 0.828583\n",
            "Iteration 26, loss = 0.40384530\n",
            "Validation score: 0.828675\n",
            "Iteration 27, loss = 0.40377543\n",
            "Validation score: 0.828755\n",
            "Iteration 28, loss = 0.40370988\n",
            "Validation score: 0.828790\n",
            "Iteration 29, loss = 0.40360951\n",
            "Validation score: 0.828767\n",
            "Iteration 30, loss = 0.40356598\n",
            "Validation score: 0.828835\n",
            "Iteration 31, loss = 0.40348681\n",
            "Validation score: 0.828870\n",
            "Iteration 32, loss = 0.40341516\n",
            "Validation score: 0.828962\n",
            "Iteration 33, loss = 0.40335706\n",
            "Validation score: 0.828652\n",
            "Iteration 34, loss = 0.40332063\n",
            "Validation score: 0.828778\n",
            "Iteration 35, loss = 0.40326162\n",
            "Validation score: 0.828571\n",
            "Iteration 36, loss = 0.40316561\n",
            "Validation score: 0.828824\n",
            "Iteration 37, loss = 0.40317967\n",
            "Validation score: 0.828835\n",
            "Iteration 38, loss = 0.40307178\n",
            "Validation score: 0.828721\n",
            "Iteration 39, loss = 0.40306274\n",
            "Validation score: 0.828927\n",
            "Iteration 40, loss = 0.40300234\n",
            "Validation score: 0.828835\n",
            "Iteration 41, loss = 0.40296030\n",
            "Validation score: 0.828790\n",
            "Iteration 42, loss = 0.40293444\n",
            "Validation score: 0.828973\n",
            "Iteration 43, loss = 0.40289354\n",
            "Validation score: 0.828973\n",
            "Iteration 44, loss = 0.40283442\n",
            "Validation score: 0.828732\n",
            "Iteration 45, loss = 0.40284034\n",
            "Validation score: 0.828560\n",
            "Iteration 46, loss = 0.40276673\n",
            "Validation score: 0.828858\n",
            "Iteration 47, loss = 0.40273036\n",
            "Validation score: 0.828434\n",
            "Iteration 48, loss = 0.40272020\n",
            "Validation score: 0.828732\n",
            "Iteration 49, loss = 0.40265328\n",
            "Validation score: 0.829054\n",
            "Iteration 50, loss = 0.40264642\n",
            "Validation score: 0.829191\n",
            "Iteration 51, loss = 0.40262529\n",
            "Validation score: 0.828640\n",
            "Iteration 52, loss = 0.40257043\n",
            "Validation score: 0.828801\n",
            "Iteration 53, loss = 0.40253896\n",
            "Validation score: 0.828675\n",
            "Iteration 54, loss = 0.40252230\n",
            "Validation score: 0.829088\n",
            "Iteration 55, loss = 0.40248121\n",
            "Validation score: 0.828812\n",
            "Iteration 56, loss = 0.40247429\n",
            "Validation score: 0.828996\n",
            "Iteration 57, loss = 0.40242059\n",
            "Validation score: 0.828778\n",
            "Iteration 58, loss = 0.40243095\n",
            "Validation score: 0.828698\n",
            "Iteration 59, loss = 0.40238523\n",
            "Validation score: 0.828606\n",
            "Iteration 60, loss = 0.40237015\n",
            "Validation score: 0.828847\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 61, loss = 0.40234562\n",
            "Validation score: 0.828617\n",
            "Validation score did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but MLPClassifier was fitted without feature names\n",
            "  warnings.warn(\n",
            "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but MLPClassifier was fitted without feature names\n",
            "  warnings.warn(\n",
            "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 2\n",
            "Training the model with scratch implementation\n",
            "Epoch 1/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7524 - loss: 0.6003 - val_accuracy: 0.8245 - val_loss: 0.4650\n",
            "Epoch 2/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8244 - loss: 0.4594 - val_accuracy: 0.8272 - val_loss: 0.4456\n",
            "Epoch 3/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8281 - loss: 0.4426 - val_accuracy: 0.8276 - val_loss: 0.4367\n",
            "Epoch 4/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8276 - loss: 0.4358 - val_accuracy: 0.8274 - val_loss: 0.4317\n",
            "Epoch 5/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8294 - loss: 0.4288 - val_accuracy: 0.8281 - val_loss: 0.4282\n",
            "Epoch 6/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8290 - loss: 0.4261 - val_accuracy: 0.8285 - val_loss: 0.4258\n",
            "Epoch 7/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8297 - loss: 0.4233 - val_accuracy: 0.8285 - val_loss: 0.4237\n",
            "Epoch 8/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8308 - loss: 0.4206 - val_accuracy: 0.8291 - val_loss: 0.4225\n",
            "Epoch 9/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8292 - loss: 0.4218 - val_accuracy: 0.8289 - val_loss: 0.4205\n",
            "Epoch 10/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8297 - loss: 0.4191 - val_accuracy: 0.8290 - val_loss: 0.4196\n",
            "Epoch 11/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8300 - loss: 0.4183 - val_accuracy: 0.8286 - val_loss: 0.4190\n",
            "Epoch 12/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8290 - loss: 0.4189 - val_accuracy: 0.8291 - val_loss: 0.4186\n",
            "Epoch 13/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8290 - loss: 0.4180 - val_accuracy: 0.8288 - val_loss: 0.4185\n",
            "Epoch 14/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8281 - loss: 0.4184 - val_accuracy: 0.8294 - val_loss: 0.4171\n",
            "Epoch 15/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8290 - loss: 0.4161 - val_accuracy: 0.8293 - val_loss: 0.4166\n",
            "Epoch 16/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8289 - loss: 0.4170 - val_accuracy: 0.8289 - val_loss: 0.4165\n",
            "Epoch 17/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8290 - loss: 0.4166 - val_accuracy: 0.8289 - val_loss: 0.4166\n",
            "Epoch 18/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8293 - loss: 0.4156 - val_accuracy: 0.8287 - val_loss: 0.4160\n",
            "Epoch 19/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8299 - loss: 0.4142 - val_accuracy: 0.8291 - val_loss: 0.4152\n",
            "Epoch 20/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8293 - loss: 0.4144 - val_accuracy: 0.8293 - val_loss: 0.4150\n",
            "Epoch 21/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8299 - loss: 0.4128 - val_accuracy: 0.8294 - val_loss: 0.4148\n",
            "Epoch 22/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8306 - loss: 0.4125 - val_accuracy: 0.8292 - val_loss: 0.4147\n",
            "Epoch 23/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8308 - loss: 0.4124 - val_accuracy: 0.8291 - val_loss: 0.4142\n",
            "Epoch 24/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8287 - loss: 0.4148 - val_accuracy: 0.8293 - val_loss: 0.4142\n",
            "Epoch 25/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8299 - loss: 0.4132 - val_accuracy: 0.8290 - val_loss: 0.4141\n",
            "Epoch 26/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8299 - loss: 0.4127 - val_accuracy: 0.8295 - val_loss: 0.4137\n",
            "Epoch 27/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8297 - loss: 0.4122 - val_accuracy: 0.8292 - val_loss: 0.4137\n",
            "Epoch 28/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8288 - loss: 0.4136 - val_accuracy: 0.8294 - val_loss: 0.4136\n",
            "Epoch 29/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8287 - loss: 0.4145 - val_accuracy: 0.8295 - val_loss: 0.4135\n",
            "Epoch 30/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8301 - loss: 0.4128 - val_accuracy: 0.8298 - val_loss: 0.4131\n",
            "Epoch 31/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8303 - loss: 0.4114 - val_accuracy: 0.8299 - val_loss: 0.4129\n",
            "Epoch 32/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8296 - loss: 0.4122 - val_accuracy: 0.8297 - val_loss: 0.4132\n",
            "Epoch 33/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8300 - loss: 0.4117 - val_accuracy: 0.8292 - val_loss: 0.4131\n",
            "Epoch 34/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8298 - loss: 0.4123 - val_accuracy: 0.8296 - val_loss: 0.4127\n",
            "Epoch 35/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8289 - loss: 0.4129 - val_accuracy: 0.8297 - val_loss: 0.4129\n",
            "Epoch 36/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8297 - loss: 0.4119 - val_accuracy: 0.8298 - val_loss: 0.4126\n",
            "Epoch 37/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8295 - loss: 0.4117 - val_accuracy: 0.8296 - val_loss: 0.4124\n",
            "Epoch 38/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8306 - loss: 0.4111 - val_accuracy: 0.8294 - val_loss: 0.4126\n",
            "Epoch 39/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8295 - loss: 0.4121 - val_accuracy: 0.8298 - val_loss: 0.4124\n",
            "Epoch 40/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8306 - loss: 0.4106 - val_accuracy: 0.8293 - val_loss: 0.4128\n",
            "Epoch 41/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8297 - loss: 0.4119 - val_accuracy: 0.8296 - val_loss: 0.4121\n",
            "Epoch 42/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8293 - loss: 0.4118 - val_accuracy: 0.8296 - val_loss: 0.4119\n",
            "Epoch 43/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8290 - loss: 0.4124 - val_accuracy: 0.8299 - val_loss: 0.4119\n",
            "Epoch 44/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8300 - loss: 0.4103 - val_accuracy: 0.8293 - val_loss: 0.4123\n",
            "Epoch 45/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8304 - loss: 0.4104 - val_accuracy: 0.8295 - val_loss: 0.4122\n",
            "Epoch 46/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8297 - loss: 0.4115 - val_accuracy: 0.8295 - val_loss: 0.4123\n",
            "Epoch 47/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8309 - loss: 0.4094 - val_accuracy: 0.8294 - val_loss: 0.4120\n",
            "Epoch 48/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8300 - loss: 0.4112 - val_accuracy: 0.8299 - val_loss: 0.4121\n",
            "Epoch 49/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8299 - loss: 0.4116 - val_accuracy: 0.8292 - val_loss: 0.4120\n",
            "Epoch 50/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8300 - loss: 0.4102 - val_accuracy: 0.8298 - val_loss: 0.4119\n",
            "\u001b[1m13612/13612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 739us/step\n",
            "\u001b[1m3403/3403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 740us/step\n",
            "\u001b[1m7292/7292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 760us/step\n",
            "Training the model with sklearn implementation\n",
            "Iteration 1, loss = 0.51143576\n",
            "Validation score: 0.816117\n",
            "Iteration 2, loss = 0.43210052\n",
            "Validation score: 0.823865\n",
            "Iteration 3, loss = 0.42079197\n",
            "Validation score: 0.826838\n",
            "Iteration 4, loss = 0.41517510\n",
            "Validation score: 0.828020\n",
            "Iteration 5, loss = 0.41216531\n",
            "Validation score: 0.828755\n",
            "Iteration 6, loss = 0.41034748\n",
            "Validation score: 0.829157\n",
            "Iteration 7, loss = 0.40925869\n",
            "Validation score: 0.829157\n",
            "Iteration 8, loss = 0.40848654\n",
            "Validation score: 0.829570\n",
            "Iteration 9, loss = 0.40793540\n",
            "Validation score: 0.829605\n",
            "Iteration 10, loss = 0.40750093\n",
            "Validation score: 0.829605\n",
            "Iteration 11, loss = 0.40714455\n",
            "Validation score: 0.829398\n",
            "Iteration 12, loss = 0.40686421\n",
            "Validation score: 0.829524\n",
            "Iteration 13, loss = 0.40658634\n",
            "Validation score: 0.829673\n",
            "Iteration 14, loss = 0.40636471\n",
            "Validation score: 0.829754\n",
            "Iteration 15, loss = 0.40613781\n",
            "Validation score: 0.830121\n",
            "Iteration 16, loss = 0.40598301\n",
            "Validation score: 0.829731\n",
            "Iteration 17, loss = 0.40580433\n",
            "Validation score: 0.829593\n",
            "Iteration 18, loss = 0.40563121\n",
            "Validation score: 0.829914\n",
            "Iteration 19, loss = 0.40545840\n",
            "Validation score: 0.829823\n",
            "Iteration 20, loss = 0.40533079\n",
            "Validation score: 0.829972\n",
            "Iteration 21, loss = 0.40520384\n",
            "Validation score: 0.830029\n",
            "Iteration 22, loss = 0.40504733\n",
            "Validation score: 0.830144\n",
            "Iteration 23, loss = 0.40492436\n",
            "Validation score: 0.830006\n",
            "Iteration 24, loss = 0.40480750\n",
            "Validation score: 0.830075\n",
            "Iteration 25, loss = 0.40472599\n",
            "Validation score: 0.830006\n",
            "Iteration 26, loss = 0.40463878\n",
            "Validation score: 0.830247\n",
            "Iteration 27, loss = 0.40453756\n",
            "Validation score: 0.829960\n",
            "Iteration 28, loss = 0.40448034\n",
            "Validation score: 0.830144\n",
            "Iteration 29, loss = 0.40439000\n",
            "Validation score: 0.830041\n",
            "Iteration 30, loss = 0.40432852\n",
            "Validation score: 0.830328\n",
            "Iteration 31, loss = 0.40428070\n",
            "Validation score: 0.830178\n",
            "Iteration 32, loss = 0.40419524\n",
            "Validation score: 0.829731\n",
            "Iteration 33, loss = 0.40416714\n",
            "Validation score: 0.829949\n",
            "Iteration 34, loss = 0.40409975\n",
            "Validation score: 0.829903\n",
            "Iteration 35, loss = 0.40399409\n",
            "Validation score: 0.829972\n",
            "Iteration 36, loss = 0.40399208\n",
            "Validation score: 0.829788\n",
            "Iteration 37, loss = 0.40397089\n",
            "Validation score: 0.829869\n",
            "Iteration 38, loss = 0.40387328\n",
            "Validation score: 0.829926\n",
            "Iteration 39, loss = 0.40383954\n",
            "Validation score: 0.829983\n",
            "Iteration 40, loss = 0.40378213\n",
            "Validation score: 0.830213\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 41, loss = 0.40372402\n",
            "Validation score: 0.830316\n",
            "Validation score did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but MLPClassifier was fitted without feature names\n",
            "  warnings.warn(\n",
            "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but MLPClassifier was fitted without feature names\n",
            "  warnings.warn(\n",
            "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 3\n",
            "Training the model with scratch implementation\n",
            "Epoch 1/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7350 - loss: 0.6081 - val_accuracy: 0.8256 - val_loss: 0.4635\n",
            "Epoch 2/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8251 - loss: 0.4573 - val_accuracy: 0.8286 - val_loss: 0.4445\n",
            "Epoch 3/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8276 - loss: 0.4422 - val_accuracy: 0.8291 - val_loss: 0.4358\n",
            "Epoch 4/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8279 - loss: 0.4352 - val_accuracy: 0.8301 - val_loss: 0.4302\n",
            "Epoch 5/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8283 - loss: 0.4302 - val_accuracy: 0.8300 - val_loss: 0.4263\n",
            "Epoch 6/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8271 - loss: 0.4279 - val_accuracy: 0.8301 - val_loss: 0.4238\n",
            "Epoch 7/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8282 - loss: 0.4243 - val_accuracy: 0.8303 - val_loss: 0.4220\n",
            "Epoch 8/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8286 - loss: 0.4217 - val_accuracy: 0.8305 - val_loss: 0.4204\n",
            "Epoch 9/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8294 - loss: 0.4200 - val_accuracy: 0.8305 - val_loss: 0.4192\n",
            "Epoch 10/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8287 - loss: 0.4200 - val_accuracy: 0.8305 - val_loss: 0.4188\n",
            "Epoch 11/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8283 - loss: 0.4197 - val_accuracy: 0.8305 - val_loss: 0.4175\n",
            "Epoch 12/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8293 - loss: 0.4172 - val_accuracy: 0.8306 - val_loss: 0.4167\n",
            "Epoch 13/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8291 - loss: 0.4173 - val_accuracy: 0.8304 - val_loss: 0.4163\n",
            "Epoch 14/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8287 - loss: 0.4166 - val_accuracy: 0.8307 - val_loss: 0.4159\n",
            "Epoch 15/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8296 - loss: 0.4158 - val_accuracy: 0.8307 - val_loss: 0.4154\n",
            "Epoch 16/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8292 - loss: 0.4161 - val_accuracy: 0.8309 - val_loss: 0.4149\n",
            "Epoch 17/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8296 - loss: 0.4148 - val_accuracy: 0.8309 - val_loss: 0.4150\n",
            "Epoch 18/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8298 - loss: 0.4137 - val_accuracy: 0.8308 - val_loss: 0.4142\n",
            "Epoch 19/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8295 - loss: 0.4141 - val_accuracy: 0.8308 - val_loss: 0.4143\n",
            "Epoch 20/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8293 - loss: 0.4140 - val_accuracy: 0.8310 - val_loss: 0.4139\n",
            "Epoch 21/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8292 - loss: 0.4142 - val_accuracy: 0.8310 - val_loss: 0.4141\n",
            "Epoch 22/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8293 - loss: 0.4129 - val_accuracy: 0.8311 - val_loss: 0.4138\n",
            "Epoch 23/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8299 - loss: 0.4131 - val_accuracy: 0.8308 - val_loss: 0.4132\n",
            "Epoch 24/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8291 - loss: 0.4132 - val_accuracy: 0.8308 - val_loss: 0.4132\n",
            "Epoch 25/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8291 - loss: 0.4129 - val_accuracy: 0.8310 - val_loss: 0.4128\n",
            "Epoch 26/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8295 - loss: 0.4116 - val_accuracy: 0.8310 - val_loss: 0.4125\n",
            "Epoch 27/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8298 - loss: 0.4119 - val_accuracy: 0.8303 - val_loss: 0.4138\n",
            "Epoch 28/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8295 - loss: 0.4120 - val_accuracy: 0.8307 - val_loss: 0.4129\n",
            "Epoch 29/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8293 - loss: 0.4128 - val_accuracy: 0.8311 - val_loss: 0.4124\n",
            "Epoch 30/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8295 - loss: 0.4120 - val_accuracy: 0.8310 - val_loss: 0.4124\n",
            "Epoch 31/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8297 - loss: 0.4118 - val_accuracy: 0.8310 - val_loss: 0.4123\n",
            "Epoch 32/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8289 - loss: 0.4139 - val_accuracy: 0.8308 - val_loss: 0.4125\n",
            "Epoch 33/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8301 - loss: 0.4121 - val_accuracy: 0.8307 - val_loss: 0.4125\n",
            "Epoch 34/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8299 - loss: 0.4117 - val_accuracy: 0.8311 - val_loss: 0.4117\n",
            "Epoch 35/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8279 - loss: 0.4136 - val_accuracy: 0.8310 - val_loss: 0.4116\n",
            "Epoch 36/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8294 - loss: 0.4117 - val_accuracy: 0.8313 - val_loss: 0.4117\n",
            "Epoch 37/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8298 - loss: 0.4119 - val_accuracy: 0.8310 - val_loss: 0.4115\n",
            "Epoch 38/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8295 - loss: 0.4118 - val_accuracy: 0.8308 - val_loss: 0.4121\n",
            "Epoch 39/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8294 - loss: 0.4114 - val_accuracy: 0.8311 - val_loss: 0.4114\n",
            "Epoch 40/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8303 - loss: 0.4107 - val_accuracy: 0.8309 - val_loss: 0.4120\n",
            "Epoch 41/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8298 - loss: 0.4110 - val_accuracy: 0.8313 - val_loss: 0.4113\n",
            "Epoch 42/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8306 - loss: 0.4092 - val_accuracy: 0.8308 - val_loss: 0.4127\n",
            "Epoch 43/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8302 - loss: 0.4102 - val_accuracy: 0.8314 - val_loss: 0.4114\n",
            "Epoch 44/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8294 - loss: 0.4120 - val_accuracy: 0.8311 - val_loss: 0.4111\n",
            "Epoch 45/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8299 - loss: 0.4104 - val_accuracy: 0.8313 - val_loss: 0.4111\n",
            "Epoch 46/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8296 - loss: 0.4120 - val_accuracy: 0.8311 - val_loss: 0.4111\n",
            "Epoch 47/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8301 - loss: 0.4109 - val_accuracy: 0.8309 - val_loss: 0.4111\n",
            "Epoch 48/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8293 - loss: 0.4108 - val_accuracy: 0.8312 - val_loss: 0.4109\n",
            "Epoch 49/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8294 - loss: 0.4116 - val_accuracy: 0.8312 - val_loss: 0.4110\n",
            "Epoch 50/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8304 - loss: 0.4099 - val_accuracy: 0.8312 - val_loss: 0.4111\n",
            "\u001b[1m13612/13612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 828us/step\n",
            "\u001b[1m3403/3403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 809us/step\n",
            "\u001b[1m7292/7292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 765us/step\n",
            "Training the model with sklearn implementation\n",
            "Iteration 1, loss = 0.52345664\n",
            "Validation score: 0.816082\n",
            "Iteration 2, loss = 0.43215009\n",
            "Validation score: 0.823704\n",
            "Iteration 3, loss = 0.41964313\n",
            "Validation score: 0.825151\n",
            "Iteration 4, loss = 0.41452444\n",
            "Validation score: 0.826930\n",
            "Iteration 5, loss = 0.41182463\n",
            "Validation score: 0.827114\n",
            "Iteration 6, loss = 0.41027545\n",
            "Validation score: 0.827630\n",
            "Iteration 7, loss = 0.40925047\n",
            "Validation score: 0.828514\n",
            "Iteration 8, loss = 0.40848105\n",
            "Validation score: 0.828239\n",
            "Iteration 9, loss = 0.40785441\n",
            "Validation score: 0.828009\n",
            "Iteration 10, loss = 0.40735563\n",
            "Validation score: 0.828330\n",
            "Iteration 11, loss = 0.40697626\n",
            "Validation score: 0.828732\n",
            "Iteration 12, loss = 0.40663163\n",
            "Validation score: 0.828904\n",
            "Iteration 13, loss = 0.40636400\n",
            "Validation score: 0.828812\n",
            "Iteration 14, loss = 0.40611165\n",
            "Validation score: 0.828904\n",
            "Iteration 15, loss = 0.40590859\n",
            "Validation score: 0.828962\n",
            "Iteration 16, loss = 0.40569414\n",
            "Validation score: 0.828640\n",
            "Iteration 17, loss = 0.40558322\n",
            "Validation score: 0.828801\n",
            "Iteration 18, loss = 0.40546107\n",
            "Validation score: 0.828881\n",
            "Iteration 19, loss = 0.40531130\n",
            "Validation score: 0.828812\n",
            "Iteration 20, loss = 0.40519418\n",
            "Validation score: 0.828881\n",
            "Iteration 21, loss = 0.40510894\n",
            "Validation score: 0.828801\n",
            "Iteration 22, loss = 0.40502850\n",
            "Validation score: 0.828767\n",
            "Iteration 23, loss = 0.40496391\n",
            "Validation score: 0.828790\n",
            "Iteration 24, loss = 0.40486986\n",
            "Validation score: 0.828893\n",
            "Iteration 25, loss = 0.40481952\n",
            "Validation score: 0.828939\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 26, loss = 0.40472899\n",
            "Validation score: 0.828617\n",
            "Validation score did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but MLPClassifier was fitted without feature names\n",
            "  warnings.warn(\n",
            "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but MLPClassifier was fitted without feature names\n",
            "  warnings.warn(\n",
            "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 4\n",
            "Training the model with scratch implementation\n",
            "Epoch 1/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.7251 - loss: 0.6510 - val_accuracy: 0.8260 - val_loss: 0.5522\n",
            "Epoch 2/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8261 - loss: 0.5385 - val_accuracy: 0.8295 - val_loss: 0.4989\n",
            "Epoch 3/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8289 - loss: 0.4910 - val_accuracy: 0.8297 - val_loss: 0.4671\n",
            "Epoch 4/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8288 - loss: 0.4631 - val_accuracy: 0.8299 - val_loss: 0.4476\n",
            "Epoch 5/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8295 - loss: 0.4445 - val_accuracy: 0.8300 - val_loss: 0.4357\n",
            "Epoch 6/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8288 - loss: 0.4349 - val_accuracy: 0.8295 - val_loss: 0.4285\n",
            "Epoch 7/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8292 - loss: 0.4281 - val_accuracy: 0.8296 - val_loss: 0.4243\n",
            "Epoch 8/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8288 - loss: 0.4243 - val_accuracy: 0.8299 - val_loss: 0.4214\n",
            "Epoch 9/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8282 - loss: 0.4227 - val_accuracy: 0.8294 - val_loss: 0.4199\n",
            "Epoch 10/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8280 - loss: 0.4219 - val_accuracy: 0.8295 - val_loss: 0.4182\n",
            "Epoch 11/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8290 - loss: 0.4191 - val_accuracy: 0.8298 - val_loss: 0.4173\n",
            "Epoch 12/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8291 - loss: 0.4170 - val_accuracy: 0.8298 - val_loss: 0.4162\n",
            "Epoch 13/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8288 - loss: 0.4172 - val_accuracy: 0.8297 - val_loss: 0.4159\n",
            "Epoch 14/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8296 - loss: 0.4158 - val_accuracy: 0.8299 - val_loss: 0.4156\n",
            "Epoch 15/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8291 - loss: 0.4156 - val_accuracy: 0.8301 - val_loss: 0.4149\n",
            "Epoch 16/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8292 - loss: 0.4149 - val_accuracy: 0.8299 - val_loss: 0.4143\n",
            "Epoch 17/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8285 - loss: 0.4164 - val_accuracy: 0.8299 - val_loss: 0.4138\n",
            "Epoch 18/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8302 - loss: 0.4138 - val_accuracy: 0.8297 - val_loss: 0.4139\n",
            "Epoch 19/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8280 - loss: 0.4156 - val_accuracy: 0.8300 - val_loss: 0.4134\n",
            "Epoch 20/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8293 - loss: 0.4142 - val_accuracy: 0.8299 - val_loss: 0.4135\n",
            "Epoch 21/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8290 - loss: 0.4146 - val_accuracy: 0.8302 - val_loss: 0.4131\n",
            "Epoch 22/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8306 - loss: 0.4118 - val_accuracy: 0.8303 - val_loss: 0.4130\n",
            "Epoch 23/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8291 - loss: 0.4144 - val_accuracy: 0.8298 - val_loss: 0.4127\n",
            "Epoch 24/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8295 - loss: 0.4131 - val_accuracy: 0.8301 - val_loss: 0.4124\n",
            "Epoch 25/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8302 - loss: 0.4131 - val_accuracy: 0.8302 - val_loss: 0.4124\n",
            "Epoch 26/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8300 - loss: 0.4125 - val_accuracy: 0.8300 - val_loss: 0.4127\n",
            "Epoch 27/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8302 - loss: 0.4126 - val_accuracy: 0.8302 - val_loss: 0.4125\n",
            "Epoch 28/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8301 - loss: 0.4117 - val_accuracy: 0.8300 - val_loss: 0.4121\n",
            "Epoch 29/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8291 - loss: 0.4140 - val_accuracy: 0.8301 - val_loss: 0.4119\n",
            "Epoch 30/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8294 - loss: 0.4135 - val_accuracy: 0.8302 - val_loss: 0.4120\n",
            "Epoch 31/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8305 - loss: 0.4122 - val_accuracy: 0.8300 - val_loss: 0.4117\n",
            "Epoch 32/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8293 - loss: 0.4128 - val_accuracy: 0.8303 - val_loss: 0.4116\n",
            "Epoch 33/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8303 - loss: 0.4116 - val_accuracy: 0.8301 - val_loss: 0.4115\n",
            "Epoch 34/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8299 - loss: 0.4129 - val_accuracy: 0.8300 - val_loss: 0.4116\n",
            "Epoch 35/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8293 - loss: 0.4126 - val_accuracy: 0.8300 - val_loss: 0.4115\n",
            "Epoch 36/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8293 - loss: 0.4125 - val_accuracy: 0.8300 - val_loss: 0.4115\n",
            "Epoch 37/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8302 - loss: 0.4116 - val_accuracy: 0.8303 - val_loss: 0.4113\n",
            "Epoch 38/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8305 - loss: 0.4107 - val_accuracy: 0.8303 - val_loss: 0.4115\n",
            "Epoch 39/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8294 - loss: 0.4124 - val_accuracy: 0.8300 - val_loss: 0.4112\n",
            "Epoch 40/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8294 - loss: 0.4127 - val_accuracy: 0.8302 - val_loss: 0.4114\n",
            "Epoch 41/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8305 - loss: 0.4107 - val_accuracy: 0.8299 - val_loss: 0.4113\n",
            "Epoch 42/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8300 - loss: 0.4112 - val_accuracy: 0.8302 - val_loss: 0.4111\n",
            "Epoch 43/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8298 - loss: 0.4118 - val_accuracy: 0.8300 - val_loss: 0.4111\n",
            "Epoch 44/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8300 - loss: 0.4103 - val_accuracy: 0.8304 - val_loss: 0.4110\n",
            "Epoch 45/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8304 - loss: 0.4111 - val_accuracy: 0.8301 - val_loss: 0.4113\n",
            "Epoch 46/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8307 - loss: 0.4111 - val_accuracy: 0.8304 - val_loss: 0.4108\n",
            "Epoch 47/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8309 - loss: 0.4101 - val_accuracy: 0.8301 - val_loss: 0.4111\n",
            "Epoch 48/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8294 - loss: 0.4119 - val_accuracy: 0.8300 - val_loss: 0.4109\n",
            "Epoch 49/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8299 - loss: 0.4113 - val_accuracy: 0.8302 - val_loss: 0.4106\n",
            "Epoch 50/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8301 - loss: 0.4110 - val_accuracy: 0.8303 - val_loss: 0.4105\n",
            "\u001b[1m13612/13612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 827us/step\n",
            "\u001b[1m3403/3403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 790us/step\n",
            "\u001b[1m7292/7292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 778us/step\n",
            "Training the model with sklearn implementation\n",
            "Iteration 1, loss = 0.50222701\n",
            "Validation score: 0.817506\n",
            "Iteration 2, loss = 0.42821624\n",
            "Validation score: 0.821982\n",
            "Iteration 3, loss = 0.41756609\n",
            "Validation score: 0.823670\n",
            "Iteration 4, loss = 0.41316162\n",
            "Validation score: 0.824600\n",
            "Iteration 5, loss = 0.41090959\n",
            "Validation score: 0.825288\n",
            "Iteration 6, loss = 0.40958748\n",
            "Validation score: 0.826012\n",
            "Iteration 7, loss = 0.40867336\n",
            "Validation score: 0.825839\n",
            "Iteration 8, loss = 0.40809238\n",
            "Validation score: 0.826436\n",
            "Iteration 9, loss = 0.40767399\n",
            "Validation score: 0.826413\n",
            "Iteration 10, loss = 0.40730421\n",
            "Validation score: 0.826413\n",
            "Iteration 11, loss = 0.40702924\n",
            "Validation score: 0.826781\n",
            "Iteration 12, loss = 0.40675776\n",
            "Validation score: 0.826517\n",
            "Iteration 13, loss = 0.40656506\n",
            "Validation score: 0.826976\n",
            "Iteration 14, loss = 0.40636886\n",
            "Validation score: 0.826953\n",
            "Iteration 15, loss = 0.40622978\n",
            "Validation score: 0.827045\n",
            "Iteration 16, loss = 0.40603738\n",
            "Validation score: 0.827091\n",
            "Iteration 17, loss = 0.40586022\n",
            "Validation score: 0.827068\n",
            "Iteration 18, loss = 0.40570878\n",
            "Validation score: 0.827194\n",
            "Iteration 19, loss = 0.40553801\n",
            "Validation score: 0.827378\n",
            "Iteration 20, loss = 0.40535660\n",
            "Validation score: 0.827332\n",
            "Iteration 21, loss = 0.40522761\n",
            "Validation score: 0.827401\n",
            "Iteration 22, loss = 0.40510124\n",
            "Validation score: 0.827527\n",
            "Iteration 23, loss = 0.40493158\n",
            "Validation score: 0.827355\n",
            "Iteration 24, loss = 0.40478854\n",
            "Validation score: 0.827550\n",
            "Iteration 25, loss = 0.40464217\n",
            "Validation score: 0.827550\n",
            "Iteration 26, loss = 0.40454144\n",
            "Validation score: 0.827688\n",
            "Iteration 27, loss = 0.40441257\n",
            "Validation score: 0.827653\n",
            "Iteration 28, loss = 0.40428003\n",
            "Validation score: 0.827458\n",
            "Iteration 29, loss = 0.40420254\n",
            "Validation score: 0.827768\n",
            "Iteration 30, loss = 0.40410958\n",
            "Validation score: 0.827378\n",
            "Iteration 31, loss = 0.40403602\n",
            "Validation score: 0.827446\n",
            "Iteration 32, loss = 0.40393249\n",
            "Validation score: 0.827745\n",
            "Iteration 33, loss = 0.40379925\n",
            "Validation score: 0.827561\n",
            "Iteration 34, loss = 0.40378747\n",
            "Validation score: 0.827848\n",
            "Iteration 35, loss = 0.40370524\n",
            "Validation score: 0.827722\n",
            "Iteration 36, loss = 0.40359812\n",
            "Validation score: 0.827538\n",
            "Iteration 37, loss = 0.40357869\n",
            "Validation score: 0.827814\n",
            "Iteration 38, loss = 0.40348685\n",
            "Validation score: 0.827791\n",
            "Iteration 39, loss = 0.40344939\n",
            "Validation score: 0.827722\n",
            "Iteration 40, loss = 0.40338403\n",
            "Validation score: 0.827756\n",
            "Iteration 41, loss = 0.40331549\n",
            "Validation score: 0.827917\n",
            "Iteration 42, loss = 0.40326270\n",
            "Validation score: 0.827492\n",
            "Iteration 43, loss = 0.40320985\n",
            "Validation score: 0.827688\n",
            "Iteration 44, loss = 0.40314799\n",
            "Validation score: 0.827653\n",
            "Iteration 45, loss = 0.40311007\n",
            "Validation score: 0.827688\n",
            "Iteration 46, loss = 0.40304967\n",
            "Validation score: 0.827733\n",
            "Iteration 47, loss = 0.40301642\n",
            "Validation score: 0.827619\n",
            "Iteration 48, loss = 0.40298652\n",
            "Validation score: 0.827848\n",
            "Iteration 49, loss = 0.40295306\n",
            "Validation score: 0.827676\n",
            "Iteration 50, loss = 0.40286806\n",
            "Validation score: 0.828020\n",
            "Iteration 51, loss = 0.40286435\n",
            "Validation score: 0.827779\n",
            "Iteration 52, loss = 0.40285789\n",
            "Validation score: 0.827745\n",
            "Iteration 53, loss = 0.40280215\n",
            "Validation score: 0.827756\n",
            "Iteration 54, loss = 0.40276456\n",
            "Validation score: 0.827917\n",
            "Iteration 55, loss = 0.40273944\n",
            "Validation score: 0.827894\n",
            "Iteration 56, loss = 0.40266041\n",
            "Validation score: 0.828066\n",
            "Iteration 57, loss = 0.40266582\n",
            "Validation score: 0.828009\n",
            "Iteration 58, loss = 0.40259890\n",
            "Validation score: 0.827837\n",
            "Iteration 59, loss = 0.40258721\n",
            "Validation score: 0.828032\n",
            "Iteration 60, loss = 0.40257489\n",
            "Validation score: 0.828170\n",
            "Iteration 61, loss = 0.40251379\n",
            "Validation score: 0.827860\n",
            "Iteration 62, loss = 0.40249607\n",
            "Validation score: 0.828135\n",
            "Iteration 63, loss = 0.40245399\n",
            "Validation score: 0.827676\n",
            "Iteration 64, loss = 0.40245006\n",
            "Validation score: 0.827917\n",
            "Iteration 65, loss = 0.40242647\n",
            "Validation score: 0.828043\n",
            "Iteration 66, loss = 0.40239382\n",
            "Validation score: 0.827883\n",
            "Iteration 67, loss = 0.40236903\n",
            "Validation score: 0.828147\n",
            "Iteration 68, loss = 0.40234538\n",
            "Validation score: 0.828089\n",
            "Iteration 69, loss = 0.40235449\n",
            "Validation score: 0.827963\n",
            "Iteration 70, loss = 0.40229750\n",
            "Validation score: 0.827997\n",
            "Iteration 71, loss = 0.40229024\n",
            "Validation score: 0.828204\n",
            "Iteration 72, loss = 0.40225697\n",
            "Validation score: 0.828066\n",
            "Iteration 73, loss = 0.40225548\n",
            "Validation score: 0.828227\n",
            "Iteration 74, loss = 0.40221086\n",
            "Validation score: 0.827963\n",
            "Iteration 75, loss = 0.40222495\n",
            "Validation score: 0.828239\n",
            "Iteration 76, loss = 0.40220948\n",
            "Validation score: 0.828170\n",
            "Iteration 77, loss = 0.40219379\n",
            "Validation score: 0.828043\n",
            "Iteration 78, loss = 0.40217891\n",
            "Validation score: 0.828284\n",
            "Iteration 79, loss = 0.40215665\n",
            "Validation score: 0.828009\n",
            "Iteration 80, loss = 0.40209274\n",
            "Validation score: 0.827745\n",
            "Iteration 81, loss = 0.40212808\n",
            "Validation score: 0.827997\n",
            "Iteration 82, loss = 0.40211642\n",
            "Validation score: 0.828239\n",
            "Iteration 83, loss = 0.40209613\n",
            "Validation score: 0.828261\n",
            "Iteration 84, loss = 0.40208869\n",
            "Validation score: 0.827768\n",
            "Iteration 85, loss = 0.40204297\n",
            "Validation score: 0.828009\n",
            "Iteration 86, loss = 0.40206841\n",
            "Validation score: 0.827894\n",
            "Iteration 87, loss = 0.40202938\n",
            "Validation score: 0.828147\n",
            "Iteration 88, loss = 0.40203097\n",
            "Validation score: 0.828193\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 89, loss = 0.40204447\n",
            "Validation score: 0.828032\n",
            "Validation score did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but MLPClassifier was fitted without feature names\n",
            "  warnings.warn(\n",
            "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but MLPClassifier was fitted without feature names\n",
            "  warnings.warn(\n",
            "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 5\n",
            "Training the model with scratch implementation\n",
            "Epoch 1/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7248 - loss: 0.5907 - val_accuracy: 0.8244 - val_loss: 0.4592\n",
            "Epoch 2/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8264 - loss: 0.4515 - val_accuracy: 0.8272 - val_loss: 0.4396\n",
            "Epoch 3/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8273 - loss: 0.4367 - val_accuracy: 0.8284 - val_loss: 0.4315\n",
            "Epoch 4/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8276 - loss: 0.4306 - val_accuracy: 0.8286 - val_loss: 0.4269\n",
            "Epoch 5/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8298 - loss: 0.4233 - val_accuracy: 0.8291 - val_loss: 0.4241\n",
            "Epoch 6/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8288 - loss: 0.4229 - val_accuracy: 0.8290 - val_loss: 0.4220\n",
            "Epoch 7/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8291 - loss: 0.4203 - val_accuracy: 0.8289 - val_loss: 0.4204\n",
            "Epoch 8/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8292 - loss: 0.4191 - val_accuracy: 0.8287 - val_loss: 0.4193\n",
            "Epoch 9/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8287 - loss: 0.4192 - val_accuracy: 0.8291 - val_loss: 0.4187\n",
            "Epoch 10/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8294 - loss: 0.4166 - val_accuracy: 0.8294 - val_loss: 0.4176\n",
            "Epoch 11/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8287 - loss: 0.4176 - val_accuracy: 0.8292 - val_loss: 0.4172\n",
            "Epoch 12/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8286 - loss: 0.4163 - val_accuracy: 0.8291 - val_loss: 0.4166\n",
            "Epoch 13/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8286 - loss: 0.4165 - val_accuracy: 0.8291 - val_loss: 0.4162\n",
            "Epoch 14/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8286 - loss: 0.4159 - val_accuracy: 0.8295 - val_loss: 0.4158\n",
            "Epoch 15/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8296 - loss: 0.4147 - val_accuracy: 0.8293 - val_loss: 0.4154\n",
            "Epoch 16/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8294 - loss: 0.4152 - val_accuracy: 0.8294 - val_loss: 0.4151\n",
            "Epoch 17/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8295 - loss: 0.4140 - val_accuracy: 0.8294 - val_loss: 0.4155\n",
            "Epoch 18/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8294 - loss: 0.4137 - val_accuracy: 0.8294 - val_loss: 0.4148\n",
            "Epoch 19/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8304 - loss: 0.4119 - val_accuracy: 0.8292 - val_loss: 0.4152\n",
            "Epoch 20/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8299 - loss: 0.4131 - val_accuracy: 0.8295 - val_loss: 0.4141\n",
            "Epoch 21/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8296 - loss: 0.4136 - val_accuracy: 0.8299 - val_loss: 0.4147\n",
            "Epoch 22/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8293 - loss: 0.4135 - val_accuracy: 0.8294 - val_loss: 0.4142\n",
            "Epoch 23/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8294 - loss: 0.4131 - val_accuracy: 0.8294 - val_loss: 0.4141\n",
            "Epoch 24/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8295 - loss: 0.4133 - val_accuracy: 0.8294 - val_loss: 0.4136\n",
            "Epoch 25/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8297 - loss: 0.4128 - val_accuracy: 0.8296 - val_loss: 0.4140\n",
            "Epoch 26/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8295 - loss: 0.4130 - val_accuracy: 0.8298 - val_loss: 0.4134\n",
            "Epoch 27/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8306 - loss: 0.4123 - val_accuracy: 0.8291 - val_loss: 0.4135\n",
            "Epoch 28/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8298 - loss: 0.4126 - val_accuracy: 0.8298 - val_loss: 0.4131\n",
            "Epoch 29/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8296 - loss: 0.4124 - val_accuracy: 0.8294 - val_loss: 0.4131\n",
            "Epoch 30/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8306 - loss: 0.4114 - val_accuracy: 0.8296 - val_loss: 0.4133\n",
            "Epoch 31/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8300 - loss: 0.4109 - val_accuracy: 0.8289 - val_loss: 0.4135\n",
            "Epoch 32/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8294 - loss: 0.4121 - val_accuracy: 0.8295 - val_loss: 0.4132\n",
            "Epoch 33/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8299 - loss: 0.4112 - val_accuracy: 0.8299 - val_loss: 0.4127\n",
            "Epoch 34/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8299 - loss: 0.4115 - val_accuracy: 0.8296 - val_loss: 0.4128\n",
            "Epoch 35/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8302 - loss: 0.4115 - val_accuracy: 0.8294 - val_loss: 0.4126\n",
            "Epoch 36/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8306 - loss: 0.4113 - val_accuracy: 0.8295 - val_loss: 0.4126\n",
            "Epoch 37/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8304 - loss: 0.4106 - val_accuracy: 0.8298 - val_loss: 0.4126\n",
            "Epoch 38/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8304 - loss: 0.4105 - val_accuracy: 0.8298 - val_loss: 0.4122\n",
            "Epoch 39/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8304 - loss: 0.4107 - val_accuracy: 0.8296 - val_loss: 0.4126\n",
            "Epoch 40/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8298 - loss: 0.4115 - val_accuracy: 0.8296 - val_loss: 0.4123\n",
            "Epoch 41/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8289 - loss: 0.4122 - val_accuracy: 0.8297 - val_loss: 0.4127\n",
            "Epoch 42/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8301 - loss: 0.4109 - val_accuracy: 0.8296 - val_loss: 0.4122\n",
            "Epoch 43/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8285 - loss: 0.4124 - val_accuracy: 0.8299 - val_loss: 0.4122\n",
            "Epoch 44/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8301 - loss: 0.4114 - val_accuracy: 0.8298 - val_loss: 0.4117\n",
            "Epoch 45/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8295 - loss: 0.4103 - val_accuracy: 0.8299 - val_loss: 0.4122\n",
            "Epoch 46/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8294 - loss: 0.4121 - val_accuracy: 0.8298 - val_loss: 0.4119\n",
            "Epoch 47/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8300 - loss: 0.4107 - val_accuracy: 0.8298 - val_loss: 0.4118\n",
            "Epoch 48/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8295 - loss: 0.4114 - val_accuracy: 0.8296 - val_loss: 0.4125\n",
            "Epoch 49/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8293 - loss: 0.4112 - val_accuracy: 0.8298 - val_loss: 0.4118\n",
            "Epoch 50/50\n",
            "\u001b[1m2178/2178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8305 - loss: 0.4095 - val_accuracy: 0.8298 - val_loss: 0.4118\n",
            "\u001b[1m13612/13612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 849us/step\n",
            "\u001b[1m3403/3403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 859us/step\n",
            "\u001b[1m7292/7292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 853us/step\n",
            "Training the model with sklearn implementation\n",
            "Iteration 1, loss = 0.54026664\n",
            "Validation score: 0.815600\n",
            "Iteration 2, loss = 0.43073295\n",
            "Validation score: 0.822051\n",
            "Iteration 3, loss = 0.41932511\n",
            "Validation score: 0.824393\n",
            "Iteration 4, loss = 0.41391757\n",
            "Validation score: 0.825587\n",
            "Iteration 5, loss = 0.41089090\n",
            "Validation score: 0.826138\n",
            "Iteration 6, loss = 0.40919882\n",
            "Validation score: 0.827056\n",
            "Iteration 7, loss = 0.40816980\n",
            "Validation score: 0.826792\n",
            "Iteration 8, loss = 0.40745179\n",
            "Validation score: 0.827102\n",
            "Iteration 9, loss = 0.40698583\n",
            "Validation score: 0.826804\n",
            "Iteration 10, loss = 0.40660992\n",
            "Validation score: 0.826999\n",
            "Iteration 11, loss = 0.40629091\n",
            "Validation score: 0.827194\n",
            "Iteration 12, loss = 0.40600408\n",
            "Validation score: 0.826895\n",
            "Iteration 13, loss = 0.40576451\n",
            "Validation score: 0.827102\n",
            "Iteration 14, loss = 0.40550775\n",
            "Validation score: 0.827228\n",
            "Iteration 15, loss = 0.40535866\n",
            "Validation score: 0.827033\n",
            "Iteration 16, loss = 0.40514446\n",
            "Validation score: 0.827056\n",
            "Iteration 17, loss = 0.40498526\n",
            "Validation score: 0.827355\n",
            "Iteration 18, loss = 0.40476297\n",
            "Validation score: 0.827355\n",
            "Iteration 19, loss = 0.40464582\n",
            "Validation score: 0.827217\n",
            "Iteration 20, loss = 0.40451239\n",
            "Validation score: 0.827171\n",
            "Iteration 21, loss = 0.40431727\n",
            "Validation score: 0.827182\n",
            "Iteration 22, loss = 0.40422177\n",
            "Validation score: 0.826976\n",
            "Iteration 23, loss = 0.40408198\n",
            "Validation score: 0.827010\n",
            "Iteration 24, loss = 0.40395022\n",
            "Validation score: 0.827469\n",
            "Iteration 25, loss = 0.40384555\n",
            "Validation score: 0.827286\n",
            "Iteration 26, loss = 0.40371354\n",
            "Validation score: 0.827550\n",
            "Iteration 27, loss = 0.40360409\n",
            "Validation score: 0.827401\n",
            "Iteration 28, loss = 0.40348265\n",
            "Validation score: 0.827125\n",
            "Iteration 29, loss = 0.40341519\n",
            "Validation score: 0.827332\n",
            "Iteration 30, loss = 0.40332483\n",
            "Validation score: 0.827401\n",
            "Iteration 31, loss = 0.40324437\n",
            "Validation score: 0.827389\n",
            "Iteration 32, loss = 0.40315451\n",
            "Validation score: 0.827102\n",
            "Iteration 33, loss = 0.40309624\n",
            "Validation score: 0.827515\n",
            "Iteration 34, loss = 0.40301381\n",
            "Validation score: 0.827240\n",
            "Iteration 35, loss = 0.40299251\n",
            "Validation score: 0.827091\n",
            "Iteration 36, loss = 0.40287726\n",
            "Validation score: 0.827091\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 37, loss = 0.40283939\n",
            "Validation score: 0.827274\n",
            "Validation score did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but MLPClassifier was fitted without feature names\n",
            "  warnings.warn(\n",
            "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but MLPClassifier was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "y_train_pred_list = []\n",
        "y_val_pred_list = []\n",
        "y_test_pred_list = []\n",
        "\n",
        "y_train_list = []\n",
        "y_val_list = []\n",
        "\n",
        "y_train_pred_list_sk = []\n",
        "y_val_pred_list_sk = []\n",
        "y_test_pred_list_sk = []\n",
        "\n",
        "#Kfold\n",
        "num_folds = 5\n",
        "fold_size = len(X_train) // num_folds\n",
        "X_train_cv = X_train\n",
        "y_train_cv = y_train\n",
        "\n",
        "for fold in range(num_folds):\n",
        "    print(f\"Fold {fold+1}\")\n",
        "    # Define the start and end index for the validation set\n",
        "    start_val_index = fold * fold_size\n",
        "    end_val_index = (fold + 1) * fold_size\n",
        "\n",
        "    # New training set\n",
        "    X_train = np.append(X_train_cv[:start_val_index], X_train_cv[end_val_index:], axis=0)\n",
        "    y_train = np.append(y_train_cv[:start_val_index], y_train_cv[end_val_index:])\n",
        "    y_train_list.append(y_train)\n",
        "\n",
        "    X_val = X_train_cv[start_val_index:end_val_index]\n",
        "    y_val = y_train_cv[start_val_index:end_val_index]\n",
        "    y_val_list.append(y_val)\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(23, kernel_regularizer=regularizers.l2(0.001), activation='relu', input_shape=(X_train.shape[1],)),\n",
        "            tf.keras.layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu'),\n",
        "            tf.keras.layers.Dense(8, kernel_regularizer=regularizers.l2(0.001), activation='relu'),\n",
        "            tf.keras.layers.Dense(4, kernel_regularizer=regularizers.l2(0.001), activation='relu'),\n",
        "            tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "    ])\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    print('Training the model with scratch implementation')\n",
        "    history = model.fit(X_train, y_train, epochs=50, batch_size=200, validation_data=(X_val, y_val))\n",
        "\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_val_pred = model.predict(X_val)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    y_train_pred_list.append(y_train_pred)\n",
        "    y_val_pred_list.append(y_val_pred)\n",
        "    y_test_pred_list.append(y_test_pred)\n",
        "\n",
        "    print('Training the model with sklearn implementation')\n",
        "    model_sk = MLPClassifier(hidden_layer_sizes=(16,8,4), tol=0.00001, alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.0002, validation_fraction=0.2, verbose = True, early_stopping=True, activation='relu', solver='adam', max_iter=200)\n",
        "    model_history = model_sk.fit(X_train,y_train)\n",
        "    model.save(f\"model_{fold+1}.h5\")\n",
        "\n",
        "    y_train_pred_sk = model_sk.predict(X_train)\n",
        "    y_val_pred_sk = model_sk.predict(X_val)\n",
        "    y_test_pred_sk = model_sk.predict(X_test)\n",
        "    y_train_pred_list_sk.append(y_train_pred_sk)\n",
        "    y_val_pred_list_sk.append(y_val_pred_sk)\n",
        "    y_test_pred_list_sk.append(y_test_pred_sk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAaZulLTulON",
        "outputId": "071ec500-db7c-44ce-a33b-4e895d781d68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*************************************************************************************Fold1*********************************************************************************************\n",
            "Multi layer Perceptron from Scratch Model\n",
            "Training Accuracy fold1 scratch model: 0.830\n",
            "Validation Accuracy fold1 scratch model: 0.831\n",
            "Testing Accuracy fold1 scratch model: 0.830\n",
            "Classification Report training scratch model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    218060\n",
            "           1       0.90      0.74      0.81    217514\n",
            "\n",
            "    accuracy                           0.83    435574\n",
            "   macro avg       0.84      0.83      0.83    435574\n",
            "weighted avg       0.84      0.83      0.83    435574\n",
            "\n",
            "Classification Report validation scratch model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.85     54456\n",
            "           1       0.91      0.74      0.81     54437\n",
            "\n",
            "    accuracy                           0.83    108893\n",
            "   macro avg       0.84      0.83      0.83    108893\n",
            "weighted avg       0.84      0.83      0.83    108893\n",
            "\n",
            "Classification Report testing scratch model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    116389\n",
            "           1       0.91      0.74      0.81    116954\n",
            "\n",
            "    accuracy                           0.83    233343\n",
            "   macro avg       0.84      0.83      0.83    233343\n",
            "weighted avg       0.84      0.83      0.83    233343\n",
            "\n",
            "Sklearn Multi layer Perceptron Model\n",
            "Training Accuracy fold1 Sklearn model: 0.830\n",
            "Validation Accuracy fold1 Sklearn model: 0.831\n",
            "Testing Accuracy fold1 Sklearn model: 0.830\n",
            "Classification Report training Sklearn model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    218060\n",
            "           1       0.90      0.74      0.81    217514\n",
            "\n",
            "    accuracy                           0.83    435574\n",
            "   macro avg       0.84      0.83      0.83    435574\n",
            "weighted avg       0.84      0.83      0.83    435574\n",
            "\n",
            "Classification Report validation Sklearn model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84     54456\n",
            "           1       0.90      0.74      0.81     54437\n",
            "\n",
            "    accuracy                           0.83    108893\n",
            "   macro avg       0.84      0.83      0.83    108893\n",
            "weighted avg       0.84      0.83      0.83    108893\n",
            "\n",
            "Classification Report testing Sklearn model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    116389\n",
            "           1       0.90      0.74      0.81    116954\n",
            "\n",
            "    accuracy                           0.83    233343\n",
            "   macro avg       0.84      0.83      0.83    233343\n",
            "weighted avg       0.84      0.83      0.83    233343\n",
            "\n",
            "**********************************************************************************************************************************************************************************\n",
            "*************************************************************************************Fold2*********************************************************************************************\n",
            "Multi layer Perceptron from Scratch Model\n",
            "Training Accuracy fold2 scratch model: 0.830\n",
            "Validation Accuracy fold2 scratch model: 0.831\n",
            "Testing Accuracy fold2 scratch model: 0.830\n",
            "Classification Report training scratch model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.91      0.84    218068\n",
            "           1       0.90      0.75      0.81    217506\n",
            "\n",
            "    accuracy                           0.83    435574\n",
            "   macro avg       0.84      0.83      0.83    435574\n",
            "weighted avg       0.84      0.83      0.83    435574\n",
            "\n",
            "Classification Report validation scratch model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.91      0.84     54448\n",
            "           1       0.90      0.75      0.82     54445\n",
            "\n",
            "    accuracy                           0.83    108893\n",
            "   macro avg       0.84      0.83      0.83    108893\n",
            "weighted avg       0.84      0.83      0.83    108893\n",
            "\n",
            "Classification Report testing scratch model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.91      0.84    116389\n",
            "           1       0.90      0.75      0.81    116954\n",
            "\n",
            "    accuracy                           0.83    233343\n",
            "   macro avg       0.84      0.83      0.83    233343\n",
            "weighted avg       0.84      0.83      0.83    233343\n",
            "\n",
            "Sklearn Multi layer Perceptron Model\n",
            "Training Accuracy fold2 Sklearn model: 0.829\n",
            "Validation Accuracy fold2 Sklearn model: 0.831\n",
            "Testing Accuracy fold2 Sklearn model: 0.829\n",
            "Classification Report training Sklearn model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    218068\n",
            "           1       0.91      0.74      0.81    217506\n",
            "\n",
            "    accuracy                           0.83    435574\n",
            "   macro avg       0.84      0.83      0.83    435574\n",
            "weighted avg       0.84      0.83      0.83    435574\n",
            "\n",
            "Classification Report validation Sklearn model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.85     54448\n",
            "           1       0.91      0.74      0.81     54445\n",
            "\n",
            "    accuracy                           0.83    108893\n",
            "   macro avg       0.84      0.83      0.83    108893\n",
            "weighted avg       0.84      0.83      0.83    108893\n",
            "\n",
            "Classification Report testing Sklearn model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    116389\n",
            "           1       0.91      0.74      0.81    116954\n",
            "\n",
            "    accuracy                           0.83    233343\n",
            "   macro avg       0.84      0.83      0.83    233343\n",
            "weighted avg       0.84      0.83      0.83    233343\n",
            "\n",
            "**********************************************************************************************************************************************************************************\n",
            "*************************************************************************************Fold3*********************************************************************************************\n",
            "Multi layer Perceptron from Scratch Model\n",
            "Training Accuracy fold3 scratch model: 0.830\n",
            "Validation Accuracy fold3 scratch model: 0.830\n",
            "Testing Accuracy fold3 scratch model: 0.830\n",
            "Classification Report training scratch model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.91      0.84    217921\n",
            "           1       0.90      0.75      0.81    217653\n",
            "\n",
            "    accuracy                           0.83    435574\n",
            "   macro avg       0.84      0.83      0.83    435574\n",
            "weighted avg       0.84      0.83      0.83    435574\n",
            "\n",
            "Classification Report validation scratch model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.91      0.84     54595\n",
            "           1       0.89      0.75      0.81     54298\n",
            "\n",
            "    accuracy                           0.83    108893\n",
            "   macro avg       0.84      0.83      0.83    108893\n",
            "weighted avg       0.84      0.83      0.83    108893\n",
            "\n",
            "Classification Report testing scratch model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.91      0.84    116389\n",
            "           1       0.90      0.75      0.81    116954\n",
            "\n",
            "    accuracy                           0.83    233343\n",
            "   macro avg       0.84      0.83      0.83    233343\n",
            "weighted avg       0.84      0.83      0.83    233343\n",
            "\n",
            "Sklearn Multi layer Perceptron Model\n",
            "Training Accuracy fold3 Sklearn model: 0.830\n",
            "Validation Accuracy fold3 Sklearn model: 0.829\n",
            "Testing Accuracy fold3 Sklearn model: 0.829\n",
            "Classification Report training Sklearn model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    217921\n",
            "           1       0.90      0.74      0.81    217653\n",
            "\n",
            "    accuracy                           0.83    435574\n",
            "   macro avg       0.84      0.83      0.83    435574\n",
            "weighted avg       0.84      0.83      0.83    435574\n",
            "\n",
            "Classification Report validation Sklearn model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84     54595\n",
            "           1       0.90      0.74      0.81     54298\n",
            "\n",
            "    accuracy                           0.83    108893\n",
            "   macro avg       0.84      0.83      0.83    108893\n",
            "weighted avg       0.84      0.83      0.83    108893\n",
            "\n",
            "Classification Report testing Sklearn model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    116389\n",
            "           1       0.90      0.74      0.81    116954\n",
            "\n",
            "    accuracy                           0.83    233343\n",
            "   macro avg       0.84      0.83      0.83    233343\n",
            "weighted avg       0.84      0.83      0.83    233343\n",
            "\n",
            "**********************************************************************************************************************************************************************************\n",
            "*************************************************************************************Fold4*********************************************************************************************\n",
            "Multi layer Perceptron from Scratch Model\n",
            "Training Accuracy fold4 scratch model: 0.831\n",
            "Validation Accuracy fold4 scratch model: 0.827\n",
            "Testing Accuracy fold4 scratch model: 0.830\n",
            "Classification Report training scratch model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    218017\n",
            "           1       0.90      0.74      0.81    217557\n",
            "\n",
            "    accuracy                           0.83    435574\n",
            "   macro avg       0.84      0.83      0.83    435574\n",
            "weighted avg       0.84      0.83      0.83    435574\n",
            "\n",
            "Classification Report validation scratch model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84     54499\n",
            "           1       0.90      0.74      0.81     54394\n",
            "\n",
            "    accuracy                           0.83    108893\n",
            "   macro avg       0.84      0.83      0.83    108893\n",
            "weighted avg       0.84      0.83      0.83    108893\n",
            "\n",
            "Classification Report testing scratch model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    116389\n",
            "           1       0.90      0.74      0.81    116954\n",
            "\n",
            "    accuracy                           0.83    233343\n",
            "   macro avg       0.84      0.83      0.83    233343\n",
            "weighted avg       0.84      0.83      0.83    233343\n",
            "\n",
            "Sklearn Multi layer Perceptron Model\n",
            "Training Accuracy fold4 Sklearn model: 0.831\n",
            "Validation Accuracy fold4 Sklearn model: 0.826\n",
            "Testing Accuracy fold4 Sklearn model: 0.829\n",
            "Classification Report training Sklearn model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    218017\n",
            "           1       0.90      0.74      0.81    217557\n",
            "\n",
            "    accuracy                           0.83    435574\n",
            "   macro avg       0.84      0.83      0.83    435574\n",
            "weighted avg       0.84      0.83      0.83    435574\n",
            "\n",
            "Classification Report validation Sklearn model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84     54499\n",
            "           1       0.90      0.74      0.81     54394\n",
            "\n",
            "    accuracy                           0.83    108893\n",
            "   macro avg       0.84      0.83      0.82    108893\n",
            "weighted avg       0.84      0.83      0.82    108893\n",
            "\n",
            "Classification Report testing Sklearn model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    116389\n",
            "           1       0.90      0.74      0.81    116954\n",
            "\n",
            "    accuracy                           0.83    233343\n",
            "   macro avg       0.84      0.83      0.83    233343\n",
            "weighted avg       0.84      0.83      0.83    233343\n",
            "\n",
            "**********************************************************************************************************************************************************************************\n",
            "*************************************************************************************Fold5*********************************************************************************************\n",
            "Multi layer Perceptron from Scratch Model\n",
            "Training Accuracy fold5 scratch model: 0.830\n",
            "Validation Accuracy fold5 scratch model: 0.832\n",
            "Testing Accuracy fold5 scratch model: 0.830\n",
            "Classification Report training scratch model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    217999\n",
            "           1       0.90      0.74      0.81    217575\n",
            "\n",
            "    accuracy                           0.83    435574\n",
            "   macro avg       0.84      0.83      0.83    435574\n",
            "weighted avg       0.84      0.83      0.83    435574\n",
            "\n",
            "Classification Report validation scratch model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.85     54517\n",
            "           1       0.90      0.74      0.81     54376\n",
            "\n",
            "    accuracy                           0.83    108893\n",
            "   macro avg       0.84      0.83      0.83    108893\n",
            "weighted avg       0.84      0.83      0.83    108893\n",
            "\n",
            "Classification Report testing scratch model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    116389\n",
            "           1       0.90      0.74      0.81    116954\n",
            "\n",
            "    accuracy                           0.83    233343\n",
            "   macro avg       0.84      0.83      0.83    233343\n",
            "weighted avg       0.84      0.83      0.83    233343\n",
            "\n",
            "Sklearn Multi layer Perceptron Model\n",
            "Training Accuracy fold5 Sklearn model: 0.830\n",
            "Validation Accuracy fold5 Sklearn model: 0.831\n",
            "Testing Accuracy fold5 Sklearn model: 0.829\n",
            "Classification Report training Sklearn model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    217999\n",
            "           1       0.90      0.74      0.81    217575\n",
            "\n",
            "    accuracy                           0.83    435574\n",
            "   macro avg       0.84      0.83      0.83    435574\n",
            "weighted avg       0.84      0.83      0.83    435574\n",
            "\n",
            "Classification Report validation Sklearn model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.85     54517\n",
            "           1       0.90      0.74      0.81     54376\n",
            "\n",
            "    accuracy                           0.83    108893\n",
            "   macro avg       0.84      0.83      0.83    108893\n",
            "weighted avg       0.84      0.83      0.83    108893\n",
            "\n",
            "Classification Report testing Sklearn model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    116389\n",
            "           1       0.90      0.74      0.81    116954\n",
            "\n",
            "    accuracy                           0.83    233343\n",
            "   macro avg       0.84      0.83      0.83    233343\n",
            "weighted avg       0.84      0.83      0.83    233343\n",
            "\n",
            "**********************************************************************************************************************************************************************************\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "threshold = 0.5\n",
        "\n",
        "y_train_pred_list = np.array(y_train_pred_list) >= threshold\n",
        "y_val_pred_list = np.array(y_val_pred_list) >= threshold\n",
        "y_test_pred_list = np.array(y_test_pred_list) >= threshold\n",
        "\n",
        "for i in range((len(y_train_pred_list))):\n",
        "  training_accuracy = accuracy_score(y_train_list[i], y_train_pred_list[i])\n",
        "  validation_accuracy = accuracy_score(y_val_list[i], y_val_pred_list[i])\n",
        "  testing_accuracy = accuracy_score(y_test, y_test_pred_list[i])\n",
        "\n",
        "  training_report = classification_report(y_train_list[i], y_train_pred_list[i])\n",
        "  validation_report = classification_report(y_val_list[i], y_val_pred_list[i])\n",
        "  testing_report = classification_report(y_test, y_test_pred_list[i])\n",
        "\n",
        "  training_accuracy_sk = accuracy_score(y_train_list[i], y_train_pred_list_sk[i])\n",
        "  validation_accuracy_sk = accuracy_score(y_val_list[i], y_val_pred_list_sk[i])\n",
        "  testing_accuracy_sk = accuracy_score(y_test, y_test_pred_list_sk[i])\n",
        "\n",
        "  training_report_sk = classification_report(y_train_list[i], y_train_pred_list_sk[i])\n",
        "  validation_report_sk = classification_report(y_val_list[i], y_val_pred_list_sk[i])\n",
        "  testing_report_sk = classification_report(y_test, y_test_pred_list_sk[i])\n",
        "\n",
        "  print(f\"*************************************************************************************Fold{i+1}*********************************************************************************************\")\n",
        "  print(\"Multi layer Perceptron from Scratch Model\")\n",
        "  print(f\"Training Accuracy fold{i+1} scratch model: {training_accuracy:.3f}\")\n",
        "  print(f\"Validation Accuracy fold{i+1} scratch model: {validation_accuracy:.3f}\")\n",
        "  print(f\"Testing Accuracy fold{i+1} scratch model: {testing_accuracy:.3f}\")\n",
        "  print(\"Classification Report training scratch model:\\n\", training_report)\n",
        "  print(\"Classification Report validation scratch model:\\n\", validation_report)\n",
        "  print(\"Classification Report testing scratch model:\\n\", testing_report)\n",
        "  print(\"Sklearn Multi layer Perceptron Model\")\n",
        "  print(f\"Training Accuracy fold{i+1} Sklearn model: {training_accuracy_sk:.3f}\")\n",
        "  print(f\"Validation Accuracy fold{i+1} Sklearn model: {validation_accuracy_sk:.3f}\")\n",
        "  print(f\"Testing Accuracy fold{i+1} Sklearn model: {testing_accuracy_sk:.3f}\")\n",
        "  print(\"Classification Report training Sklearn model:\\n\", training_report_sk)\n",
        "  print(\"Classification Report validation Sklearn model:\\n\", validation_report_sk)\n",
        "  print(\"Classification Report testing Sklearn model:\\n\", testing_report_sk)\n",
        "  print(\"**********************************************************************************************************************************************************************************\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX7Vv673mQOB"
      },
      "source": [
        "#Statistical Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "1i_gbv83oBVF",
        "outputId": "45773e31-19f3-403d-aecf-7a9a8bc436b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "2178/2178 [==============================] - 6s 2ms/step - loss: 0.5611 - accuracy: 0.7690 - val_loss: 0.4635 - val_accuracy: 0.8249\n",
            "Epoch 2/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4516 - accuracy: 0.8260 - val_loss: 0.4391 - val_accuracy: 0.8285\n",
            "Epoch 3/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4362 - accuracy: 0.8278 - val_loss: 0.4295 - val_accuracy: 0.8291\n",
            "Epoch 4/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4293 - accuracy: 0.8284 - val_loss: 0.4244 - val_accuracy: 0.8303\n",
            "Epoch 5/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4254 - accuracy: 0.8287 - val_loss: 0.4216 - val_accuracy: 0.8298\n",
            "Epoch 6/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4229 - accuracy: 0.8287 - val_loss: 0.4193 - val_accuracy: 0.8306\n",
            "Epoch 7/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4210 - accuracy: 0.8289 - val_loss: 0.4176 - val_accuracy: 0.8306\n",
            "Epoch 8/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4195 - accuracy: 0.8291 - val_loss: 0.4166 - val_accuracy: 0.8308\n",
            "Epoch 9/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4184 - accuracy: 0.8291 - val_loss: 0.4154 - val_accuracy: 0.8309\n",
            "Epoch 10/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4175 - accuracy: 0.8292 - val_loss: 0.4146 - val_accuracy: 0.8309\n",
            "Epoch 11/50\n",
            "2178/2178 [==============================] - 6s 3ms/step - loss: 0.4168 - accuracy: 0.8293 - val_loss: 0.4139 - val_accuracy: 0.8307\n",
            "Epoch 12/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4161 - accuracy: 0.8292 - val_loss: 0.4134 - val_accuracy: 0.8310\n",
            "Epoch 13/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4156 - accuracy: 0.8294 - val_loss: 0.4128 - val_accuracy: 0.8309\n",
            "Epoch 14/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4151 - accuracy: 0.8294 - val_loss: 0.4123 - val_accuracy: 0.8310\n",
            "Epoch 15/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4146 - accuracy: 0.8294 - val_loss: 0.4120 - val_accuracy: 0.8308\n",
            "Epoch 16/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4143 - accuracy: 0.8295 - val_loss: 0.4118 - val_accuracy: 0.8311\n",
            "Epoch 17/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4140 - accuracy: 0.8296 - val_loss: 0.4116 - val_accuracy: 0.8306\n",
            "Epoch 18/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4137 - accuracy: 0.8297 - val_loss: 0.4109 - val_accuracy: 0.8309\n",
            "Epoch 19/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4133 - accuracy: 0.8295 - val_loss: 0.4106 - val_accuracy: 0.8311\n",
            "Epoch 20/50\n",
            "2178/2178 [==============================] - 6s 3ms/step - loss: 0.4132 - accuracy: 0.8294 - val_loss: 0.4108 - val_accuracy: 0.8308\n",
            "Epoch 21/50\n",
            "2178/2178 [==============================] - 8s 4ms/step - loss: 0.4130 - accuracy: 0.8296 - val_loss: 0.4104 - val_accuracy: 0.8310\n",
            "Epoch 22/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4127 - accuracy: 0.8295 - val_loss: 0.4102 - val_accuracy: 0.8307\n",
            "Epoch 23/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4124 - accuracy: 0.8297 - val_loss: 0.4102 - val_accuracy: 0.8312\n",
            "Epoch 24/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4123 - accuracy: 0.8296 - val_loss: 0.4098 - val_accuracy: 0.8308\n",
            "Epoch 25/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4122 - accuracy: 0.8297 - val_loss: 0.4096 - val_accuracy: 0.8308\n",
            "Epoch 26/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4120 - accuracy: 0.8297 - val_loss: 0.4094 - val_accuracy: 0.8308\n",
            "Epoch 27/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4118 - accuracy: 0.8295 - val_loss: 0.4091 - val_accuracy: 0.8308\n",
            "Epoch 28/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4117 - accuracy: 0.8296 - val_loss: 0.4093 - val_accuracy: 0.8310\n",
            "Epoch 29/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4116 - accuracy: 0.8297 - val_loss: 0.4094 - val_accuracy: 0.8309\n",
            "Epoch 30/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4114 - accuracy: 0.8296 - val_loss: 0.4090 - val_accuracy: 0.8313\n",
            "Epoch 31/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4113 - accuracy: 0.8296 - val_loss: 0.4090 - val_accuracy: 0.8306\n",
            "Epoch 32/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4112 - accuracy: 0.8296 - val_loss: 0.4089 - val_accuracy: 0.8312\n",
            "Epoch 33/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4111 - accuracy: 0.8297 - val_loss: 0.4088 - val_accuracy: 0.8307\n",
            "Epoch 34/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4110 - accuracy: 0.8297 - val_loss: 0.4083 - val_accuracy: 0.8312\n",
            "Epoch 35/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4109 - accuracy: 0.8296 - val_loss: 0.4083 - val_accuracy: 0.8309\n",
            "Epoch 36/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4108 - accuracy: 0.8297 - val_loss: 0.4080 - val_accuracy: 0.8312\n",
            "Epoch 37/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4107 - accuracy: 0.8297 - val_loss: 0.4081 - val_accuracy: 0.8311\n",
            "Epoch 38/50\n",
            "2178/2178 [==============================] - 6s 3ms/step - loss: 0.4106 - accuracy: 0.8298 - val_loss: 0.4082 - val_accuracy: 0.8315\n",
            "Epoch 39/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4106 - accuracy: 0.8297 - val_loss: 0.4083 - val_accuracy: 0.8307\n",
            "Epoch 40/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4105 - accuracy: 0.8298 - val_loss: 0.4081 - val_accuracy: 0.8312\n",
            "Epoch 41/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4104 - accuracy: 0.8299 - val_loss: 0.4080 - val_accuracy: 0.8315\n",
            "Epoch 42/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4103 - accuracy: 0.8298 - val_loss: 0.4080 - val_accuracy: 0.8310\n",
            "Epoch 43/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4103 - accuracy: 0.8297 - val_loss: 0.4075 - val_accuracy: 0.8311\n",
            "Epoch 44/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4102 - accuracy: 0.8297 - val_loss: 0.4075 - val_accuracy: 0.8314\n",
            "Epoch 45/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4101 - accuracy: 0.8298 - val_loss: 0.4079 - val_accuracy: 0.8315\n",
            "Epoch 46/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4101 - accuracy: 0.8298 - val_loss: 0.4076 - val_accuracy: 0.8312\n",
            "Epoch 47/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4099 - accuracy: 0.8298 - val_loss: 0.4073 - val_accuracy: 0.8311\n",
            "Epoch 48/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4099 - accuracy: 0.8298 - val_loss: 0.4075 - val_accuracy: 0.8308\n",
            "Epoch 49/50\n",
            "2178/2178 [==============================] - 6s 3ms/step - loss: 0.4099 - accuracy: 0.8297 - val_loss: 0.4075 - val_accuracy: 0.8315\n",
            "Epoch 50/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4098 - accuracy: 0.8299 - val_loss: 0.4072 - val_accuracy: 0.8307\n",
            "7292/7292 [==============================] - 12s 2ms/step\n",
            "Scratch T-statistic with ground truth: [-57.34564652]\n",
            "Scratch P-value with ground truth: [0.]\n",
            "Iteration 1, loss = 0.41523111\n",
            "Validation score: 0.829293\n",
            "Iteration 2, loss = 0.40651111\n",
            "Validation score: 0.830128\n",
            "Iteration 3, loss = 0.40531375\n",
            "Validation score: 0.829862\n",
            "Iteration 4, loss = 0.40469527\n",
            "Validation score: 0.829807\n",
            "Iteration 5, loss = 0.40425038\n",
            "Validation score: 0.829853\n",
            "Iteration 6, loss = 0.40412947\n",
            "Validation score: 0.830275\n",
            "Iteration 7, loss = 0.40386203\n",
            "Validation score: 0.830119\n",
            "Iteration 8, loss = 0.40367762\n",
            "Validation score: 0.829081\n",
            "Iteration 9, loss = 0.40349845\n",
            "Validation score: 0.830312\n",
            "Iteration 10, loss = 0.40344156\n",
            "Validation score: 0.830753\n",
            "Iteration 11, loss = 0.40329769\n",
            "Validation score: 0.830551\n",
            "Iteration 12, loss = 0.40331912\n",
            "Validation score: 0.830349\n",
            "Iteration 13, loss = 0.40323335\n",
            "Validation score: 0.830275\n",
            "Iteration 14, loss = 0.40312011\n",
            "Validation score: 0.830110\n",
            "Iteration 15, loss = 0.40306140\n",
            "Validation score: 0.830358\n",
            "Iteration 16, loss = 0.40296763\n",
            "Validation score: 0.830018\n",
            "Iteration 17, loss = 0.40283011\n",
            "Validation score: 0.830358\n",
            "Iteration 18, loss = 0.40286200\n",
            "Validation score: 0.830367\n",
            "Iteration 19, loss = 0.40281870\n",
            "Validation score: 0.829715\n",
            "Iteration 20, loss = 0.40284933\n",
            "Validation score: 0.829945\n",
            "Iteration 21, loss = 0.40271538\n",
            "Validation score: 0.830514\n",
            "Validation score did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Scratch T-statistic with ground truth: -63.60629243357312\n",
            "Scratch P-value with ground truth: 0.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import regularizers\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "y_train = np.array(y_train).astype(int)\n",
        "y_test = np.array(y_test).astype(int)\n",
        "\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# From Scratch Model\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(23, kernel_regularizer=regularizers.l2(0.001), activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu'),\n",
        "    tf.keras.layers.Dense(8, kernel_regularizer=regularizers.l2(0.001), activation='relu'),\n",
        "    tf.keras.layers.Dense(4, kernel_regularizer=regularizers.l2(0.001), activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=200, validation_split=0.2)\n",
        "\n",
        "threshold = 0.5\n",
        "\n",
        "y_pred_scratch = model.predict(X_test)\n",
        "y_pred_scratch = np.array(y_pred_scratch) >= threshold\n",
        "\n",
        "t_statistic, p_value = ttest_ind(y_pred_scratch, y_test)\n",
        "\n",
        "print(\"Scratch T-statistic with ground truth:\", t_statistic)\n",
        "print(\"Scratch P-value with ground truth:\", p_value)\n",
        "\n",
        "# From Sklearn Model\n",
        "\n",
        "model_sk = MLPClassifier(hidden_layer_sizes=(16,8,4), tol=0.00001, alpha=0.0002, batch_size='auto', learning_rate='constant', learning_rate_init=0.004, validation_fraction=0.2, verbose = True, early_stopping=True, activation='relu', solver='adam', max_iter=200)\n",
        "model_history = model_sk.fit(X_train,y_train)\n",
        "\n",
        "y_pred_sk = model_sk.predict(X_test)\n",
        "\n",
        "t_statistic_sk, p_value_sk = ttest_ind(y_pred_sk, y_test)\n",
        "\n",
        "print(\"Scratch T-statistic with ground truth:\", t_statistic_sk)\n",
        "print(\"Scratch P-value with ground truth:\", p_value_sk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ET6OtQM8h0sL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Gg9gcaPSlTcs",
        "JOgeTsMRkkrU",
        "TX7Vv673mQOB"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
