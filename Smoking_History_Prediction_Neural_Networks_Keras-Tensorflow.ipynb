{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcaevIiW3ZHJ"
      },
      "source": [
        "#Loading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCP8K9O_5VDn",
        "outputId": "b6f833d8-c2e6-44b9-cf48-72be5868bdf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "2lh9Q5vm4DaH",
        "outputId": "517ed71d-7b54-45f5-9e20-eb79ef111a60"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>waistline</th>\n",
              "      <th>sight_left</th>\n",
              "      <th>sight_right</th>\n",
              "      <th>hear_left</th>\n",
              "      <th>hear_right</th>\n",
              "      <th>SBP</th>\n",
              "      <th>...</th>\n",
              "      <th>LDL_chole</th>\n",
              "      <th>triglyceride</th>\n",
              "      <th>hemoglobin</th>\n",
              "      <th>urine_protein</th>\n",
              "      <th>serum_creatinine</th>\n",
              "      <th>SGOT_AST</th>\n",
              "      <th>SGOT_ALT</th>\n",
              "      <th>gamma_GTP</th>\n",
              "      <th>SMK_stat_type_cd</th>\n",
              "      <th>DRK_YN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Male</td>\n",
              "      <td>35</td>\n",
              "      <td>170</td>\n",
              "      <td>75</td>\n",
              "      <td>90.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>...</td>\n",
              "      <td>126.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>17.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Male</td>\n",
              "      <td>30</td>\n",
              "      <td>180</td>\n",
              "      <td>80</td>\n",
              "      <td>89.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>...</td>\n",
              "      <td>148.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>20.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Male</td>\n",
              "      <td>40</td>\n",
              "      <td>165</td>\n",
              "      <td>75</td>\n",
              "      <td>91.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>...</td>\n",
              "      <td>74.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>47.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Male</td>\n",
              "      <td>50</td>\n",
              "      <td>175</td>\n",
              "      <td>80</td>\n",
              "      <td>91.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>...</td>\n",
              "      <td>104.0</td>\n",
              "      <td>106.0</td>\n",
              "      <td>17.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>29.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Male</td>\n",
              "      <td>50</td>\n",
              "      <td>165</td>\n",
              "      <td>60</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>...</td>\n",
              "      <td>117.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>13.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>19.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991341</th>\n",
              "      <td>Male</td>\n",
              "      <td>45</td>\n",
              "      <td>175</td>\n",
              "      <td>80</td>\n",
              "      <td>92.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>...</td>\n",
              "      <td>125.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991342</th>\n",
              "      <td>Male</td>\n",
              "      <td>35</td>\n",
              "      <td>170</td>\n",
              "      <td>75</td>\n",
              "      <td>86.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>...</td>\n",
              "      <td>84.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>14.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991343</th>\n",
              "      <td>Female</td>\n",
              "      <td>40</td>\n",
              "      <td>155</td>\n",
              "      <td>50</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>...</td>\n",
              "      <td>77.0</td>\n",
              "      <td>157.0</td>\n",
              "      <td>14.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>30.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991344</th>\n",
              "      <td>Male</td>\n",
              "      <td>25</td>\n",
              "      <td>175</td>\n",
              "      <td>60</td>\n",
              "      <td>72.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>...</td>\n",
              "      <td>73.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>14.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>21.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991345</th>\n",
              "      <td>Male</td>\n",
              "      <td>50</td>\n",
              "      <td>160</td>\n",
              "      <td>70</td>\n",
              "      <td>90.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>...</td>\n",
              "      <td>153.0</td>\n",
              "      <td>163.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>24.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Y</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>991346 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           sex  age  height  weight  waistline  sight_left  sight_right  \\\n",
              "0         Male   35     170      75       90.0         1.0          1.0   \n",
              "1         Male   30     180      80       89.0         0.9          1.2   \n",
              "2         Male   40     165      75       91.0         1.2          1.5   \n",
              "3         Male   50     175      80       91.0         1.5          1.2   \n",
              "4         Male   50     165      60       80.0         1.0          1.2   \n",
              "...        ...  ...     ...     ...        ...         ...          ...   \n",
              "991341    Male   45     175      80       92.1         1.5          1.5   \n",
              "991342    Male   35     170      75       86.0         1.0          1.5   \n",
              "991343  Female   40     155      50       68.0         1.0          0.7   \n",
              "991344    Male   25     175      60       72.0         1.5          1.0   \n",
              "991345    Male   50     160      70       90.5         1.0          1.5   \n",
              "\n",
              "        hear_left  hear_right    SBP  ...  LDL_chole  triglyceride  \\\n",
              "0             1.0         1.0  120.0  ...      126.0          92.0   \n",
              "1             1.0         1.0  130.0  ...      148.0         121.0   \n",
              "2             1.0         1.0  120.0  ...       74.0         104.0   \n",
              "3             1.0         1.0  145.0  ...      104.0         106.0   \n",
              "4             1.0         1.0  138.0  ...      117.0         104.0   \n",
              "...           ...         ...    ...  ...        ...           ...   \n",
              "991341        1.0         1.0  114.0  ...      125.0         132.0   \n",
              "991342        1.0         1.0  119.0  ...       84.0          45.0   \n",
              "991343        1.0         1.0  110.0  ...       77.0         157.0   \n",
              "991344        1.0         1.0  119.0  ...       73.0          53.0   \n",
              "991345        1.0         1.0  133.0  ...      153.0         163.0   \n",
              "\n",
              "        hemoglobin  urine_protein  serum_creatinine  SGOT_AST  SGOT_ALT  \\\n",
              "0             17.1            1.0               1.0      21.0      35.0   \n",
              "1             15.8            1.0               0.9      20.0      36.0   \n",
              "2             15.8            1.0               0.9      47.0      32.0   \n",
              "3             17.6            1.0               1.1      29.0      34.0   \n",
              "4             13.8            1.0               0.8      19.0      12.0   \n",
              "...            ...            ...               ...       ...       ...   \n",
              "991341        15.0            1.0               1.0      26.0      36.0   \n",
              "991342        15.8            1.0               1.1      14.0      17.0   \n",
              "991343        14.3            1.0               0.8      30.0      27.0   \n",
              "991344        14.5            1.0               0.8      21.0      14.0   \n",
              "991345        15.8            1.0               0.9      24.0      43.0   \n",
              "\n",
              "        gamma_GTP  SMK_stat_type_cd  DRK_YN  \n",
              "0            40.0               1.0       Y  \n",
              "1            27.0               3.0       N  \n",
              "2            68.0               1.0       N  \n",
              "3            18.0               1.0       N  \n",
              "4            25.0               1.0       N  \n",
              "...           ...               ...     ...  \n",
              "991341       27.0               1.0       N  \n",
              "991342       15.0               1.0       N  \n",
              "991343       17.0               3.0       Y  \n",
              "991344       17.0               1.0       N  \n",
              "991345       36.0               3.0       Y  \n",
              "\n",
              "[991346 rows x 24 columns]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/DMT-CS235-Project/smoking_driking_dataset.csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "Dg_TbUXazOHP",
        "outputId": "7d6c118b-b0bb-41df-90a0-6bf882415714"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>waistline</th>\n",
              "      <th>sight_left</th>\n",
              "      <th>sight_right</th>\n",
              "      <th>hear_left</th>\n",
              "      <th>hear_right</th>\n",
              "      <th>SBP</th>\n",
              "      <th>DBP</th>\n",
              "      <th>...</th>\n",
              "      <th>HDL_chole</th>\n",
              "      <th>LDL_chole</th>\n",
              "      <th>triglyceride</th>\n",
              "      <th>hemoglobin</th>\n",
              "      <th>urine_protein</th>\n",
              "      <th>serum_creatinine</th>\n",
              "      <th>SGOT_AST</th>\n",
              "      <th>SGOT_ALT</th>\n",
              "      <th>gamma_GTP</th>\n",
              "      <th>SMK_stat_type_cd</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>47.614491</td>\n",
              "      <td>162.240625</td>\n",
              "      <td>63.284050</td>\n",
              "      <td>81.233358</td>\n",
              "      <td>0.980834</td>\n",
              "      <td>0.978429</td>\n",
              "      <td>1.031495</td>\n",
              "      <td>1.030476</td>\n",
              "      <td>122.432498</td>\n",
              "      <td>76.052627</td>\n",
              "      <td>...</td>\n",
              "      <td>56.936800</td>\n",
              "      <td>113.037692</td>\n",
              "      <td>132.141751</td>\n",
              "      <td>14.229824</td>\n",
              "      <td>1.094224</td>\n",
              "      <td>0.860467</td>\n",
              "      <td>25.989308</td>\n",
              "      <td>25.755051</td>\n",
              "      <td>37.136347</td>\n",
              "      <td>1.608122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>14.181339</td>\n",
              "      <td>9.282957</td>\n",
              "      <td>12.514241</td>\n",
              "      <td>11.850323</td>\n",
              "      <td>0.605949</td>\n",
              "      <td>0.604774</td>\n",
              "      <td>0.174650</td>\n",
              "      <td>0.171892</td>\n",
              "      <td>14.543148</td>\n",
              "      <td>9.889365</td>\n",
              "      <td>...</td>\n",
              "      <td>17.238479</td>\n",
              "      <td>35.842812</td>\n",
              "      <td>102.196985</td>\n",
              "      <td>1.584929</td>\n",
              "      <td>0.437724</td>\n",
              "      <td>0.480530</td>\n",
              "      <td>23.493386</td>\n",
              "      <td>26.308599</td>\n",
              "      <td>50.424153</td>\n",
              "      <td>0.818507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>20.000000</td>\n",
              "      <td>130.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>35.000000</td>\n",
              "      <td>155.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>74.100000</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>70.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>46.000000</td>\n",
              "      <td>89.000000</td>\n",
              "      <td>73.000000</td>\n",
              "      <td>13.200000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>45.000000</td>\n",
              "      <td>160.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>81.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>120.000000</td>\n",
              "      <td>76.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>111.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>14.300000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>60.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>70.000000</td>\n",
              "      <td>87.800000</td>\n",
              "      <td>1.200000</td>\n",
              "      <td>1.200000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>131.000000</td>\n",
              "      <td>82.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>66.000000</td>\n",
              "      <td>135.000000</td>\n",
              "      <td>159.000000</td>\n",
              "      <td>15.400000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>85.000000</td>\n",
              "      <td>190.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>9.900000</td>\n",
              "      <td>9.900000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>273.000000</td>\n",
              "      <td>185.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>8110.000000</td>\n",
              "      <td>5119.000000</td>\n",
              "      <td>9490.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>98.000000</td>\n",
              "      <td>9999.000000</td>\n",
              "      <td>7210.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 22 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 age         height         weight      waistline  \\\n",
              "count  991346.000000  991346.000000  991346.000000  991346.000000   \n",
              "mean       47.614491     162.240625      63.284050      81.233358   \n",
              "std        14.181339       9.282957      12.514241      11.850323   \n",
              "min        20.000000     130.000000      25.000000       8.000000   \n",
              "25%        35.000000     155.000000      55.000000      74.100000   \n",
              "50%        45.000000     160.000000      60.000000      81.000000   \n",
              "75%        60.000000     170.000000      70.000000      87.800000   \n",
              "max        85.000000     190.000000     140.000000     999.000000   \n",
              "\n",
              "          sight_left    sight_right      hear_left     hear_right  \\\n",
              "count  991346.000000  991346.000000  991346.000000  991346.000000   \n",
              "mean        0.980834       0.978429       1.031495       1.030476   \n",
              "std         0.605949       0.604774       0.174650       0.171892   \n",
              "min         0.100000       0.100000       1.000000       1.000000   \n",
              "25%         0.700000       0.700000       1.000000       1.000000   \n",
              "50%         1.000000       1.000000       1.000000       1.000000   \n",
              "75%         1.200000       1.200000       1.000000       1.000000   \n",
              "max         9.900000       9.900000       2.000000       2.000000   \n",
              "\n",
              "                 SBP            DBP  ...      HDL_chole      LDL_chole  \\\n",
              "count  991346.000000  991346.000000  ...  991346.000000  991346.000000   \n",
              "mean      122.432498      76.052627  ...      56.936800     113.037692   \n",
              "std        14.543148       9.889365  ...      17.238479      35.842812   \n",
              "min        67.000000      32.000000  ...       1.000000       1.000000   \n",
              "25%       112.000000      70.000000  ...      46.000000      89.000000   \n",
              "50%       120.000000      76.000000  ...      55.000000     111.000000   \n",
              "75%       131.000000      82.000000  ...      66.000000     135.000000   \n",
              "max       273.000000     185.000000  ...    8110.000000    5119.000000   \n",
              "\n",
              "        triglyceride     hemoglobin  urine_protein  serum_creatinine  \\\n",
              "count  991346.000000  991346.000000  991346.000000     991346.000000   \n",
              "mean      132.141751      14.229824       1.094224          0.860467   \n",
              "std       102.196985       1.584929       0.437724          0.480530   \n",
              "min         1.000000       1.000000       1.000000          0.100000   \n",
              "25%        73.000000      13.200000       1.000000          0.700000   \n",
              "50%       106.000000      14.300000       1.000000          0.800000   \n",
              "75%       159.000000      15.400000       1.000000          1.000000   \n",
              "max      9490.000000      25.000000       6.000000         98.000000   \n",
              "\n",
              "            SGOT_AST       SGOT_ALT      gamma_GTP  SMK_stat_type_cd  \n",
              "count  991346.000000  991346.000000  991346.000000     991346.000000  \n",
              "mean       25.989308      25.755051      37.136347          1.608122  \n",
              "std        23.493386      26.308599      50.424153          0.818507  \n",
              "min         1.000000       1.000000       1.000000          1.000000  \n",
              "25%        19.000000      15.000000      16.000000          1.000000  \n",
              "50%        23.000000      20.000000      23.000000          1.000000  \n",
              "75%        28.000000      29.000000      39.000000          2.000000  \n",
              "max      9999.000000    7210.000000     999.000000          3.000000  \n",
              "\n",
              "[8 rows x 22 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s46MP_Hs5by-"
      },
      "outputs": [],
      "source": [
        "columns = df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg9gcaPSlTcs"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szQT3HIelXJD",
        "outputId": "c57372dc-e6a0-4253-ac48-c52515bb87d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "sex                 0\n",
              "age                 0\n",
              "height              0\n",
              "weight              0\n",
              "waistline           0\n",
              "sight_left          0\n",
              "sight_right         0\n",
              "hear_left           0\n",
              "hear_right          0\n",
              "SBP                 0\n",
              "DBP                 0\n",
              "BLDS                0\n",
              "tot_chole           0\n",
              "HDL_chole           0\n",
              "LDL_chole           0\n",
              "triglyceride        0\n",
              "hemoglobin          0\n",
              "urine_protein       0\n",
              "serum_creatinine    0\n",
              "SGOT_AST            0\n",
              "SGOT_ALT            0\n",
              "gamma_GTP           0\n",
              "SMK_stat_type_cd    0\n",
              "DRK_YN              0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIOT_jDVlYPT",
        "outputId": "19b051ac-aec1-46ff-8ff4-e3a6d57ec243"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "p3Ot7rYOlaJk",
        "outputId": "d387e89f-74ac-4acc-efa6-270da2548572"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>waistline</th>\n",
              "      <th>sight_left</th>\n",
              "      <th>sight_right</th>\n",
              "      <th>hear_left</th>\n",
              "      <th>hear_right</th>\n",
              "      <th>SBP</th>\n",
              "      <th>...</th>\n",
              "      <th>LDL_chole</th>\n",
              "      <th>triglyceride</th>\n",
              "      <th>hemoglobin</th>\n",
              "      <th>urine_protein</th>\n",
              "      <th>serum_creatinine</th>\n",
              "      <th>SGOT_AST</th>\n",
              "      <th>SGOT_ALT</th>\n",
              "      <th>gamma_GTP</th>\n",
              "      <th>SMK_stat_type_cd</th>\n",
              "      <th>DRK_YN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Male</td>\n",
              "      <td>35</td>\n",
              "      <td>170</td>\n",
              "      <td>75</td>\n",
              "      <td>90.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>...</td>\n",
              "      <td>126.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>17.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Male</td>\n",
              "      <td>30</td>\n",
              "      <td>180</td>\n",
              "      <td>80</td>\n",
              "      <td>89.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>...</td>\n",
              "      <td>148.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>20.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Male</td>\n",
              "      <td>40</td>\n",
              "      <td>165</td>\n",
              "      <td>75</td>\n",
              "      <td>91.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>...</td>\n",
              "      <td>74.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>47.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Male</td>\n",
              "      <td>50</td>\n",
              "      <td>175</td>\n",
              "      <td>80</td>\n",
              "      <td>91.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>...</td>\n",
              "      <td>104.0</td>\n",
              "      <td>106.0</td>\n",
              "      <td>17.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>29.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Male</td>\n",
              "      <td>50</td>\n",
              "      <td>165</td>\n",
              "      <td>60</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>...</td>\n",
              "      <td>117.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>13.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>19.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991341</th>\n",
              "      <td>Male</td>\n",
              "      <td>45</td>\n",
              "      <td>175</td>\n",
              "      <td>80</td>\n",
              "      <td>92.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>...</td>\n",
              "      <td>125.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991342</th>\n",
              "      <td>Male</td>\n",
              "      <td>35</td>\n",
              "      <td>170</td>\n",
              "      <td>75</td>\n",
              "      <td>86.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>...</td>\n",
              "      <td>84.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>14.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991343</th>\n",
              "      <td>Female</td>\n",
              "      <td>40</td>\n",
              "      <td>155</td>\n",
              "      <td>50</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>...</td>\n",
              "      <td>77.0</td>\n",
              "      <td>157.0</td>\n",
              "      <td>14.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>30.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991344</th>\n",
              "      <td>Male</td>\n",
              "      <td>25</td>\n",
              "      <td>175</td>\n",
              "      <td>60</td>\n",
              "      <td>72.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>...</td>\n",
              "      <td>73.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>14.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>21.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991345</th>\n",
              "      <td>Male</td>\n",
              "      <td>50</td>\n",
              "      <td>160</td>\n",
              "      <td>70</td>\n",
              "      <td>90.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>...</td>\n",
              "      <td>153.0</td>\n",
              "      <td>163.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>24.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Y</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>991320 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           sex  age  height  weight  waistline  sight_left  sight_right  \\\n",
              "0         Male   35     170      75       90.0         1.0          1.0   \n",
              "1         Male   30     180      80       89.0         0.9          1.2   \n",
              "2         Male   40     165      75       91.0         1.2          1.5   \n",
              "3         Male   50     175      80       91.0         1.5          1.2   \n",
              "4         Male   50     165      60       80.0         1.0          1.2   \n",
              "...        ...  ...     ...     ...        ...         ...          ...   \n",
              "991341    Male   45     175      80       92.1         1.5          1.5   \n",
              "991342    Male   35     170      75       86.0         1.0          1.5   \n",
              "991343  Female   40     155      50       68.0         1.0          0.7   \n",
              "991344    Male   25     175      60       72.0         1.5          1.0   \n",
              "991345    Male   50     160      70       90.5         1.0          1.5   \n",
              "\n",
              "        hear_left  hear_right    SBP  ...  LDL_chole  triglyceride  \\\n",
              "0             1.0         1.0  120.0  ...      126.0          92.0   \n",
              "1             1.0         1.0  130.0  ...      148.0         121.0   \n",
              "2             1.0         1.0  120.0  ...       74.0         104.0   \n",
              "3             1.0         1.0  145.0  ...      104.0         106.0   \n",
              "4             1.0         1.0  138.0  ...      117.0         104.0   \n",
              "...           ...         ...    ...  ...        ...           ...   \n",
              "991341        1.0         1.0  114.0  ...      125.0         132.0   \n",
              "991342        1.0         1.0  119.0  ...       84.0          45.0   \n",
              "991343        1.0         1.0  110.0  ...       77.0         157.0   \n",
              "991344        1.0         1.0  119.0  ...       73.0          53.0   \n",
              "991345        1.0         1.0  133.0  ...      153.0         163.0   \n",
              "\n",
              "        hemoglobin  urine_protein  serum_creatinine  SGOT_AST  SGOT_ALT  \\\n",
              "0             17.1            1.0               1.0      21.0      35.0   \n",
              "1             15.8            1.0               0.9      20.0      36.0   \n",
              "2             15.8            1.0               0.9      47.0      32.0   \n",
              "3             17.6            1.0               1.1      29.0      34.0   \n",
              "4             13.8            1.0               0.8      19.0      12.0   \n",
              "...            ...            ...               ...       ...       ...   \n",
              "991341        15.0            1.0               1.0      26.0      36.0   \n",
              "991342        15.8            1.0               1.1      14.0      17.0   \n",
              "991343        14.3            1.0               0.8      30.0      27.0   \n",
              "991344        14.5            1.0               0.8      21.0      14.0   \n",
              "991345        15.8            1.0               0.9      24.0      43.0   \n",
              "\n",
              "        gamma_GTP  SMK_stat_type_cd  DRK_YN  \n",
              "0            40.0               1.0       Y  \n",
              "1            27.0               3.0       N  \n",
              "2            68.0               1.0       N  \n",
              "3            18.0               1.0       N  \n",
              "4            25.0               1.0       N  \n",
              "...           ...               ...     ...  \n",
              "991341       27.0               1.0       N  \n",
              "991342       15.0               1.0       N  \n",
              "991343       17.0               3.0       Y  \n",
              "991344       17.0               1.0       N  \n",
              "991345       36.0               3.0       Y  \n",
              "\n",
              "[991320 rows x 24 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "2MQlf5lzTN1f",
        "outputId": "175be87c-8259-4e7c-9f7c-2d262fe63509"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>waistline</th>\n",
              "      <th>sight_left</th>\n",
              "      <th>sight_right</th>\n",
              "      <th>hear_left</th>\n",
              "      <th>hear_right</th>\n",
              "      <th>SBP</th>\n",
              "      <th>...</th>\n",
              "      <th>LDL_chole</th>\n",
              "      <th>triglyceride</th>\n",
              "      <th>hemoglobin</th>\n",
              "      <th>urine_protein</th>\n",
              "      <th>serum_creatinine</th>\n",
              "      <th>SGOT_AST</th>\n",
              "      <th>SGOT_ALT</th>\n",
              "      <th>gamma_GTP</th>\n",
              "      <th>SMK_stat_type_cd</th>\n",
              "      <th>DRK_YN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>35</td>\n",
              "      <td>170</td>\n",
              "      <td>75</td>\n",
              "      <td>90.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>...</td>\n",
              "      <td>126.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>17.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>30</td>\n",
              "      <td>180</td>\n",
              "      <td>80</td>\n",
              "      <td>89.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>...</td>\n",
              "      <td>148.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>20.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>40</td>\n",
              "      <td>165</td>\n",
              "      <td>75</td>\n",
              "      <td>91.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>...</td>\n",
              "      <td>74.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>47.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>175</td>\n",
              "      <td>80</td>\n",
              "      <td>91.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>...</td>\n",
              "      <td>104.0</td>\n",
              "      <td>106.0</td>\n",
              "      <td>17.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>29.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>165</td>\n",
              "      <td>60</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>...</td>\n",
              "      <td>117.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>13.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>19.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991341</th>\n",
              "      <td>1</td>\n",
              "      <td>45</td>\n",
              "      <td>175</td>\n",
              "      <td>80</td>\n",
              "      <td>92.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>...</td>\n",
              "      <td>125.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991342</th>\n",
              "      <td>1</td>\n",
              "      <td>35</td>\n",
              "      <td>170</td>\n",
              "      <td>75</td>\n",
              "      <td>86.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>...</td>\n",
              "      <td>84.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>14.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991343</th>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>155</td>\n",
              "      <td>50</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>...</td>\n",
              "      <td>77.0</td>\n",
              "      <td>157.0</td>\n",
              "      <td>14.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>30.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991344</th>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>175</td>\n",
              "      <td>60</td>\n",
              "      <td>72.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>...</td>\n",
              "      <td>73.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>14.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>21.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991345</th>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>160</td>\n",
              "      <td>70</td>\n",
              "      <td>90.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>...</td>\n",
              "      <td>153.0</td>\n",
              "      <td>163.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>24.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>991346 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        sex  age  height  weight  waistline  sight_left  sight_right  \\\n",
              "0         1   35     170      75       90.0         1.0          1.0   \n",
              "1         1   30     180      80       89.0         0.9          1.2   \n",
              "2         1   40     165      75       91.0         1.2          1.5   \n",
              "3         1   50     175      80       91.0         1.5          1.2   \n",
              "4         1   50     165      60       80.0         1.0          1.2   \n",
              "...     ...  ...     ...     ...        ...         ...          ...   \n",
              "991341    1   45     175      80       92.1         1.5          1.5   \n",
              "991342    1   35     170      75       86.0         1.0          1.5   \n",
              "991343    0   40     155      50       68.0         1.0          0.7   \n",
              "991344    1   25     175      60       72.0         1.5          1.0   \n",
              "991345    1   50     160      70       90.5         1.0          1.5   \n",
              "\n",
              "        hear_left  hear_right    SBP  ...  LDL_chole  triglyceride  \\\n",
              "0             1.0         1.0  120.0  ...      126.0          92.0   \n",
              "1             1.0         1.0  130.0  ...      148.0         121.0   \n",
              "2             1.0         1.0  120.0  ...       74.0         104.0   \n",
              "3             1.0         1.0  145.0  ...      104.0         106.0   \n",
              "4             1.0         1.0  138.0  ...      117.0         104.0   \n",
              "...           ...         ...    ...  ...        ...           ...   \n",
              "991341        1.0         1.0  114.0  ...      125.0         132.0   \n",
              "991342        1.0         1.0  119.0  ...       84.0          45.0   \n",
              "991343        1.0         1.0  110.0  ...       77.0         157.0   \n",
              "991344        1.0         1.0  119.0  ...       73.0          53.0   \n",
              "991345        1.0         1.0  133.0  ...      153.0         163.0   \n",
              "\n",
              "        hemoglobin  urine_protein  serum_creatinine  SGOT_AST  SGOT_ALT  \\\n",
              "0             17.1            1.0               1.0      21.0      35.0   \n",
              "1             15.8            1.0               0.9      20.0      36.0   \n",
              "2             15.8            1.0               0.9      47.0      32.0   \n",
              "3             17.6            1.0               1.1      29.0      34.0   \n",
              "4             13.8            1.0               0.8      19.0      12.0   \n",
              "...            ...            ...               ...       ...       ...   \n",
              "991341        15.0            1.0               1.0      26.0      36.0   \n",
              "991342        15.8            1.0               1.1      14.0      17.0   \n",
              "991343        14.3            1.0               0.8      30.0      27.0   \n",
              "991344        14.5            1.0               0.8      21.0      14.0   \n",
              "991345        15.8            1.0               0.9      24.0      43.0   \n",
              "\n",
              "        gamma_GTP  SMK_stat_type_cd  DRK_YN  \n",
              "0            40.0               1.0       1  \n",
              "1            27.0               3.0       0  \n",
              "2            68.0               1.0       0  \n",
              "3            18.0               1.0       0  \n",
              "4            25.0               1.0       0  \n",
              "...           ...               ...     ...  \n",
              "991341       27.0               1.0       0  \n",
              "991342       15.0               1.0       0  \n",
              "991343       17.0               3.0       1  \n",
              "991344       17.0               1.0       0  \n",
              "991345       36.0               3.0       1  \n",
              "\n",
              "[991346 rows x 24 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "le = LabelEncoder()\n",
        "df[\"sex\"] = le.fit_transform(df[\"sex\"])\n",
        "df[\"DRK_YN\"] = le.fit_transform(df[\"DRK_YN\"])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "v_s--iVCSj9L",
        "outputId": "c50d4392-fc98-4591-80f2-bd8500ca38dd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>waistline</th>\n",
              "      <th>sight_left</th>\n",
              "      <th>sight_right</th>\n",
              "      <th>hear_left</th>\n",
              "      <th>hear_right</th>\n",
              "      <th>SBP</th>\n",
              "      <th>...</th>\n",
              "      <th>LDL_chole</th>\n",
              "      <th>triglyceride</th>\n",
              "      <th>hemoglobin</th>\n",
              "      <th>urine_protein</th>\n",
              "      <th>serum_creatinine</th>\n",
              "      <th>SGOT_AST</th>\n",
              "      <th>SGOT_ALT</th>\n",
              "      <th>gamma_GTP</th>\n",
              "      <th>SMK_stat_type_cd</th>\n",
              "      <th>DRK_YN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>...</td>\n",
              "      <td>126.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>17.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>180.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>...</td>\n",
              "      <td>148.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>20.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>...</td>\n",
              "      <td>74.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>47.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>...</td>\n",
              "      <td>104.0</td>\n",
              "      <td>106.0</td>\n",
              "      <td>17.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>29.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>...</td>\n",
              "      <td>117.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>13.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>19.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991341</th>\n",
              "      <td>1.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>92.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>...</td>\n",
              "      <td>125.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991342</th>\n",
              "      <td>1.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>...</td>\n",
              "      <td>84.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>14.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991343</th>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>155.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>...</td>\n",
              "      <td>77.0</td>\n",
              "      <td>157.0</td>\n",
              "      <td>14.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>30.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991344</th>\n",
              "      <td>1.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>...</td>\n",
              "      <td>73.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>14.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>21.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991345</th>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>90.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>...</td>\n",
              "      <td>153.0</td>\n",
              "      <td>163.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>24.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>991346 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        sex   age  height  weight  waistline  sight_left  sight_right  \\\n",
              "0       1.0  35.0   170.0    75.0       90.0         1.0          1.0   \n",
              "1       1.0  30.0   180.0    80.0       89.0         0.9          1.2   \n",
              "2       1.0  40.0   165.0    75.0       91.0         1.2          1.5   \n",
              "3       1.0  50.0   175.0    80.0       91.0         1.5          1.2   \n",
              "4       1.0  50.0   165.0    60.0       80.0         1.0          1.2   \n",
              "...     ...   ...     ...     ...        ...         ...          ...   \n",
              "991341  1.0  45.0   175.0    80.0       92.1         1.5          1.5   \n",
              "991342  1.0  35.0   170.0    75.0       86.0         1.0          1.5   \n",
              "991343  0.0  40.0   155.0    50.0       68.0         1.0          0.7   \n",
              "991344  1.0  25.0   175.0    60.0       72.0         1.5          1.0   \n",
              "991345  1.0  50.0   160.0    70.0       90.5         1.0          1.5   \n",
              "\n",
              "        hear_left  hear_right    SBP  ...  LDL_chole  triglyceride  \\\n",
              "0             1.0         1.0  120.0  ...      126.0          92.0   \n",
              "1             1.0         1.0  130.0  ...      148.0         121.0   \n",
              "2             1.0         1.0  120.0  ...       74.0         104.0   \n",
              "3             1.0         1.0  145.0  ...      104.0         106.0   \n",
              "4             1.0         1.0  138.0  ...      117.0         104.0   \n",
              "...           ...         ...    ...  ...        ...           ...   \n",
              "991341        1.0         1.0  114.0  ...      125.0         132.0   \n",
              "991342        1.0         1.0  119.0  ...       84.0          45.0   \n",
              "991343        1.0         1.0  110.0  ...       77.0         157.0   \n",
              "991344        1.0         1.0  119.0  ...       73.0          53.0   \n",
              "991345        1.0         1.0  133.0  ...      153.0         163.0   \n",
              "\n",
              "        hemoglobin  urine_protein  serum_creatinine  SGOT_AST  SGOT_ALT  \\\n",
              "0             17.1            1.0               1.0      21.0      35.0   \n",
              "1             15.8            1.0               0.9      20.0      36.0   \n",
              "2             15.8            1.0               0.9      47.0      32.0   \n",
              "3             17.6            1.0               1.1      29.0      34.0   \n",
              "4             13.8            1.0               0.8      19.0      12.0   \n",
              "...            ...            ...               ...       ...       ...   \n",
              "991341        15.0            1.0               1.0      26.0      36.0   \n",
              "991342        15.8            1.0               1.1      14.0      17.0   \n",
              "991343        14.3            1.0               0.8      30.0      27.0   \n",
              "991344        14.5            1.0               0.8      21.0      14.0   \n",
              "991345        15.8            1.0               0.9      24.0      43.0   \n",
              "\n",
              "        gamma_GTP  SMK_stat_type_cd  DRK_YN  \n",
              "0            40.0               1.0     1.0  \n",
              "1            27.0               3.0     0.0  \n",
              "2            68.0               1.0     0.0  \n",
              "3            18.0               1.0     0.0  \n",
              "4            25.0               1.0     0.0  \n",
              "...           ...               ...     ...  \n",
              "991341       27.0               1.0     0.0  \n",
              "991342       15.0               1.0     0.0  \n",
              "991343       17.0               3.0     1.0  \n",
              "991344       17.0               1.0     0.0  \n",
              "991345       36.0               3.0     1.0  \n",
              "\n",
              "[991346 rows x 24 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for i in df.columns:\n",
        "  df[i] = df[i].astype('float64')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "_d_ud0SiSj9L",
        "outputId": "2fc7f469-e192-404e-cc27-6a22a721a934"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>waistline</th>\n",
              "      <th>sight_left</th>\n",
              "      <th>sight_right</th>\n",
              "      <th>hear_left</th>\n",
              "      <th>hear_right</th>\n",
              "      <th>SBP</th>\n",
              "      <th>...</th>\n",
              "      <th>LDL_chole</th>\n",
              "      <th>triglyceride</th>\n",
              "      <th>hemoglobin</th>\n",
              "      <th>urine_protein</th>\n",
              "      <th>serum_creatinine</th>\n",
              "      <th>SGOT_AST</th>\n",
              "      <th>SGOT_ALT</th>\n",
              "      <th>gamma_GTP</th>\n",
              "      <th>SMK_stat_type_cd</th>\n",
              "      <th>DRK_YN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>...</td>\n",
              "      <td>126.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>17.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>180.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>...</td>\n",
              "      <td>148.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>20.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>...</td>\n",
              "      <td>74.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>47.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>...</td>\n",
              "      <td>104.0</td>\n",
              "      <td>106.0</td>\n",
              "      <td>17.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>29.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>...</td>\n",
              "      <td>117.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>13.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>19.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991341</th>\n",
              "      <td>1.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>92.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>...</td>\n",
              "      <td>125.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991342</th>\n",
              "      <td>1.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>...</td>\n",
              "      <td>84.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>14.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991343</th>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>155.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>...</td>\n",
              "      <td>77.0</td>\n",
              "      <td>157.0</td>\n",
              "      <td>14.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>30.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991344</th>\n",
              "      <td>1.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>...</td>\n",
              "      <td>73.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>14.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>21.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991345</th>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>90.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>...</td>\n",
              "      <td>153.0</td>\n",
              "      <td>163.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>24.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>991346 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        sex   age  height  weight  waistline  sight_left  sight_right  \\\n",
              "0       1.0  35.0   170.0    75.0       90.0         1.0          1.0   \n",
              "1       1.0  30.0   180.0    80.0       89.0         0.9          1.2   \n",
              "2       1.0  40.0   165.0    75.0       91.0         1.2          1.5   \n",
              "3       1.0  50.0   175.0    80.0       91.0         1.5          1.2   \n",
              "4       1.0  50.0   165.0    60.0       80.0         1.0          1.2   \n",
              "...     ...   ...     ...     ...        ...         ...          ...   \n",
              "991341  1.0  45.0   175.0    80.0       92.1         1.5          1.5   \n",
              "991342  1.0  35.0   170.0    75.0       86.0         1.0          1.5   \n",
              "991343  0.0  40.0   155.0    50.0       68.0         1.0          0.7   \n",
              "991344  1.0  25.0   175.0    60.0       72.0         1.5          1.0   \n",
              "991345  1.0  50.0   160.0    70.0       90.5         1.0          1.5   \n",
              "\n",
              "        hear_left  hear_right    SBP  ...  LDL_chole  triglyceride  \\\n",
              "0             1.0         1.0  120.0  ...      126.0          92.0   \n",
              "1             1.0         1.0  130.0  ...      148.0         121.0   \n",
              "2             1.0         1.0  120.0  ...       74.0         104.0   \n",
              "3             1.0         1.0  145.0  ...      104.0         106.0   \n",
              "4             1.0         1.0  138.0  ...      117.0         104.0   \n",
              "...           ...         ...    ...  ...        ...           ...   \n",
              "991341        1.0         1.0  114.0  ...      125.0         132.0   \n",
              "991342        1.0         1.0  119.0  ...       84.0          45.0   \n",
              "991343        1.0         1.0  110.0  ...       77.0         157.0   \n",
              "991344        1.0         1.0  119.0  ...       73.0          53.0   \n",
              "991345        1.0         1.0  133.0  ...      153.0         163.0   \n",
              "\n",
              "        hemoglobin  urine_protein  serum_creatinine  SGOT_AST  SGOT_ALT  \\\n",
              "0             17.1            1.0               1.0      21.0      35.0   \n",
              "1             15.8            1.0               0.9      20.0      36.0   \n",
              "2             15.8            1.0               0.9      47.0      32.0   \n",
              "3             17.6            1.0               1.1      29.0      34.0   \n",
              "4             13.8            1.0               0.8      19.0      12.0   \n",
              "...            ...            ...               ...       ...       ...   \n",
              "991341        15.0            1.0               1.0      26.0      36.0   \n",
              "991342        15.8            1.0               1.1      14.0      17.0   \n",
              "991343        14.3            1.0               0.8      30.0      27.0   \n",
              "991344        14.5            1.0               0.8      21.0      14.0   \n",
              "991345        15.8            1.0               0.9      24.0      43.0   \n",
              "\n",
              "        gamma_GTP  SMK_stat_type_cd  DRK_YN  \n",
              "0            40.0               1.0     1.0  \n",
              "1            27.0               0.0     0.0  \n",
              "2            68.0               1.0     0.0  \n",
              "3            18.0               1.0     0.0  \n",
              "4            25.0               1.0     0.0  \n",
              "...           ...               ...     ...  \n",
              "991341       27.0               1.0     0.0  \n",
              "991342       15.0               1.0     0.0  \n",
              "991343       17.0               0.0     1.0  \n",
              "991344       17.0               1.0     0.0  \n",
              "991345       36.0               0.0     1.0  \n",
              "\n",
              "[991346 rows x 24 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.loc[df[\"SMK_stat_type_cd\"] == 3.0, \"SMK_stat_type_cd\"] = 0.0\n",
        "df.loc[df[\"SMK_stat_type_cd\"] == 2.0, \"SMK_stat_type_cd\"] = 0.0\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dLsnS9fSj9M"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# for i in df.columns:\n",
        "#     plt.figure()\n",
        "#     sns.boxplot(data=df[i])\n",
        "#     plt.title(f'Box Plot of {i}')\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "h5GUm28E4-Iq",
        "outputId": "cf04627d-9b00-4a6f-f4ab-17d87e8529be"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>waistline</th>\n",
              "      <th>sight_left</th>\n",
              "      <th>sight_right</th>\n",
              "      <th>hear_left</th>\n",
              "      <th>hear_right</th>\n",
              "      <th>SBP</th>\n",
              "      <th>...</th>\n",
              "      <th>LDL_chole</th>\n",
              "      <th>triglyceride</th>\n",
              "      <th>hemoglobin</th>\n",
              "      <th>urine_protein</th>\n",
              "      <th>serum_creatinine</th>\n",
              "      <th>SGOT_AST</th>\n",
              "      <th>SGOT_ALT</th>\n",
              "      <th>gamma_GTP</th>\n",
              "      <th>SMK_stat_type_cd</th>\n",
              "      <th>DRK_YN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>1.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>9.9</td>\n",
              "      <td>9.9</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>146.0</td>\n",
              "      <td>...</td>\n",
              "      <td>117.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>15.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>19.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>322</th>\n",
              "      <td>0.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>9.9</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>...</td>\n",
              "      <td>117.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>14.7</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>15.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>656</th>\n",
              "      <td>1.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>111.0</td>\n",
              "      <td>9.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>...</td>\n",
              "      <td>69.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>14.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>17.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1378</th>\n",
              "      <td>1.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>9.9</td>\n",
              "      <td>0.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>...</td>\n",
              "      <td>74.0</td>\n",
              "      <td>164.0</td>\n",
              "      <td>16.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>19.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1441</th>\n",
              "      <td>1.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>9.9</td>\n",
              "      <td>0.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>157.0</td>\n",
              "      <td>...</td>\n",
              "      <td>157.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>23.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>990284</th>\n",
              "      <td>1.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>9.9</td>\n",
              "      <td>9.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>...</td>\n",
              "      <td>86.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>13.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.3</td>\n",
              "      <td>19.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>990841</th>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>83.0</td>\n",
              "      <td>9.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>...</td>\n",
              "      <td>123.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>16.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>24.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>990873</th>\n",
              "      <td>1.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>9.9</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>...</td>\n",
              "      <td>107.0</td>\n",
              "      <td>106.0</td>\n",
              "      <td>13.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991092</th>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>9.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>...</td>\n",
              "      <td>121.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>12.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>25.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991129</th>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>79.1</td>\n",
              "      <td>9.9</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>...</td>\n",
              "      <td>137.0</td>\n",
              "      <td>143.0</td>\n",
              "      <td>13.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>12.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3118 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        sex   age  height  weight  waistline  sight_left  sight_right  \\\n",
              "76      1.0  65.0   165.0    70.0       94.0         9.9          9.9   \n",
              "322     0.0  70.0   150.0    60.0       95.0         9.9          0.2   \n",
              "656     1.0  70.0   165.0    90.0      111.0         9.9          1.0   \n",
              "1378    1.0  65.0   160.0    65.0       91.0         9.9          0.9   \n",
              "1441    1.0  75.0   170.0    70.0       89.0         9.9          0.7   \n",
              "...     ...   ...     ...     ...        ...         ...          ...   \n",
              "990284  1.0  60.0   160.0    70.0       86.0         9.9          9.9   \n",
              "990841  1.0  50.0   170.0    70.0       83.0         9.9          1.0   \n",
              "990873  1.0  75.0   160.0    70.0      100.0         9.9          0.3   \n",
              "991092  0.0  50.0   165.0    50.0       75.0         9.9          1.0   \n",
              "991129  0.0  60.0   150.0    60.0       79.1         9.9          1.2   \n",
              "\n",
              "        hear_left  hear_right    SBP  ...  LDL_chole  triglyceride  \\\n",
              "76            2.0         1.0  146.0  ...      117.0          89.0   \n",
              "322           1.0         1.0  124.0  ...      117.0          85.0   \n",
              "656           1.0         1.0  126.0  ...       69.0          86.0   \n",
              "1378          1.0         1.0  125.0  ...       74.0         164.0   \n",
              "1441          1.0         1.0  157.0  ...      157.0          76.0   \n",
              "...           ...         ...    ...  ...        ...           ...   \n",
              "990284        1.0         1.0  160.0  ...       86.0          90.0   \n",
              "990841        1.0         1.0  150.0  ...      123.0          99.0   \n",
              "990873        1.0         1.0  160.0  ...      107.0         106.0   \n",
              "991092        1.0         1.0  124.0  ...      121.0          79.0   \n",
              "991129        1.0         1.0  124.0  ...      137.0         143.0   \n",
              "\n",
              "        hemoglobin  urine_protein  serum_creatinine  SGOT_AST  SGOT_ALT  \\\n",
              "76            15.6            1.0               0.7      19.0      16.0   \n",
              "322           14.7            5.0               0.9      15.0      17.0   \n",
              "656           14.8            1.0               1.1      17.0      14.0   \n",
              "1378          16.8            1.0               0.8      19.0      19.0   \n",
              "1441          14.0            2.0               1.5      23.0      24.0   \n",
              "...            ...            ...               ...       ...       ...   \n",
              "990284        13.9            3.0               3.3      19.0      17.0   \n",
              "990841        16.5            1.0               1.1      24.0      17.0   \n",
              "990873        13.5            1.0               1.0      19.0      14.0   \n",
              "991092        12.4            1.0               0.6      25.0      20.0   \n",
              "991129        13.3            1.0               0.6      12.0      14.0   \n",
              "\n",
              "        gamma_GTP  SMK_stat_type_cd  DRK_YN  \n",
              "76           29.0               0.0     1.0  \n",
              "322          12.0               1.0     0.0  \n",
              "656          19.0               1.0     1.0  \n",
              "1378         75.0               0.0     1.0  \n",
              "1441         74.0               1.0     0.0  \n",
              "...           ...               ...     ...  \n",
              "990284       33.0               1.0     0.0  \n",
              "990841       19.0               0.0     1.0  \n",
              "990873       19.0               0.0     0.0  \n",
              "991092       60.0               1.0     1.0  \n",
              "991129       21.0               1.0     0.0  \n",
              "\n",
              "[3118 rows x 24 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "filtered_df = df[df['sight_left'] > 9.8]\n",
        "filtered_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "QJdipm_JSj9N",
        "outputId": "cbfa9e84-853c-4194-bcf3-4f63938e9571"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>waistline</th>\n",
              "      <th>sight_left</th>\n",
              "      <th>sight_right</th>\n",
              "      <th>hear_left</th>\n",
              "      <th>hear_right</th>\n",
              "      <th>SBP</th>\n",
              "      <th>...</th>\n",
              "      <th>LDL_chole</th>\n",
              "      <th>triglyceride</th>\n",
              "      <th>hemoglobin</th>\n",
              "      <th>urine_protein</th>\n",
              "      <th>serum_creatinine</th>\n",
              "      <th>SGOT_AST</th>\n",
              "      <th>SGOT_ALT</th>\n",
              "      <th>gamma_GTP</th>\n",
              "      <th>SMK_stat_type_cd</th>\n",
              "      <th>DRK_YN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>9.486833</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>...</td>\n",
              "      <td>11.224972</td>\n",
              "      <td>9.591663</td>\n",
              "      <td>17.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.582576</td>\n",
              "      <td>5.916080</td>\n",
              "      <td>6.324555</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>180.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9.433981</td>\n",
              "      <td>0.948683</td>\n",
              "      <td>1.095445</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>...</td>\n",
              "      <td>12.165525</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.948683</td>\n",
              "      <td>4.472136</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>5.196152</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>9.539392</td>\n",
              "      <td>1.095445</td>\n",
              "      <td>1.224745</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>...</td>\n",
              "      <td>8.602325</td>\n",
              "      <td>10.198039</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.948683</td>\n",
              "      <td>6.855655</td>\n",
              "      <td>5.656854</td>\n",
              "      <td>8.246211</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9.539392</td>\n",
              "      <td>1.224745</td>\n",
              "      <td>1.095445</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>...</td>\n",
              "      <td>10.198039</td>\n",
              "      <td>10.295630</td>\n",
              "      <td>17.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.048809</td>\n",
              "      <td>5.385165</td>\n",
              "      <td>5.830952</td>\n",
              "      <td>4.242641</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>8.944272</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.095445</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>...</td>\n",
              "      <td>10.816654</td>\n",
              "      <td>10.198039</td>\n",
              "      <td>13.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.894427</td>\n",
              "      <td>4.358899</td>\n",
              "      <td>3.464102</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991341</th>\n",
              "      <td>1.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9.596874</td>\n",
              "      <td>1.224745</td>\n",
              "      <td>1.224745</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>...</td>\n",
              "      <td>11.180340</td>\n",
              "      <td>11.489125</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.099020</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>5.196152</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991342</th>\n",
              "      <td>1.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>9.273618</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.224745</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>...</td>\n",
              "      <td>9.165151</td>\n",
              "      <td>6.708204</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.048809</td>\n",
              "      <td>3.741657</td>\n",
              "      <td>4.123106</td>\n",
              "      <td>3.872983</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991343</th>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>155.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>8.246211</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.836660</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>...</td>\n",
              "      <td>8.774964</td>\n",
              "      <td>12.529964</td>\n",
              "      <td>14.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.894427</td>\n",
              "      <td>5.477226</td>\n",
              "      <td>5.196152</td>\n",
              "      <td>4.123106</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991344</th>\n",
              "      <td>1.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>8.485281</td>\n",
              "      <td>1.224745</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>...</td>\n",
              "      <td>8.544004</td>\n",
              "      <td>7.280110</td>\n",
              "      <td>14.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.894427</td>\n",
              "      <td>4.582576</td>\n",
              "      <td>3.741657</td>\n",
              "      <td>4.123106</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991345</th>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>9.513149</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.224745</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>...</td>\n",
              "      <td>12.369317</td>\n",
              "      <td>12.767145</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.948683</td>\n",
              "      <td>4.898979</td>\n",
              "      <td>6.557439</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>991346 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        sex   age  height  weight  waistline  sight_left  sight_right  \\\n",
              "0       1.0  35.0   170.0    75.0   9.486833    1.000000     1.000000   \n",
              "1       1.0  30.0   180.0    80.0   9.433981    0.948683     1.095445   \n",
              "2       1.0  40.0   165.0    75.0   9.539392    1.095445     1.224745   \n",
              "3       1.0  50.0   175.0    80.0   9.539392    1.224745     1.095445   \n",
              "4       1.0  50.0   165.0    60.0   8.944272    1.000000     1.095445   \n",
              "...     ...   ...     ...     ...        ...         ...          ...   \n",
              "991341  1.0  45.0   175.0    80.0   9.596874    1.224745     1.224745   \n",
              "991342  1.0  35.0   170.0    75.0   9.273618    1.000000     1.224745   \n",
              "991343  0.0  40.0   155.0    50.0   8.246211    1.000000     0.836660   \n",
              "991344  1.0  25.0   175.0    60.0   8.485281    1.224745     1.000000   \n",
              "991345  1.0  50.0   160.0    70.0   9.513149    1.000000     1.224745   \n",
              "\n",
              "        hear_left  hear_right    SBP  ...  LDL_chole  triglyceride  \\\n",
              "0             1.0         1.0  120.0  ...  11.224972      9.591663   \n",
              "1             1.0         1.0  130.0  ...  12.165525     11.000000   \n",
              "2             1.0         1.0  120.0  ...   8.602325     10.198039   \n",
              "3             1.0         1.0  145.0  ...  10.198039     10.295630   \n",
              "4             1.0         1.0  138.0  ...  10.816654     10.198039   \n",
              "...           ...         ...    ...  ...        ...           ...   \n",
              "991341        1.0         1.0  114.0  ...  11.180340     11.489125   \n",
              "991342        1.0         1.0  119.0  ...   9.165151      6.708204   \n",
              "991343        1.0         1.0  110.0  ...   8.774964     12.529964   \n",
              "991344        1.0         1.0  119.0  ...   8.544004      7.280110   \n",
              "991345        1.0         1.0  133.0  ...  12.369317     12.767145   \n",
              "\n",
              "        hemoglobin  urine_protein  serum_creatinine  SGOT_AST  SGOT_ALT  \\\n",
              "0             17.1            1.0          1.000000  4.582576  5.916080   \n",
              "1             15.8            1.0          0.948683  4.472136  6.000000   \n",
              "2             15.8            1.0          0.948683  6.855655  5.656854   \n",
              "3             17.6            1.0          1.048809  5.385165  5.830952   \n",
              "4             13.8            1.0          0.894427  4.358899  3.464102   \n",
              "...            ...            ...               ...       ...       ...   \n",
              "991341        15.0            1.0          1.000000  5.099020  6.000000   \n",
              "991342        15.8            1.0          1.048809  3.741657  4.123106   \n",
              "991343        14.3            1.0          0.894427  5.477226  5.196152   \n",
              "991344        14.5            1.0          0.894427  4.582576  3.741657   \n",
              "991345        15.8            1.0          0.948683  4.898979  6.557439   \n",
              "\n",
              "        gamma_GTP  SMK_stat_type_cd  DRK_YN  \n",
              "0        6.324555               1.0     1.0  \n",
              "1        5.196152               0.0     0.0  \n",
              "2        8.246211               1.0     0.0  \n",
              "3        4.242641               1.0     0.0  \n",
              "4        5.000000               1.0     0.0  \n",
              "...           ...               ...     ...  \n",
              "991341   5.196152               1.0     0.0  \n",
              "991342   3.872983               1.0     0.0  \n",
              "991343   4.123106               0.0     1.0  \n",
              "991344   4.123106               1.0     0.0  \n",
              "991345   6.000000               0.0     1.0  \n",
              "\n",
              "[991346 rows x 24 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "columns_tranformed = ['waistline','sight_left','sight_right','hear_left','hear_right','BLDS',\n",
        "                      'tot_chole','HDL_chole','LDL_chole','triglyceride','urine_protein',\n",
        "                      'serum_creatinine','SGOT_AST','SGOT_ALT','gamma_GTP']\n",
        "\n",
        "for i in columns_tranformed:\n",
        "    df[i] = np.sqrt(df[i])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSNUpth2Sj9N",
        "outputId": "68993ce4-14df-442e-da3b-841f6e38385b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "sex                -0.642598\n",
              "age                 0.073802\n",
              "height             -0.491795\n",
              "weight             -0.395614\n",
              "waistline          -0.280184\n",
              "sight_left         -0.095448\n",
              "sight_right        -0.096737\n",
              "hear_left           0.010414\n",
              "hear_right          0.012682\n",
              "SBP                -0.107369\n",
              "DBP                -0.142998\n",
              "BLDS               -0.108436\n",
              "tot_chole          -0.005722\n",
              "HDL_chole           0.196042\n",
              "LDL_chole           0.015089\n",
              "triglyceride       -0.237070\n",
              "hemoglobin         -0.464186\n",
              "urine_protein      -0.018831\n",
              "serum_creatinine   -0.304228\n",
              "SGOT_AST           -0.126406\n",
              "SGOT_ALT           -0.214921\n",
              "gamma_GTP          -0.332368\n",
              "SMK_stat_type_cd    1.000000\n",
              "DRK_YN             -0.362274\n",
              "Name: SMK_stat_type_cd, dtype: float64"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corr = df.corr()\n",
        "corr['SMK_stat_type_cd']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVMxUuF6Sj9O"
      },
      "outputs": [],
      "source": [
        "columns_to_be_scaled = ['age','height','weight','waistline','sight_left','sight_right','hear_left','hear_right',\n",
        "                        'SBP','DBP','BLDS','tot_chole','HDL_chole','LDL_chole','triglyceride','hemoglobin','urine_protein',\n",
        "                        'serum_creatinine','SGOT_AST','SGOT_ALT','gamma_GTP']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEMRg5u9Sj9O"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "StandardScaler_object = StandardScaler()\n",
        "df[columns_to_be_scaled] = StandardScaler_object.fit_transform(df[columns_to_be_scaled])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtihaUO2Sj9P",
        "outputId": "9191dc06-4af5-4977-935a-cc901ebbf3dc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({0.0: 388905, 1.0: 388905})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from collections import Counter\n",
        "\n",
        "X = df.drop(\"SMK_stat_type_cd\",axis=1)\n",
        "y = df[\"SMK_stat_type_cd\"]\n",
        "#y = (df[\"SMK_stat_type_cd\"] == 1).astype(int)\n",
        "undersampler = RandomUnderSampler(sampling_strategy=1)\n",
        "X, y = undersampler.fit_resample(X, y)\n",
        "Counter(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOgeTsMRkkrU"
      },
      "source": [
        "# Multi-layer Perceptron from scratch with k fold cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hwi90dip0WEQ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import regularizers\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdkBpHKbNY11"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbUouea6edug"
      },
      "outputs": [],
      "source": [
        "y_train = np.array(y_train).astype(int)\n",
        "y_test = np.array(y_test).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8kX1Y4e9_Fd",
        "outputId": "a23d4285-23fa-426e-f920-06dcd29a8079"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1\n",
            "Training the model with scratch implementation\n",
            "Epoch 1/50\n",
            "2178/2178 [==============================] - 9s 4ms/step - loss: 0.5295 - accuracy: 0.7890 - val_loss: 0.4568 - val_accuracy: 0.8261\n",
            "Epoch 2/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4474 - accuracy: 0.8269 - val_loss: 0.4389 - val_accuracy: 0.8284\n",
            "Epoch 3/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4353 - accuracy: 0.8282 - val_loss: 0.4304 - val_accuracy: 0.8295\n",
            "Epoch 4/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4289 - accuracy: 0.8286 - val_loss: 0.4258 - val_accuracy: 0.8300\n",
            "Epoch 5/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4251 - accuracy: 0.8290 - val_loss: 0.4228 - val_accuracy: 0.8298\n",
            "Epoch 6/50\n",
            "2178/2178 [==============================] - 8s 4ms/step - loss: 0.4225 - accuracy: 0.8292 - val_loss: 0.4209 - val_accuracy: 0.8299\n",
            "Epoch 7/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4207 - accuracy: 0.8293 - val_loss: 0.4192 - val_accuracy: 0.8302\n",
            "Epoch 8/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4193 - accuracy: 0.8294 - val_loss: 0.4179 - val_accuracy: 0.8305\n",
            "Epoch 9/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4182 - accuracy: 0.8294 - val_loss: 0.4170 - val_accuracy: 0.8303\n",
            "Epoch 10/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4173 - accuracy: 0.8296 - val_loss: 0.4171 - val_accuracy: 0.8296\n",
            "Epoch 11/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4164 - accuracy: 0.8296 - val_loss: 0.4153 - val_accuracy: 0.8305\n",
            "Epoch 12/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4157 - accuracy: 0.8297 - val_loss: 0.4149 - val_accuracy: 0.8308\n",
            "Epoch 13/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4151 - accuracy: 0.8299 - val_loss: 0.4140 - val_accuracy: 0.8306\n",
            "Epoch 14/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4147 - accuracy: 0.8299 - val_loss: 0.4137 - val_accuracy: 0.8309\n",
            "Epoch 15/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4142 - accuracy: 0.8299 - val_loss: 0.4138 - val_accuracy: 0.8302\n",
            "Epoch 16/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4139 - accuracy: 0.8297 - val_loss: 0.4130 - val_accuracy: 0.8306\n",
            "Epoch 17/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4135 - accuracy: 0.8300 - val_loss: 0.4124 - val_accuracy: 0.8308\n",
            "Epoch 18/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4132 - accuracy: 0.8299 - val_loss: 0.4123 - val_accuracy: 0.8308\n",
            "Epoch 19/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4129 - accuracy: 0.8300 - val_loss: 0.4121 - val_accuracy: 0.8306\n",
            "Epoch 20/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4126 - accuracy: 0.8301 - val_loss: 0.4117 - val_accuracy: 0.8308\n",
            "Epoch 21/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4124 - accuracy: 0.8299 - val_loss: 0.4115 - val_accuracy: 0.8310\n",
            "Epoch 22/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4122 - accuracy: 0.8300 - val_loss: 0.4114 - val_accuracy: 0.8309\n",
            "Epoch 23/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4120 - accuracy: 0.8298 - val_loss: 0.4114 - val_accuracy: 0.8309\n",
            "Epoch 24/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4118 - accuracy: 0.8301 - val_loss: 0.4134 - val_accuracy: 0.8292\n",
            "Epoch 25/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4117 - accuracy: 0.8301 - val_loss: 0.4109 - val_accuracy: 0.8310\n",
            "Epoch 26/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4115 - accuracy: 0.8301 - val_loss: 0.4106 - val_accuracy: 0.8308\n",
            "Epoch 27/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4114 - accuracy: 0.8301 - val_loss: 0.4109 - val_accuracy: 0.8311\n",
            "Epoch 28/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4112 - accuracy: 0.8301 - val_loss: 0.4105 - val_accuracy: 0.8310\n",
            "Epoch 29/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4112 - accuracy: 0.8299 - val_loss: 0.4102 - val_accuracy: 0.8311\n",
            "Epoch 30/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4110 - accuracy: 0.8300 - val_loss: 0.4106 - val_accuracy: 0.8308\n",
            "Epoch 31/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4109 - accuracy: 0.8302 - val_loss: 0.4107 - val_accuracy: 0.8307\n",
            "Epoch 32/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4108 - accuracy: 0.8303 - val_loss: 0.4107 - val_accuracy: 0.8307\n",
            "Epoch 33/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4107 - accuracy: 0.8301 - val_loss: 0.4101 - val_accuracy: 0.8309\n",
            "Epoch 34/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4107 - accuracy: 0.8301 - val_loss: 0.4101 - val_accuracy: 0.8309\n",
            "Epoch 35/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4106 - accuracy: 0.8301 - val_loss: 0.4104 - val_accuracy: 0.8307\n",
            "Epoch 36/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4105 - accuracy: 0.8302 - val_loss: 0.4102 - val_accuracy: 0.8312\n",
            "Epoch 37/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4104 - accuracy: 0.8304 - val_loss: 0.4100 - val_accuracy: 0.8309\n",
            "Epoch 38/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4103 - accuracy: 0.8303 - val_loss: 0.4102 - val_accuracy: 0.8309\n",
            "Epoch 39/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4103 - accuracy: 0.8303 - val_loss: 0.4096 - val_accuracy: 0.8310\n",
            "Epoch 40/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4103 - accuracy: 0.8302 - val_loss: 0.4099 - val_accuracy: 0.8310\n",
            "Epoch 41/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4102 - accuracy: 0.8302 - val_loss: 0.4094 - val_accuracy: 0.8307\n",
            "Epoch 42/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4101 - accuracy: 0.8300 - val_loss: 0.4099 - val_accuracy: 0.8310\n",
            "Epoch 43/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4100 - accuracy: 0.8303 - val_loss: 0.4094 - val_accuracy: 0.8310\n",
            "Epoch 44/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4100 - accuracy: 0.8302 - val_loss: 0.4094 - val_accuracy: 0.8311\n",
            "Epoch 45/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4100 - accuracy: 0.8301 - val_loss: 0.4094 - val_accuracy: 0.8310\n",
            "Epoch 46/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4098 - accuracy: 0.8302 - val_loss: 0.4094 - val_accuracy: 0.8309\n",
            "Epoch 47/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4099 - accuracy: 0.8303 - val_loss: 0.4092 - val_accuracy: 0.8310\n",
            "Epoch 48/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4098 - accuracy: 0.8303 - val_loss: 0.4090 - val_accuracy: 0.8312\n",
            "Epoch 49/50\n",
            "2178/2178 [==============================] - 6s 3ms/step - loss: 0.4097 - accuracy: 0.8302 - val_loss: 0.4090 - val_accuracy: 0.8309\n",
            "Epoch 50/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4097 - accuracy: 0.8301 - val_loss: 0.4094 - val_accuracy: 0.8308\n",
            "13612/13612 [==============================] - 21s 2ms/step\n",
            "3403/3403 [==============================] - 4s 1ms/step\n",
            "7292/7292 [==============================] - 8s 1ms/step\n",
            "Training the model with sklearn implementation\n",
            "Iteration 1, loss = 0.54657141\n",
            "Validation score: 0.813752\n",
            "Iteration 2, loss = 0.43746821\n",
            "Validation score: 0.821431\n",
            "Iteration 3, loss = 0.42235352\n",
            "Validation score: 0.825931\n",
            "Iteration 4, loss = 0.41552514\n",
            "Validation score: 0.827389\n",
            "Iteration 5, loss = 0.41210048\n",
            "Validation score: 0.828066\n",
            "Iteration 6, loss = 0.41018269\n",
            "Validation score: 0.828273\n",
            "Iteration 7, loss = 0.40896542\n",
            "Validation score: 0.828376\n",
            "Iteration 8, loss = 0.40814202\n",
            "Validation score: 0.828744\n",
            "Iteration 9, loss = 0.40754235\n",
            "Validation score: 0.828939\n",
            "Iteration 10, loss = 0.40706988\n",
            "Validation score: 0.829191\n",
            "Iteration 11, loss = 0.40670846\n",
            "Validation score: 0.828904\n",
            "Iteration 12, loss = 0.40639571\n",
            "Validation score: 0.828893\n",
            "Iteration 13, loss = 0.40614766\n",
            "Validation score: 0.829042\n",
            "Iteration 14, loss = 0.40594159\n",
            "Validation score: 0.829203\n",
            "Iteration 15, loss = 0.40574361\n",
            "Validation score: 0.829203\n",
            "Iteration 16, loss = 0.40556245\n",
            "Validation score: 0.829214\n",
            "Iteration 17, loss = 0.40539630\n",
            "Validation score: 0.829214\n",
            "Iteration 18, loss = 0.40524958\n",
            "Validation score: 0.829249\n",
            "Iteration 19, loss = 0.40512558\n",
            "Validation score: 0.829398\n",
            "Iteration 20, loss = 0.40501059\n",
            "Validation score: 0.829421\n",
            "Iteration 21, loss = 0.40488984\n",
            "Validation score: 0.829605\n",
            "Iteration 22, loss = 0.40476973\n",
            "Validation score: 0.829949\n",
            "Iteration 23, loss = 0.40466288\n",
            "Validation score: 0.829628\n",
            "Iteration 24, loss = 0.40449432\n",
            "Validation score: 0.829409\n",
            "Iteration 25, loss = 0.40439868\n",
            "Validation score: 0.829719\n",
            "Iteration 26, loss = 0.40431882\n",
            "Validation score: 0.829765\n",
            "Iteration 27, loss = 0.40421301\n",
            "Validation score: 0.829972\n",
            "Iteration 28, loss = 0.40413055\n",
            "Validation score: 0.829765\n",
            "Iteration 29, loss = 0.40401456\n",
            "Validation score: 0.829880\n",
            "Iteration 30, loss = 0.40391682\n",
            "Validation score: 0.830075\n",
            "Iteration 31, loss = 0.40386927\n",
            "Validation score: 0.830052\n",
            "Iteration 32, loss = 0.40377775\n",
            "Validation score: 0.829800\n",
            "Iteration 33, loss = 0.40368876\n",
            "Validation score: 0.829949\n",
            "Iteration 34, loss = 0.40360963\n",
            "Validation score: 0.829869\n",
            "Iteration 35, loss = 0.40353778\n",
            "Validation score: 0.829903\n",
            "Iteration 36, loss = 0.40345353\n",
            "Validation score: 0.829777\n",
            "Iteration 37, loss = 0.40340576\n",
            "Validation score: 0.830201\n",
            "Iteration 38, loss = 0.40333910\n",
            "Validation score: 0.830133\n",
            "Iteration 39, loss = 0.40327204\n",
            "Validation score: 0.830064\n",
            "Iteration 40, loss = 0.40323289\n",
            "Validation score: 0.830064\n",
            "Iteration 41, loss = 0.40313876\n",
            "Validation score: 0.830178\n",
            "Iteration 42, loss = 0.40308763\n",
            "Validation score: 0.830270\n",
            "Iteration 43, loss = 0.40303213\n",
            "Validation score: 0.830351\n",
            "Iteration 44, loss = 0.40297388\n",
            "Validation score: 0.830075\n",
            "Iteration 45, loss = 0.40292183\n",
            "Validation score: 0.830374\n",
            "Iteration 46, loss = 0.40283163\n",
            "Validation score: 0.830018\n",
            "Iteration 47, loss = 0.40280443\n",
            "Validation score: 0.830615\n",
            "Iteration 48, loss = 0.40276728\n",
            "Validation score: 0.830282\n",
            "Iteration 49, loss = 0.40272837\n",
            "Validation score: 0.830362\n",
            "Iteration 50, loss = 0.40267046\n",
            "Validation score: 0.830293\n",
            "Iteration 51, loss = 0.40263624\n",
            "Validation score: 0.830178\n",
            "Iteration 52, loss = 0.40261026\n",
            "Validation score: 0.830282\n",
            "Iteration 53, loss = 0.40251921\n",
            "Validation score: 0.830282\n",
            "Iteration 54, loss = 0.40251062\n",
            "Validation score: 0.830408\n",
            "Iteration 55, loss = 0.40246959\n",
            "Validation score: 0.830156\n",
            "Iteration 56, loss = 0.40243599\n",
            "Validation score: 0.830316\n",
            "Iteration 57, loss = 0.40239559\n",
            "Validation score: 0.830293\n",
            "Iteration 58, loss = 0.40237497\n",
            "Validation score: 0.830500\n",
            "Validation score did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\sklearn\\base.py:458: UserWarning: X has feature names, but MLPClassifier was fitted without feature names\n",
            "  warnings.warn(\n",
            "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\sklearn\\base.py:458: UserWarning: X has feature names, but MLPClassifier was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 2\n",
            "Training the model with scratch implementation\n",
            "Epoch 1/50\n",
            "2178/2178 [==============================] - 6s 2ms/step - loss: 0.5424 - accuracy: 0.7867 - val_loss: 0.4614 - val_accuracy: 0.8271\n",
            "Epoch 2/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4532 - accuracy: 0.8264 - val_loss: 0.4424 - val_accuracy: 0.8290\n",
            "Epoch 3/50\n",
            "2178/2178 [==============================] - 5s 3ms/step - loss: 0.4407 - accuracy: 0.8276 - val_loss: 0.4338 - val_accuracy: 0.8293\n",
            "Epoch 4/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4338 - accuracy: 0.8283 - val_loss: 0.4289 - val_accuracy: 0.8301\n",
            "Epoch 5/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4295 - accuracy: 0.8285 - val_loss: 0.4257 - val_accuracy: 0.8306\n",
            "Epoch 6/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4264 - accuracy: 0.8288 - val_loss: 0.4225 - val_accuracy: 0.8304\n",
            "Epoch 7/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4242 - accuracy: 0.8289 - val_loss: 0.4207 - val_accuracy: 0.8302\n",
            "Epoch 8/50\n",
            "2178/2178 [==============================] - 6s 3ms/step - loss: 0.4225 - accuracy: 0.8290 - val_loss: 0.4193 - val_accuracy: 0.8301\n",
            "Epoch 9/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4212 - accuracy: 0.8291 - val_loss: 0.4179 - val_accuracy: 0.8303\n",
            "Epoch 10/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4202 - accuracy: 0.8290 - val_loss: 0.4170 - val_accuracy: 0.8307\n",
            "Epoch 11/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4193 - accuracy: 0.8292 - val_loss: 0.4163 - val_accuracy: 0.8298\n",
            "Epoch 12/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4185 - accuracy: 0.8292 - val_loss: 0.4155 - val_accuracy: 0.8306\n",
            "Epoch 13/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4179 - accuracy: 0.8291 - val_loss: 0.4148 - val_accuracy: 0.8305\n",
            "Epoch 14/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4173 - accuracy: 0.8290 - val_loss: 0.4144 - val_accuracy: 0.8308\n",
            "Epoch 15/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4168 - accuracy: 0.8291 - val_loss: 0.4138 - val_accuracy: 0.8309\n",
            "Epoch 16/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4163 - accuracy: 0.8292 - val_loss: 0.4134 - val_accuracy: 0.8307\n",
            "Epoch 17/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4159 - accuracy: 0.8294 - val_loss: 0.4133 - val_accuracy: 0.8306\n",
            "Epoch 18/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4155 - accuracy: 0.8294 - val_loss: 0.4127 - val_accuracy: 0.8305\n",
            "Epoch 19/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4153 - accuracy: 0.8293 - val_loss: 0.4124 - val_accuracy: 0.8314\n",
            "Epoch 20/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4148 - accuracy: 0.8294 - val_loss: 0.4129 - val_accuracy: 0.8299\n",
            "Epoch 21/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4146 - accuracy: 0.8294 - val_loss: 0.4121 - val_accuracy: 0.8302\n",
            "Epoch 22/50\n",
            "2178/2178 [==============================] - 8s 3ms/step - loss: 0.4143 - accuracy: 0.8295 - val_loss: 0.4113 - val_accuracy: 0.8312\n",
            "Epoch 23/50\n",
            "2178/2178 [==============================] - 8s 4ms/step - loss: 0.4141 - accuracy: 0.8295 - val_loss: 0.4111 - val_accuracy: 0.8308\n",
            "Epoch 24/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4138 - accuracy: 0.8294 - val_loss: 0.4112 - val_accuracy: 0.8307\n",
            "Epoch 25/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4136 - accuracy: 0.8295 - val_loss: 0.4112 - val_accuracy: 0.8306\n",
            "Epoch 26/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4133 - accuracy: 0.8297 - val_loss: 0.4107 - val_accuracy: 0.8315\n",
            "Epoch 27/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4131 - accuracy: 0.8296 - val_loss: 0.4117 - val_accuracy: 0.8293\n",
            "Epoch 28/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4130 - accuracy: 0.8296 - val_loss: 0.4101 - val_accuracy: 0.8308\n",
            "Epoch 29/50\n",
            "2178/2178 [==============================] - 8s 3ms/step - loss: 0.4127 - accuracy: 0.8297 - val_loss: 0.4106 - val_accuracy: 0.8305\n",
            "Epoch 30/50\n",
            "2178/2178 [==============================] - 8s 4ms/step - loss: 0.4126 - accuracy: 0.8297 - val_loss: 0.4098 - val_accuracy: 0.8313\n",
            "Epoch 31/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4125 - accuracy: 0.8298 - val_loss: 0.4097 - val_accuracy: 0.8314\n",
            "Epoch 32/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4124 - accuracy: 0.8298 - val_loss: 0.4099 - val_accuracy: 0.8312\n",
            "Epoch 33/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4122 - accuracy: 0.8297 - val_loss: 0.4093 - val_accuracy: 0.8313\n",
            "Epoch 34/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4122 - accuracy: 0.8298 - val_loss: 0.4098 - val_accuracy: 0.8316\n",
            "Epoch 35/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4121 - accuracy: 0.8298 - val_loss: 0.4093 - val_accuracy: 0.8313\n",
            "Epoch 36/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4119 - accuracy: 0.8300 - val_loss: 0.4090 - val_accuracy: 0.8313\n",
            "Epoch 37/50\n",
            "2178/2178 [==============================] - 5s 3ms/step - loss: 0.4119 - accuracy: 0.8297 - val_loss: 0.4089 - val_accuracy: 0.8312\n",
            "Epoch 38/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4118 - accuracy: 0.8297 - val_loss: 0.4089 - val_accuracy: 0.8314\n",
            "Epoch 39/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4116 - accuracy: 0.8297 - val_loss: 0.4090 - val_accuracy: 0.8313\n",
            "Epoch 40/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4116 - accuracy: 0.8298 - val_loss: 0.4089 - val_accuracy: 0.8311\n",
            "Epoch 41/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4115 - accuracy: 0.8298 - val_loss: 0.4088 - val_accuracy: 0.8309\n",
            "Epoch 42/50\n",
            "2178/2178 [==============================] - 6s 3ms/step - loss: 0.4114 - accuracy: 0.8298 - val_loss: 0.4089 - val_accuracy: 0.8311\n",
            "Epoch 43/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4113 - accuracy: 0.8299 - val_loss: 0.4090 - val_accuracy: 0.8308\n",
            "Epoch 44/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4112 - accuracy: 0.8300 - val_loss: 0.4084 - val_accuracy: 0.8315\n",
            "Epoch 45/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4112 - accuracy: 0.8299 - val_loss: 0.4086 - val_accuracy: 0.8317\n",
            "Epoch 46/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4111 - accuracy: 0.8297 - val_loss: 0.4084 - val_accuracy: 0.8309\n",
            "Epoch 47/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4110 - accuracy: 0.8299 - val_loss: 0.4083 - val_accuracy: 0.8316\n",
            "Epoch 48/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4110 - accuracy: 0.8298 - val_loss: 0.4081 - val_accuracy: 0.8315\n",
            "Epoch 49/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4109 - accuracy: 0.8298 - val_loss: 0.4081 - val_accuracy: 0.8314\n",
            "Epoch 50/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4108 - accuracy: 0.8300 - val_loss: 0.4082 - val_accuracy: 0.8312\n",
            "13612/13612 [==============================] - 18s 1ms/step\n",
            "3403/3403 [==============================] - 5s 2ms/step\n",
            "7292/7292 [==============================] - 10s 1ms/step\n",
            "Training the model with sklearn implementation\n",
            "Iteration 1, loss = 0.53109882\n",
            "Validation score: 0.815520\n",
            "Iteration 2, loss = 0.42888149\n",
            "Validation score: 0.821914\n",
            "Iteration 3, loss = 0.41903991\n",
            "Validation score: 0.824083\n",
            "Iteration 4, loss = 0.41451088\n",
            "Validation score: 0.825943\n",
            "Iteration 5, loss = 0.41187381\n",
            "Validation score: 0.826012\n",
            "Iteration 6, loss = 0.41028440\n",
            "Validation score: 0.826964\n",
            "Iteration 7, loss = 0.40914059\n",
            "Validation score: 0.827171\n",
            "Iteration 8, loss = 0.40828480\n",
            "Validation score: 0.827228\n",
            "Iteration 9, loss = 0.40769952\n",
            "Validation score: 0.827791\n",
            "Iteration 10, loss = 0.40718998\n",
            "Validation score: 0.827997\n",
            "Iteration 11, loss = 0.40675192\n",
            "Validation score: 0.827940\n",
            "Iteration 12, loss = 0.40637776\n",
            "Validation score: 0.827791\n",
            "Iteration 13, loss = 0.40602594\n",
            "Validation score: 0.828503\n",
            "Iteration 14, loss = 0.40574269\n",
            "Validation score: 0.828250\n",
            "Iteration 15, loss = 0.40552376\n",
            "Validation score: 0.828158\n",
            "Iteration 16, loss = 0.40534692\n",
            "Validation score: 0.828135\n",
            "Iteration 17, loss = 0.40515442\n",
            "Validation score: 0.828066\n",
            "Iteration 18, loss = 0.40496694\n",
            "Validation score: 0.828411\n",
            "Iteration 19, loss = 0.40483249\n",
            "Validation score: 0.827929\n",
            "Iteration 20, loss = 0.40470007\n",
            "Validation score: 0.828411\n",
            "Iteration 21, loss = 0.40454024\n",
            "Validation score: 0.828181\n",
            "Iteration 22, loss = 0.40444022\n",
            "Validation score: 0.827940\n",
            "Iteration 23, loss = 0.40433968\n",
            "Validation score: 0.828112\n",
            "Iteration 24, loss = 0.40423446\n",
            "Validation score: 0.828193\n",
            "Validation score did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\sklearn\\base.py:458: UserWarning: X has feature names, but MLPClassifier was fitted without feature names\n",
            "  warnings.warn(\n",
            "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\sklearn\\base.py:458: UserWarning: X has feature names, but MLPClassifier was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 3\n",
            "Training the model with scratch implementation\n",
            "Epoch 1/50\n",
            "2178/2178 [==============================] - 6s 2ms/step - loss: 0.5337 - accuracy: 0.7881 - val_loss: 0.4690 - val_accuracy: 0.8231\n",
            "Epoch 2/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4523 - accuracy: 0.8263 - val_loss: 0.4448 - val_accuracy: 0.8274\n",
            "Epoch 3/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4372 - accuracy: 0.8283 - val_loss: 0.4350 - val_accuracy: 0.8286\n",
            "Epoch 4/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4299 - accuracy: 0.8290 - val_loss: 0.4293 - val_accuracy: 0.8290\n",
            "Epoch 5/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4254 - accuracy: 0.8294 - val_loss: 0.4259 - val_accuracy: 0.8290\n",
            "Epoch 6/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4223 - accuracy: 0.8296 - val_loss: 0.4235 - val_accuracy: 0.8293\n",
            "Epoch 7/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4202 - accuracy: 0.8298 - val_loss: 0.4215 - val_accuracy: 0.8295\n",
            "Epoch 8/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4187 - accuracy: 0.8299 - val_loss: 0.4205 - val_accuracy: 0.8294\n",
            "Epoch 9/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4175 - accuracy: 0.8298 - val_loss: 0.4192 - val_accuracy: 0.8295\n",
            "Epoch 10/50\n",
            "2178/2178 [==============================] - 6s 3ms/step - loss: 0.4166 - accuracy: 0.8300 - val_loss: 0.4189 - val_accuracy: 0.8297\n",
            "Epoch 11/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4158 - accuracy: 0.8298 - val_loss: 0.4176 - val_accuracy: 0.8297\n",
            "Epoch 12/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4151 - accuracy: 0.8301 - val_loss: 0.4171 - val_accuracy: 0.8298\n",
            "Epoch 13/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4145 - accuracy: 0.8300 - val_loss: 0.4170 - val_accuracy: 0.8297\n",
            "Epoch 14/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4140 - accuracy: 0.8301 - val_loss: 0.4163 - val_accuracy: 0.8297\n",
            "Epoch 15/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4136 - accuracy: 0.8302 - val_loss: 0.4158 - val_accuracy: 0.8298\n",
            "Epoch 16/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4132 - accuracy: 0.8302 - val_loss: 0.4156 - val_accuracy: 0.8299\n",
            "Epoch 17/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4129 - accuracy: 0.8300 - val_loss: 0.4149 - val_accuracy: 0.8299\n",
            "Epoch 18/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4125 - accuracy: 0.8302 - val_loss: 0.4151 - val_accuracy: 0.8299\n",
            "Epoch 19/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4123 - accuracy: 0.8303 - val_loss: 0.4145 - val_accuracy: 0.8298\n",
            "Epoch 20/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4120 - accuracy: 0.8302 - val_loss: 0.4143 - val_accuracy: 0.8298\n",
            "Epoch 21/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4117 - accuracy: 0.8302 - val_loss: 0.4142 - val_accuracy: 0.8299\n",
            "Epoch 22/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4115 - accuracy: 0.8302 - val_loss: 0.4143 - val_accuracy: 0.8299\n",
            "Epoch 23/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4114 - accuracy: 0.8301 - val_loss: 0.4143 - val_accuracy: 0.8297\n",
            "Epoch 24/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4111 - accuracy: 0.8301 - val_loss: 0.4138 - val_accuracy: 0.8300\n",
            "Epoch 25/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4110 - accuracy: 0.8302 - val_loss: 0.4135 - val_accuracy: 0.8301\n",
            "Epoch 26/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4109 - accuracy: 0.8302 - val_loss: 0.4135 - val_accuracy: 0.8298\n",
            "Epoch 27/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4107 - accuracy: 0.8302 - val_loss: 0.4134 - val_accuracy: 0.8299\n",
            "Epoch 28/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4106 - accuracy: 0.8304 - val_loss: 0.4132 - val_accuracy: 0.8297\n",
            "Epoch 29/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4105 - accuracy: 0.8303 - val_loss: 0.4135 - val_accuracy: 0.8297\n",
            "Epoch 30/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4104 - accuracy: 0.8303 - val_loss: 0.4130 - val_accuracy: 0.8298\n",
            "Epoch 31/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4103 - accuracy: 0.8303 - val_loss: 0.4128 - val_accuracy: 0.8298\n",
            "Epoch 32/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4101 - accuracy: 0.8303 - val_loss: 0.4128 - val_accuracy: 0.8298\n",
            "Epoch 33/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4101 - accuracy: 0.8304 - val_loss: 0.4130 - val_accuracy: 0.8297\n",
            "Epoch 34/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4100 - accuracy: 0.8303 - val_loss: 0.4126 - val_accuracy: 0.8295\n",
            "Epoch 35/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4098 - accuracy: 0.8305 - val_loss: 0.4127 - val_accuracy: 0.8299\n",
            "Epoch 36/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4098 - accuracy: 0.8303 - val_loss: 0.4123 - val_accuracy: 0.8298\n",
            "Epoch 37/50\n",
            "2178/2178 [==============================] - 6s 3ms/step - loss: 0.4097 - accuracy: 0.8303 - val_loss: 0.4125 - val_accuracy: 0.8295\n",
            "Epoch 38/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4096 - accuracy: 0.8304 - val_loss: 0.4123 - val_accuracy: 0.8297\n",
            "Epoch 39/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4096 - accuracy: 0.8304 - val_loss: 0.4122 - val_accuracy: 0.8297\n",
            "Epoch 40/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4095 - accuracy: 0.8303 - val_loss: 0.4122 - val_accuracy: 0.8298\n",
            "Epoch 41/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4094 - accuracy: 0.8304 - val_loss: 0.4120 - val_accuracy: 0.8298\n",
            "Epoch 42/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4093 - accuracy: 0.8304 - val_loss: 0.4122 - val_accuracy: 0.8297\n",
            "Epoch 43/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4093 - accuracy: 0.8304 - val_loss: 0.4128 - val_accuracy: 0.8296\n",
            "Epoch 44/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4092 - accuracy: 0.8303 - val_loss: 0.4122 - val_accuracy: 0.8299\n",
            "Epoch 45/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4092 - accuracy: 0.8303 - val_loss: 0.4121 - val_accuracy: 0.8299\n",
            "Epoch 46/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4091 - accuracy: 0.8305 - val_loss: 0.4129 - val_accuracy: 0.8298\n",
            "Epoch 47/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4091 - accuracy: 0.8303 - val_loss: 0.4117 - val_accuracy: 0.8297\n",
            "Epoch 48/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4090 - accuracy: 0.8305 - val_loss: 0.4120 - val_accuracy: 0.8298\n",
            "Epoch 49/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4090 - accuracy: 0.8304 - val_loss: 0.4113 - val_accuracy: 0.8300\n",
            "Epoch 50/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4090 - accuracy: 0.8304 - val_loss: 0.4119 - val_accuracy: 0.8300\n",
            "13612/13612 [==============================] - 16s 1ms/step\n",
            "3403/3403 [==============================] - 4s 1ms/step\n",
            "7292/7292 [==============================] - 9s 1ms/step\n",
            "Training the model with sklearn implementation\n",
            "Iteration 1, loss = 0.49215679\n",
            "Validation score: 0.820134\n",
            "Iteration 2, loss = 0.42332062\n",
            "Validation score: 0.824508\n",
            "Iteration 3, loss = 0.41621246\n",
            "Validation score: 0.826012\n",
            "Iteration 4, loss = 0.41278008\n",
            "Validation score: 0.826666\n",
            "Iteration 5, loss = 0.41072530\n",
            "Validation score: 0.826976\n",
            "Iteration 6, loss = 0.40939801\n",
            "Validation score: 0.827848\n",
            "Iteration 7, loss = 0.40837247\n",
            "Validation score: 0.827975\n",
            "Iteration 8, loss = 0.40755050\n",
            "Validation score: 0.827963\n",
            "Iteration 9, loss = 0.40690732\n",
            "Validation score: 0.828261\n",
            "Iteration 10, loss = 0.40635809\n",
            "Validation score: 0.828342\n",
            "Iteration 11, loss = 0.40592516\n",
            "Validation score: 0.828686\n",
            "Iteration 12, loss = 0.40559124\n",
            "Validation score: 0.828606\n",
            "Iteration 13, loss = 0.40526099\n",
            "Validation score: 0.828709\n",
            "Iteration 14, loss = 0.40499648\n",
            "Validation score: 0.828652\n",
            "Iteration 15, loss = 0.40476713\n",
            "Validation score: 0.829099\n",
            "Iteration 16, loss = 0.40457747\n",
            "Validation score: 0.828996\n",
            "Iteration 17, loss = 0.40441868\n",
            "Validation score: 0.828858\n",
            "Iteration 18, loss = 0.40424900\n",
            "Validation score: 0.828927\n",
            "Iteration 19, loss = 0.40414179\n",
            "Validation score: 0.829249\n",
            "Iteration 20, loss = 0.40398424\n",
            "Validation score: 0.829226\n",
            "Iteration 21, loss = 0.40386712\n",
            "Validation score: 0.829191\n",
            "Iteration 22, loss = 0.40377333\n",
            "Validation score: 0.829168\n",
            "Iteration 23, loss = 0.40367250\n",
            "Validation score: 0.829226\n",
            "Iteration 24, loss = 0.40355523\n",
            "Validation score: 0.828962\n",
            "Iteration 25, loss = 0.40344773\n",
            "Validation score: 0.829306\n",
            "Iteration 26, loss = 0.40339640\n",
            "Validation score: 0.829065\n",
            "Iteration 27, loss = 0.40329409\n",
            "Validation score: 0.829134\n",
            "Iteration 28, loss = 0.40324074\n",
            "Validation score: 0.829214\n",
            "Iteration 29, loss = 0.40313763\n",
            "Validation score: 0.829306\n",
            "Iteration 30, loss = 0.40305723\n",
            "Validation score: 0.829467\n",
            "Iteration 31, loss = 0.40300331\n",
            "Validation score: 0.829467\n",
            "Iteration 32, loss = 0.40293345\n",
            "Validation score: 0.829341\n",
            "Iteration 33, loss = 0.40290677\n",
            "Validation score: 0.829191\n",
            "Iteration 34, loss = 0.40286213\n",
            "Validation score: 0.829306\n",
            "Iteration 35, loss = 0.40275875\n",
            "Validation score: 0.829455\n",
            "Iteration 36, loss = 0.40271016\n",
            "Validation score: 0.829283\n",
            "Iteration 37, loss = 0.40267820\n",
            "Validation score: 0.829421\n",
            "Iteration 38, loss = 0.40261745\n",
            "Validation score: 0.829145\n",
            "Iteration 39, loss = 0.40255769\n",
            "Validation score: 0.829214\n",
            "Iteration 40, loss = 0.40248209\n",
            "Validation score: 0.829329\n",
            "Iteration 41, loss = 0.40244412\n",
            "Validation score: 0.829122\n",
            "Validation score did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\sklearn\\base.py:458: UserWarning: X has feature names, but MLPClassifier was fitted without feature names\n",
            "  warnings.warn(\n",
            "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\sklearn\\base.py:458: UserWarning: X has feature names, but MLPClassifier was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 4\n",
            "Training the model with scratch implementation\n",
            "Epoch 1/50\n",
            "2178/2178 [==============================] - 6s 2ms/step - loss: 0.6088 - accuracy: 0.7817 - val_loss: 0.5527 - val_accuracy: 0.8224\n",
            "Epoch 2/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.5188 - accuracy: 0.8286 - val_loss: 0.4981 - val_accuracy: 0.8256\n",
            "Epoch 3/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4772 - accuracy: 0.8299 - val_loss: 0.4667 - val_accuracy: 0.8260\n",
            "Epoch 4/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4523 - accuracy: 0.8302 - val_loss: 0.4479 - val_accuracy: 0.8262\n",
            "Epoch 5/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4371 - accuracy: 0.8304 - val_loss: 0.4364 - val_accuracy: 0.8264\n",
            "Epoch 6/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4279 - accuracy: 0.8304 - val_loss: 0.4296 - val_accuracy: 0.8265\n",
            "Epoch 7/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4225 - accuracy: 0.8304 - val_loss: 0.4256 - val_accuracy: 0.8264\n",
            "Epoch 8/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4193 - accuracy: 0.8303 - val_loss: 0.4233 - val_accuracy: 0.8266\n",
            "Epoch 9/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4173 - accuracy: 0.8305 - val_loss: 0.4216 - val_accuracy: 0.8268\n",
            "Epoch 10/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4158 - accuracy: 0.8306 - val_loss: 0.4207 - val_accuracy: 0.8268\n",
            "Epoch 11/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4149 - accuracy: 0.8305 - val_loss: 0.4197 - val_accuracy: 0.8268\n",
            "Epoch 12/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4141 - accuracy: 0.8306 - val_loss: 0.4191 - val_accuracy: 0.8264\n",
            "Epoch 13/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4136 - accuracy: 0.8307 - val_loss: 0.4186 - val_accuracy: 0.8266\n",
            "Epoch 14/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4131 - accuracy: 0.8306 - val_loss: 0.4183 - val_accuracy: 0.8266\n",
            "Epoch 15/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4127 - accuracy: 0.8306 - val_loss: 0.4187 - val_accuracy: 0.8267\n",
            "Epoch 16/50\n",
            "2178/2178 [==============================] - 6s 3ms/step - loss: 0.4124 - accuracy: 0.8306 - val_loss: 0.4176 - val_accuracy: 0.8269\n",
            "Epoch 17/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4121 - accuracy: 0.8308 - val_loss: 0.4176 - val_accuracy: 0.8267\n",
            "Epoch 18/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4118 - accuracy: 0.8309 - val_loss: 0.4182 - val_accuracy: 0.8268\n",
            "Epoch 19/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4116 - accuracy: 0.8308 - val_loss: 0.4170 - val_accuracy: 0.8264\n",
            "Epoch 20/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4114 - accuracy: 0.8310 - val_loss: 0.4167 - val_accuracy: 0.8266\n",
            "Epoch 21/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4112 - accuracy: 0.8309 - val_loss: 0.4164 - val_accuracy: 0.8269\n",
            "Epoch 22/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4111 - accuracy: 0.8308 - val_loss: 0.4167 - val_accuracy: 0.8264\n",
            "Epoch 23/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4109 - accuracy: 0.8309 - val_loss: 0.4165 - val_accuracy: 0.8267\n",
            "Epoch 24/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4108 - accuracy: 0.8310 - val_loss: 0.4163 - val_accuracy: 0.8265\n",
            "Epoch 25/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4107 - accuracy: 0.8308 - val_loss: 0.4162 - val_accuracy: 0.8265\n",
            "Epoch 26/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4105 - accuracy: 0.8309 - val_loss: 0.4163 - val_accuracy: 0.8264\n",
            "Epoch 27/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4105 - accuracy: 0.8309 - val_loss: 0.4159 - val_accuracy: 0.8267\n",
            "Epoch 28/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4103 - accuracy: 0.8309 - val_loss: 0.4162 - val_accuracy: 0.8271\n",
            "Epoch 29/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4102 - accuracy: 0.8310 - val_loss: 0.4156 - val_accuracy: 0.8270\n",
            "Epoch 30/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4102 - accuracy: 0.8310 - val_loss: 0.4156 - val_accuracy: 0.8269\n",
            "Epoch 31/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4100 - accuracy: 0.8309 - val_loss: 0.4156 - val_accuracy: 0.8266\n",
            "Epoch 32/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4100 - accuracy: 0.8310 - val_loss: 0.4156 - val_accuracy: 0.8270\n",
            "Epoch 33/50\n",
            "2178/2178 [==============================] - 6s 3ms/step - loss: 0.4099 - accuracy: 0.8312 - val_loss: 0.4153 - val_accuracy: 0.8269\n",
            "Epoch 34/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4099 - accuracy: 0.8311 - val_loss: 0.4153 - val_accuracy: 0.8268\n",
            "Epoch 35/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4098 - accuracy: 0.8309 - val_loss: 0.4155 - val_accuracy: 0.8266\n",
            "Epoch 36/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4098 - accuracy: 0.8310 - val_loss: 0.4155 - val_accuracy: 0.8268\n",
            "Epoch 37/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4096 - accuracy: 0.8310 - val_loss: 0.4156 - val_accuracy: 0.8269\n",
            "Epoch 38/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4096 - accuracy: 0.8311 - val_loss: 0.4154 - val_accuracy: 0.8270\n",
            "Epoch 39/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4096 - accuracy: 0.8310 - val_loss: 0.4153 - val_accuracy: 0.8268\n",
            "Epoch 40/50\n",
            "2178/2178 [==============================] - 6s 3ms/step - loss: 0.4095 - accuracy: 0.8310 - val_loss: 0.4151 - val_accuracy: 0.8270\n",
            "Epoch 41/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4094 - accuracy: 0.8310 - val_loss: 0.4149 - val_accuracy: 0.8270\n",
            "Epoch 42/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4093 - accuracy: 0.8310 - val_loss: 0.4150 - val_accuracy: 0.8267\n",
            "Epoch 43/50\n",
            "2178/2178 [==============================] - 6s 3ms/step - loss: 0.4093 - accuracy: 0.8312 - val_loss: 0.4151 - val_accuracy: 0.8268\n",
            "Epoch 44/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4092 - accuracy: 0.8309 - val_loss: 0.4149 - val_accuracy: 0.8268\n",
            "Epoch 45/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4092 - accuracy: 0.8311 - val_loss: 0.4149 - val_accuracy: 0.8267\n",
            "Epoch 46/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4092 - accuracy: 0.8311 - val_loss: 0.4149 - val_accuracy: 0.8271\n",
            "Epoch 47/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4092 - accuracy: 0.8310 - val_loss: 0.4148 - val_accuracy: 0.8269\n",
            "Epoch 48/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4091 - accuracy: 0.8311 - val_loss: 0.4148 - val_accuracy: 0.8269\n",
            "Epoch 49/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4090 - accuracy: 0.8311 - val_loss: 0.4146 - val_accuracy: 0.8270\n",
            "Epoch 50/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4090 - accuracy: 0.8311 - val_loss: 0.4149 - val_accuracy: 0.8267\n",
            "13612/13612 [==============================] - 22s 2ms/step\n",
            "3403/3403 [==============================] - 4s 1ms/step\n",
            "7292/7292 [==============================] - 9s 1ms/step\n",
            "Training the model with sklearn implementation\n",
            "Iteration 1, loss = 0.56793812\n",
            "Validation score: 0.817276\n",
            "Iteration 2, loss = 0.42913569\n",
            "Validation score: 0.824841\n",
            "Iteration 3, loss = 0.41660128\n",
            "Validation score: 0.827332\n",
            "Iteration 4, loss = 0.41197260\n",
            "Validation score: 0.827619\n",
            "Iteration 5, loss = 0.40973904\n",
            "Validation score: 0.828296\n",
            "Iteration 6, loss = 0.40833502\n",
            "Validation score: 0.828698\n",
            "Iteration 7, loss = 0.40744603\n",
            "Validation score: 0.828973\n",
            "Iteration 8, loss = 0.40681482\n",
            "Validation score: 0.829570\n",
            "Iteration 9, loss = 0.40629723\n",
            "Validation score: 0.829249\n",
            "Iteration 10, loss = 0.40590199\n",
            "Validation score: 0.829099\n",
            "Iteration 11, loss = 0.40561506\n",
            "Validation score: 0.829800\n",
            "Iteration 12, loss = 0.40533460\n",
            "Validation score: 0.829719\n",
            "Iteration 13, loss = 0.40509014\n",
            "Validation score: 0.829432\n",
            "Iteration 14, loss = 0.40487213\n",
            "Validation score: 0.829272\n",
            "Iteration 15, loss = 0.40471102\n",
            "Validation score: 0.830041\n",
            "Iteration 16, loss = 0.40454868\n",
            "Validation score: 0.829467\n",
            "Iteration 17, loss = 0.40441137\n",
            "Validation score: 0.829903\n",
            "Iteration 18, loss = 0.40425025\n",
            "Validation score: 0.830213\n",
            "Iteration 19, loss = 0.40415635\n",
            "Validation score: 0.830041\n",
            "Iteration 20, loss = 0.40403683\n",
            "Validation score: 0.830282\n",
            "Iteration 21, loss = 0.40395009\n",
            "Validation score: 0.830408\n",
            "Iteration 22, loss = 0.40384044\n",
            "Validation score: 0.830488\n",
            "Iteration 23, loss = 0.40372167\n",
            "Validation score: 0.830477\n",
            "Iteration 24, loss = 0.40365593\n",
            "Validation score: 0.830374\n",
            "Iteration 25, loss = 0.40356700\n",
            "Validation score: 0.830523\n",
            "Iteration 26, loss = 0.40346018\n",
            "Validation score: 0.830144\n",
            "Iteration 27, loss = 0.40340029\n",
            "Validation score: 0.830454\n",
            "Iteration 28, loss = 0.40329900\n",
            "Validation score: 0.830305\n",
            "Iteration 29, loss = 0.40323891\n",
            "Validation score: 0.830293\n",
            "Iteration 30, loss = 0.40318395\n",
            "Validation score: 0.830385\n",
            "Iteration 31, loss = 0.40309733\n",
            "Validation score: 0.830397\n",
            "Iteration 32, loss = 0.40304629\n",
            "Validation score: 0.830362\n",
            "Iteration 33, loss = 0.40297959\n",
            "Validation score: 0.830374\n",
            "Iteration 34, loss = 0.40291243\n",
            "Validation score: 0.830052\n",
            "Iteration 35, loss = 0.40284187\n",
            "Validation score: 0.830385\n",
            "Iteration 36, loss = 0.40277276\n",
            "Validation score: 0.830454\n",
            "Validation score did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\sklearn\\base.py:458: UserWarning: X has feature names, but MLPClassifier was fitted without feature names\n",
            "  warnings.warn(\n",
            "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\sklearn\\base.py:458: UserWarning: X has feature names, but MLPClassifier was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 5\n",
            "Training the model with scratch implementation\n",
            "Epoch 1/50\n",
            "2178/2178 [==============================] - 6s 2ms/step - loss: 0.5156 - accuracy: 0.8000 - val_loss: 0.4564 - val_accuracy: 0.8267\n",
            "Epoch 2/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4493 - accuracy: 0.8272 - val_loss: 0.4391 - val_accuracy: 0.8297\n",
            "Epoch 3/50\n",
            "2178/2178 [==============================] - 5s 3ms/step - loss: 0.4378 - accuracy: 0.8285 - val_loss: 0.4309 - val_accuracy: 0.8302\n",
            "Epoch 4/50\n",
            "2178/2178 [==============================] - 8s 3ms/step - loss: 0.4315 - accuracy: 0.8288 - val_loss: 0.4263 - val_accuracy: 0.8301\n",
            "Epoch 5/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4275 - accuracy: 0.8289 - val_loss: 0.4229 - val_accuracy: 0.8306\n",
            "Epoch 6/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4247 - accuracy: 0.8291 - val_loss: 0.4207 - val_accuracy: 0.8306\n",
            "Epoch 7/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4227 - accuracy: 0.8293 - val_loss: 0.4191 - val_accuracy: 0.8306\n",
            "Epoch 8/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4211 - accuracy: 0.8293 - val_loss: 0.4175 - val_accuracy: 0.8309\n",
            "Epoch 9/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4199 - accuracy: 0.8295 - val_loss: 0.4163 - val_accuracy: 0.8308\n",
            "Epoch 10/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4188 - accuracy: 0.8294 - val_loss: 0.4155 - val_accuracy: 0.8309\n",
            "Epoch 11/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4179 - accuracy: 0.8293 - val_loss: 0.4160 - val_accuracy: 0.8301\n",
            "Epoch 12/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4172 - accuracy: 0.8295 - val_loss: 0.4145 - val_accuracy: 0.8307\n",
            "Epoch 13/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4165 - accuracy: 0.8295 - val_loss: 0.4138 - val_accuracy: 0.8306\n",
            "Epoch 14/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4160 - accuracy: 0.8294 - val_loss: 0.4131 - val_accuracy: 0.8307\n",
            "Epoch 15/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4156 - accuracy: 0.8295 - val_loss: 0.4125 - val_accuracy: 0.8311\n",
            "Epoch 16/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4151 - accuracy: 0.8294 - val_loss: 0.4122 - val_accuracy: 0.8308\n",
            "Epoch 17/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4147 - accuracy: 0.8296 - val_loss: 0.4116 - val_accuracy: 0.8313\n",
            "Epoch 18/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4144 - accuracy: 0.8295 - val_loss: 0.4115 - val_accuracy: 0.8313\n",
            "Epoch 19/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4141 - accuracy: 0.8294 - val_loss: 0.4112 - val_accuracy: 0.8312\n",
            "Epoch 20/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4138 - accuracy: 0.8296 - val_loss: 0.4113 - val_accuracy: 0.8312\n",
            "Epoch 21/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4136 - accuracy: 0.8297 - val_loss: 0.4108 - val_accuracy: 0.8310\n",
            "Epoch 22/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4133 - accuracy: 0.8295 - val_loss: 0.4106 - val_accuracy: 0.8309\n",
            "Epoch 23/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4131 - accuracy: 0.8295 - val_loss: 0.4107 - val_accuracy: 0.8309\n",
            "Epoch 24/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4129 - accuracy: 0.8296 - val_loss: 0.4106 - val_accuracy: 0.8307\n",
            "Epoch 25/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4127 - accuracy: 0.8295 - val_loss: 0.4103 - val_accuracy: 0.8307\n",
            "Epoch 26/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4125 - accuracy: 0.8297 - val_loss: 0.4098 - val_accuracy: 0.8310\n",
            "Epoch 27/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4124 - accuracy: 0.8297 - val_loss: 0.4098 - val_accuracy: 0.8311\n",
            "Epoch 28/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4122 - accuracy: 0.8297 - val_loss: 0.4096 - val_accuracy: 0.8309\n",
            "Epoch 29/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4121 - accuracy: 0.8297 - val_loss: 0.4099 - val_accuracy: 0.8310\n",
            "Epoch 30/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4119 - accuracy: 0.8297 - val_loss: 0.4098 - val_accuracy: 0.8307\n",
            "Epoch 31/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4118 - accuracy: 0.8298 - val_loss: 0.4090 - val_accuracy: 0.8311\n",
            "Epoch 32/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4117 - accuracy: 0.8299 - val_loss: 0.4091 - val_accuracy: 0.8311\n",
            "Epoch 33/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4115 - accuracy: 0.8299 - val_loss: 0.4088 - val_accuracy: 0.8311\n",
            "Epoch 34/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4114 - accuracy: 0.8297 - val_loss: 0.4091 - val_accuracy: 0.8318\n",
            "Epoch 35/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4113 - accuracy: 0.8298 - val_loss: 0.4085 - val_accuracy: 0.8312\n",
            "Epoch 36/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4112 - accuracy: 0.8299 - val_loss: 0.4089 - val_accuracy: 0.8312\n",
            "Epoch 37/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4111 - accuracy: 0.8298 - val_loss: 0.4091 - val_accuracy: 0.8311\n",
            "Epoch 38/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4110 - accuracy: 0.8299 - val_loss: 0.4084 - val_accuracy: 0.8313\n",
            "Epoch 39/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4109 - accuracy: 0.8297 - val_loss: 0.4082 - val_accuracy: 0.8311\n",
            "Epoch 40/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4108 - accuracy: 0.8300 - val_loss: 0.4080 - val_accuracy: 0.8315\n",
            "Epoch 41/50\n",
            "2178/2178 [==============================] - 8s 3ms/step - loss: 0.4107 - accuracy: 0.8299 - val_loss: 0.4080 - val_accuracy: 0.8310\n",
            "Epoch 42/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4106 - accuracy: 0.8298 - val_loss: 0.4084 - val_accuracy: 0.8314\n",
            "Epoch 43/50\n",
            "2178/2178 [==============================] - 8s 4ms/step - loss: 0.4106 - accuracy: 0.8301 - val_loss: 0.4081 - val_accuracy: 0.8312\n",
            "Epoch 44/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4105 - accuracy: 0.8298 - val_loss: 0.4079 - val_accuracy: 0.8311\n",
            "Epoch 45/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4104 - accuracy: 0.8298 - val_loss: 0.4084 - val_accuracy: 0.8317\n",
            "Epoch 46/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4104 - accuracy: 0.8299 - val_loss: 0.4080 - val_accuracy: 0.8316\n",
            "Epoch 47/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4103 - accuracy: 0.8299 - val_loss: 0.4077 - val_accuracy: 0.8315\n",
            "Epoch 48/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4102 - accuracy: 0.8300 - val_loss: 0.4077 - val_accuracy: 0.8313\n",
            "Epoch 49/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4102 - accuracy: 0.8299 - val_loss: 0.4086 - val_accuracy: 0.8304\n",
            "Epoch 50/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4101 - accuracy: 0.8300 - val_loss: 0.4076 - val_accuracy: 0.8315\n",
            "13612/13612 [==============================] - 18s 1ms/step\n",
            "3403/3403 [==============================] - 5s 1ms/step\n",
            "7292/7292 [==============================] - 9s 1ms/step\n",
            "Training the model with sklearn implementation\n",
            "Iteration 1, loss = 0.49690797\n",
            "Validation score: 0.816404\n",
            "Iteration 2, loss = 0.42381400\n",
            "Validation score: 0.820984\n",
            "Iteration 3, loss = 0.41590095\n",
            "Validation score: 0.823417\n",
            "Iteration 4, loss = 0.41197680\n",
            "Validation score: 0.825116\n",
            "Iteration 5, loss = 0.40970108\n",
            "Validation score: 0.825346\n",
            "Iteration 6, loss = 0.40825804\n",
            "Validation score: 0.825713\n",
            "Iteration 7, loss = 0.40733827\n",
            "Validation score: 0.826356\n",
            "Iteration 8, loss = 0.40665578\n",
            "Validation score: 0.826322\n",
            "Iteration 9, loss = 0.40619661\n",
            "Validation score: 0.826677\n",
            "Iteration 10, loss = 0.40583494\n",
            "Validation score: 0.826930\n",
            "Iteration 11, loss = 0.40553889\n",
            "Validation score: 0.827217\n",
            "Iteration 12, loss = 0.40521927\n",
            "Validation score: 0.827366\n",
            "Iteration 13, loss = 0.40501450\n",
            "Validation score: 0.827309\n",
            "Iteration 14, loss = 0.40479284\n",
            "Validation score: 0.827251\n",
            "Iteration 15, loss = 0.40463052\n",
            "Validation score: 0.827458\n",
            "Iteration 16, loss = 0.40443928\n",
            "Validation score: 0.827515\n",
            "Iteration 17, loss = 0.40432469\n",
            "Validation score: 0.827492\n",
            "Iteration 18, loss = 0.40418994\n",
            "Validation score: 0.827343\n",
            "Iteration 19, loss = 0.40402773\n",
            "Validation score: 0.827653\n",
            "Iteration 20, loss = 0.40393580\n",
            "Validation score: 0.827653\n",
            "Iteration 21, loss = 0.40379933\n",
            "Validation score: 0.827343\n",
            "Iteration 22, loss = 0.40370545\n",
            "Validation score: 0.827412\n",
            "Iteration 23, loss = 0.40357513\n",
            "Validation score: 0.827504\n",
            "Iteration 24, loss = 0.40349150\n",
            "Validation score: 0.827492\n",
            "Iteration 25, loss = 0.40342026\n",
            "Validation score: 0.827527\n",
            "Iteration 26, loss = 0.40332175\n",
            "Validation score: 0.827550\n",
            "Iteration 27, loss = 0.40322628\n",
            "Validation score: 0.827676\n",
            "Iteration 28, loss = 0.40312325\n",
            "Validation score: 0.827642\n",
            "Iteration 29, loss = 0.40305858\n",
            "Validation score: 0.827630\n",
            "Iteration 30, loss = 0.40295920\n",
            "Validation score: 0.827733\n",
            "Iteration 31, loss = 0.40290718\n",
            "Validation score: 0.827756\n",
            "Iteration 32, loss = 0.40282326\n",
            "Validation score: 0.827665\n",
            "Iteration 33, loss = 0.40279874\n",
            "Validation score: 0.827676\n",
            "Iteration 34, loss = 0.40272144\n",
            "Validation score: 0.827768\n",
            "Iteration 35, loss = 0.40264191\n",
            "Validation score: 0.827630\n",
            "Iteration 36, loss = 0.40264397\n",
            "Validation score: 0.827699\n",
            "Iteration 37, loss = 0.40255077\n",
            "Validation score: 0.827710\n",
            "Iteration 38, loss = 0.40253414\n",
            "Validation score: 0.827860\n",
            "Iteration 39, loss = 0.40246885\n",
            "Validation score: 0.827665\n",
            "Iteration 40, loss = 0.40242373\n",
            "Validation score: 0.827906\n",
            "Iteration 41, loss = 0.40238509\n",
            "Validation score: 0.827997\n",
            "Iteration 42, loss = 0.40234560\n",
            "Validation score: 0.828020\n",
            "Iteration 43, loss = 0.40228951\n",
            "Validation score: 0.827848\n",
            "Iteration 44, loss = 0.40225499\n",
            "Validation score: 0.827906\n",
            "Iteration 45, loss = 0.40220210\n",
            "Validation score: 0.827860\n",
            "Iteration 46, loss = 0.40216570\n",
            "Validation score: 0.827848\n",
            "Iteration 47, loss = 0.40215267\n",
            "Validation score: 0.827940\n",
            "Iteration 48, loss = 0.40212451\n",
            "Validation score: 0.827802\n",
            "Iteration 49, loss = 0.40209000\n",
            "Validation score: 0.827676\n",
            "Iteration 50, loss = 0.40202681\n",
            "Validation score: 0.827779\n",
            "Iteration 51, loss = 0.40200507\n",
            "Validation score: 0.827733\n",
            "Iteration 52, loss = 0.40200503\n",
            "Validation score: 0.827733\n",
            "Iteration 53, loss = 0.40195983\n",
            "Validation score: 0.827619\n",
            "Validation score did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\sklearn\\base.py:458: UserWarning: X has feature names, but MLPClassifier was fitted without feature names\n",
            "  warnings.warn(\n",
            "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\sklearn\\base.py:458: UserWarning: X has feature names, but MLPClassifier was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "y_train_pred_list = []\n",
        "y_val_pred_list = []\n",
        "y_test_pred_list = []\n",
        "\n",
        "y_train_list = []\n",
        "y_val_list = []\n",
        "\n",
        "y_train_pred_list_sk = []\n",
        "y_val_pred_list_sk = []\n",
        "y_test_pred_list_sk = []\n",
        "\n",
        "#Kfold\n",
        "num_folds = 5\n",
        "fold_size = len(X_train) // num_folds\n",
        "X_train_cv = X_train\n",
        "y_train_cv = y_train\n",
        "\n",
        "for fold in range(num_folds):\n",
        "    print(f\"Fold {fold+1}\")\n",
        "    # Define the start and end index for the validation set\n",
        "    start_val_index = fold * fold_size\n",
        "    end_val_index = (fold + 1) * fold_size\n",
        "\n",
        "    # New training set\n",
        "    X_train = np.append(X_train_cv[:start_val_index], X_train_cv[end_val_index:], axis=0)\n",
        "    y_train = np.append(y_train_cv[:start_val_index], y_train_cv[end_val_index:])\n",
        "    y_train_list.append(y_train)\n",
        "\n",
        "    X_val = X_train_cv[start_val_index:end_val_index]\n",
        "    y_val = y_train_cv[start_val_index:end_val_index]\n",
        "    y_val_list.append(y_val)\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(23, kernel_regularizer=regularizers.l2(0.001), activation='relu', input_shape=(X_train.shape[1],)),\n",
        "            tf.keras.layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu'),\n",
        "            tf.keras.layers.Dense(8, kernel_regularizer=regularizers.l2(0.001), activation='relu'),\n",
        "            tf.keras.layers.Dense(4, kernel_regularizer=regularizers.l2(0.001), activation='relu'),\n",
        "            tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "    ])\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    print('Training the model with scratch implementation')\n",
        "    history = model.fit(X_train, y_train, epochs=50, batch_size=200, validation_data=(X_val, y_val))\n",
        "\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_val_pred = model.predict(X_val)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    y_train_pred_list.append(y_train_pred)\n",
        "    y_val_pred_list.append(y_val_pred)\n",
        "    y_test_pred_list.append(y_test_pred)\n",
        "\n",
        "    print('Training the model with sklearn implementation')\n",
        "    model_sk = MLPClassifier(hidden_layer_sizes=(16,8,4), tol=0.00001, alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.0002, validation_fraction=0.2, verbose = True, early_stopping=True, activation='relu', solver='adam', max_iter=200)\n",
        "    model_history = model_sk.fit(X_train,y_train)\n",
        "\n",
        "    y_train_pred_sk = model_sk.predict(X_train)\n",
        "    y_val_pred_sk = model_sk.predict(X_val)\n",
        "    y_test_pred_sk = model_sk.predict(X_test)\n",
        "    y_train_pred_list_sk.append(y_train_pred_sk)\n",
        "    y_val_pred_list_sk.append(y_val_pred_sk)\n",
        "    y_test_pred_list_sk.append(y_test_pred_sk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAaZulLTulON",
        "outputId": "071ec500-db7c-44ce-a33b-4e895d781d68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*************************************************************************************Fold1*********************************************************************************************\n",
            "Multi layer Perceptron from Scratch Model\n",
            "Training Accuracy fold1 scratch model: 0.830\n",
            "Validation Accuracy fold1 scratch model: 0.831\n",
            "Testing Accuracy fold1 scratch model: 0.830\n",
            "Classification Report training scratch model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    218060\n",
            "           1       0.90      0.74      0.81    217514\n",
            "\n",
            "    accuracy                           0.83    435574\n",
            "   macro avg       0.84      0.83      0.83    435574\n",
            "weighted avg       0.84      0.83      0.83    435574\n",
            "\n",
            "Classification Report validation scratch model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.85     54456\n",
            "           1       0.91      0.74      0.81     54437\n",
            "\n",
            "    accuracy                           0.83    108893\n",
            "   macro avg       0.84      0.83      0.83    108893\n",
            "weighted avg       0.84      0.83      0.83    108893\n",
            "\n",
            "Classification Report testing scratch model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    116389\n",
            "           1       0.91      0.74      0.81    116954\n",
            "\n",
            "    accuracy                           0.83    233343\n",
            "   macro avg       0.84      0.83      0.83    233343\n",
            "weighted avg       0.84      0.83      0.83    233343\n",
            "\n",
            "Sklearn Multi layer Perceptron Model\n",
            "Training Accuracy fold1 Sklearn model: 0.830\n",
            "Validation Accuracy fold1 Sklearn model: 0.831\n",
            "Testing Accuracy fold1 Sklearn model: 0.830\n",
            "Classification Report training Sklearn model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    218060\n",
            "           1       0.90      0.74      0.81    217514\n",
            "\n",
            "    accuracy                           0.83    435574\n",
            "   macro avg       0.84      0.83      0.83    435574\n",
            "weighted avg       0.84      0.83      0.83    435574\n",
            "\n",
            "Classification Report validation Sklearn model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84     54456\n",
            "           1       0.90      0.74      0.81     54437\n",
            "\n",
            "    accuracy                           0.83    108893\n",
            "   macro avg       0.84      0.83      0.83    108893\n",
            "weighted avg       0.84      0.83      0.83    108893\n",
            "\n",
            "Classification Report testing Sklearn model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    116389\n",
            "           1       0.90      0.74      0.81    116954\n",
            "\n",
            "    accuracy                           0.83    233343\n",
            "   macro avg       0.84      0.83      0.83    233343\n",
            "weighted avg       0.84      0.83      0.83    233343\n",
            "\n",
            "**********************************************************************************************************************************************************************************\n",
            "*************************************************************************************Fold2*********************************************************************************************\n",
            "Multi layer Perceptron from Scratch Model\n",
            "Training Accuracy fold2 scratch model: 0.830\n",
            "Validation Accuracy fold2 scratch model: 0.831\n",
            "Testing Accuracy fold2 scratch model: 0.830\n",
            "Classification Report training scratch model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.91      0.84    218068\n",
            "           1       0.90      0.75      0.81    217506\n",
            "\n",
            "    accuracy                           0.83    435574\n",
            "   macro avg       0.84      0.83      0.83    435574\n",
            "weighted avg       0.84      0.83      0.83    435574\n",
            "\n",
            "Classification Report validation scratch model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.91      0.84     54448\n",
            "           1       0.90      0.75      0.82     54445\n",
            "\n",
            "    accuracy                           0.83    108893\n",
            "   macro avg       0.84      0.83      0.83    108893\n",
            "weighted avg       0.84      0.83      0.83    108893\n",
            "\n",
            "Classification Report testing scratch model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.91      0.84    116389\n",
            "           1       0.90      0.75      0.81    116954\n",
            "\n",
            "    accuracy                           0.83    233343\n",
            "   macro avg       0.84      0.83      0.83    233343\n",
            "weighted avg       0.84      0.83      0.83    233343\n",
            "\n",
            "Sklearn Multi layer Perceptron Model\n",
            "Training Accuracy fold2 Sklearn model: 0.829\n",
            "Validation Accuracy fold2 Sklearn model: 0.831\n",
            "Testing Accuracy fold2 Sklearn model: 0.829\n",
            "Classification Report training Sklearn model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    218068\n",
            "           1       0.91      0.74      0.81    217506\n",
            "\n",
            "    accuracy                           0.83    435574\n",
            "   macro avg       0.84      0.83      0.83    435574\n",
            "weighted avg       0.84      0.83      0.83    435574\n",
            "\n",
            "Classification Report validation Sklearn model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.85     54448\n",
            "           1       0.91      0.74      0.81     54445\n",
            "\n",
            "    accuracy                           0.83    108893\n",
            "   macro avg       0.84      0.83      0.83    108893\n",
            "weighted avg       0.84      0.83      0.83    108893\n",
            "\n",
            "Classification Report testing Sklearn model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    116389\n",
            "           1       0.91      0.74      0.81    116954\n",
            "\n",
            "    accuracy                           0.83    233343\n",
            "   macro avg       0.84      0.83      0.83    233343\n",
            "weighted avg       0.84      0.83      0.83    233343\n",
            "\n",
            "**********************************************************************************************************************************************************************************\n",
            "*************************************************************************************Fold3*********************************************************************************************\n",
            "Multi layer Perceptron from Scratch Model\n",
            "Training Accuracy fold3 scratch model: 0.830\n",
            "Validation Accuracy fold3 scratch model: 0.830\n",
            "Testing Accuracy fold3 scratch model: 0.830\n",
            "Classification Report training scratch model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.91      0.84    217921\n",
            "           1       0.90      0.75      0.81    217653\n",
            "\n",
            "    accuracy                           0.83    435574\n",
            "   macro avg       0.84      0.83      0.83    435574\n",
            "weighted avg       0.84      0.83      0.83    435574\n",
            "\n",
            "Classification Report validation scratch model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.91      0.84     54595\n",
            "           1       0.89      0.75      0.81     54298\n",
            "\n",
            "    accuracy                           0.83    108893\n",
            "   macro avg       0.84      0.83      0.83    108893\n",
            "weighted avg       0.84      0.83      0.83    108893\n",
            "\n",
            "Classification Report testing scratch model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.91      0.84    116389\n",
            "           1       0.90      0.75      0.81    116954\n",
            "\n",
            "    accuracy                           0.83    233343\n",
            "   macro avg       0.84      0.83      0.83    233343\n",
            "weighted avg       0.84      0.83      0.83    233343\n",
            "\n",
            "Sklearn Multi layer Perceptron Model\n",
            "Training Accuracy fold3 Sklearn model: 0.830\n",
            "Validation Accuracy fold3 Sklearn model: 0.829\n",
            "Testing Accuracy fold3 Sklearn model: 0.829\n",
            "Classification Report training Sklearn model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    217921\n",
            "           1       0.90      0.74      0.81    217653\n",
            "\n",
            "    accuracy                           0.83    435574\n",
            "   macro avg       0.84      0.83      0.83    435574\n",
            "weighted avg       0.84      0.83      0.83    435574\n",
            "\n",
            "Classification Report validation Sklearn model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84     54595\n",
            "           1       0.90      0.74      0.81     54298\n",
            "\n",
            "    accuracy                           0.83    108893\n",
            "   macro avg       0.84      0.83      0.83    108893\n",
            "weighted avg       0.84      0.83      0.83    108893\n",
            "\n",
            "Classification Report testing Sklearn model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    116389\n",
            "           1       0.90      0.74      0.81    116954\n",
            "\n",
            "    accuracy                           0.83    233343\n",
            "   macro avg       0.84      0.83      0.83    233343\n",
            "weighted avg       0.84      0.83      0.83    233343\n",
            "\n",
            "**********************************************************************************************************************************************************************************\n",
            "*************************************************************************************Fold4*********************************************************************************************\n",
            "Multi layer Perceptron from Scratch Model\n",
            "Training Accuracy fold4 scratch model: 0.831\n",
            "Validation Accuracy fold4 scratch model: 0.827\n",
            "Testing Accuracy fold4 scratch model: 0.830\n",
            "Classification Report training scratch model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    218017\n",
            "           1       0.90      0.74      0.81    217557\n",
            "\n",
            "    accuracy                           0.83    435574\n",
            "   macro avg       0.84      0.83      0.83    435574\n",
            "weighted avg       0.84      0.83      0.83    435574\n",
            "\n",
            "Classification Report validation scratch model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84     54499\n",
            "           1       0.90      0.74      0.81     54394\n",
            "\n",
            "    accuracy                           0.83    108893\n",
            "   macro avg       0.84      0.83      0.83    108893\n",
            "weighted avg       0.84      0.83      0.83    108893\n",
            "\n",
            "Classification Report testing scratch model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    116389\n",
            "           1       0.90      0.74      0.81    116954\n",
            "\n",
            "    accuracy                           0.83    233343\n",
            "   macro avg       0.84      0.83      0.83    233343\n",
            "weighted avg       0.84      0.83      0.83    233343\n",
            "\n",
            "Sklearn Multi layer Perceptron Model\n",
            "Training Accuracy fold4 Sklearn model: 0.831\n",
            "Validation Accuracy fold4 Sklearn model: 0.826\n",
            "Testing Accuracy fold4 Sklearn model: 0.829\n",
            "Classification Report training Sklearn model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    218017\n",
            "           1       0.90      0.74      0.81    217557\n",
            "\n",
            "    accuracy                           0.83    435574\n",
            "   macro avg       0.84      0.83      0.83    435574\n",
            "weighted avg       0.84      0.83      0.83    435574\n",
            "\n",
            "Classification Report validation Sklearn model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84     54499\n",
            "           1       0.90      0.74      0.81     54394\n",
            "\n",
            "    accuracy                           0.83    108893\n",
            "   macro avg       0.84      0.83      0.82    108893\n",
            "weighted avg       0.84      0.83      0.82    108893\n",
            "\n",
            "Classification Report testing Sklearn model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    116389\n",
            "           1       0.90      0.74      0.81    116954\n",
            "\n",
            "    accuracy                           0.83    233343\n",
            "   macro avg       0.84      0.83      0.83    233343\n",
            "weighted avg       0.84      0.83      0.83    233343\n",
            "\n",
            "**********************************************************************************************************************************************************************************\n",
            "*************************************************************************************Fold5*********************************************************************************************\n",
            "Multi layer Perceptron from Scratch Model\n",
            "Training Accuracy fold5 scratch model: 0.830\n",
            "Validation Accuracy fold5 scratch model: 0.832\n",
            "Testing Accuracy fold5 scratch model: 0.830\n",
            "Classification Report training scratch model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    217999\n",
            "           1       0.90      0.74      0.81    217575\n",
            "\n",
            "    accuracy                           0.83    435574\n",
            "   macro avg       0.84      0.83      0.83    435574\n",
            "weighted avg       0.84      0.83      0.83    435574\n",
            "\n",
            "Classification Report validation scratch model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.85     54517\n",
            "           1       0.90      0.74      0.81     54376\n",
            "\n",
            "    accuracy                           0.83    108893\n",
            "   macro avg       0.84      0.83      0.83    108893\n",
            "weighted avg       0.84      0.83      0.83    108893\n",
            "\n",
            "Classification Report testing scratch model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    116389\n",
            "           1       0.90      0.74      0.81    116954\n",
            "\n",
            "    accuracy                           0.83    233343\n",
            "   macro avg       0.84      0.83      0.83    233343\n",
            "weighted avg       0.84      0.83      0.83    233343\n",
            "\n",
            "Sklearn Multi layer Perceptron Model\n",
            "Training Accuracy fold5 Sklearn model: 0.830\n",
            "Validation Accuracy fold5 Sklearn model: 0.831\n",
            "Testing Accuracy fold5 Sklearn model: 0.829\n",
            "Classification Report training Sklearn model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    217999\n",
            "           1       0.90      0.74      0.81    217575\n",
            "\n",
            "    accuracy                           0.83    435574\n",
            "   macro avg       0.84      0.83      0.83    435574\n",
            "weighted avg       0.84      0.83      0.83    435574\n",
            "\n",
            "Classification Report validation Sklearn model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.85     54517\n",
            "           1       0.90      0.74      0.81     54376\n",
            "\n",
            "    accuracy                           0.83    108893\n",
            "   macro avg       0.84      0.83      0.83    108893\n",
            "weighted avg       0.84      0.83      0.83    108893\n",
            "\n",
            "Classification Report testing Sklearn model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84    116389\n",
            "           1       0.90      0.74      0.81    116954\n",
            "\n",
            "    accuracy                           0.83    233343\n",
            "   macro avg       0.84      0.83      0.83    233343\n",
            "weighted avg       0.84      0.83      0.83    233343\n",
            "\n",
            "**********************************************************************************************************************************************************************************\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "threshold = 0.5\n",
        "\n",
        "y_train_pred_list = np.array(y_train_pred_list) >= threshold\n",
        "y_val_pred_list = np.array(y_val_pred_list) >= threshold\n",
        "y_test_pred_list = np.array(y_test_pred_list) >= threshold\n",
        "\n",
        "for i in range((len(y_train_pred_list))):\n",
        "  training_accuracy = accuracy_score(y_train_list[i], y_train_pred_list[i])\n",
        "  validation_accuracy = accuracy_score(y_val_list[i], y_val_pred_list[i])\n",
        "  testing_accuracy = accuracy_score(y_test, y_test_pred_list[i])\n",
        "\n",
        "  training_report = classification_report(y_train_list[i], y_train_pred_list[i])\n",
        "  validation_report = classification_report(y_val_list[i], y_val_pred_list[i])\n",
        "  testing_report = classification_report(y_test, y_test_pred_list[i])\n",
        "\n",
        "  training_accuracy_sk = accuracy_score(y_train_list[i], y_train_pred_list_sk[i])\n",
        "  validation_accuracy_sk = accuracy_score(y_val_list[i], y_val_pred_list_sk[i])\n",
        "  testing_accuracy_sk = accuracy_score(y_test, y_test_pred_list_sk[i])\n",
        "\n",
        "  training_report_sk = classification_report(y_train_list[i], y_train_pred_list_sk[i])\n",
        "  validation_report_sk = classification_report(y_val_list[i], y_val_pred_list_sk[i])\n",
        "  testing_report_sk = classification_report(y_test, y_test_pred_list_sk[i])\n",
        "\n",
        "  print(f\"*************************************************************************************Fold{i+1}*********************************************************************************************\")\n",
        "  print(\"Multi layer Perceptron from Scratch Model\")\n",
        "  print(f\"Training Accuracy fold{i+1} scratch model: {training_accuracy:.3f}\")\n",
        "  print(f\"Validation Accuracy fold{i+1} scratch model: {validation_accuracy:.3f}\")\n",
        "  print(f\"Testing Accuracy fold{i+1} scratch model: {testing_accuracy:.3f}\")\n",
        "  print(\"Classification Report training scratch model:\\n\", training_report)\n",
        "  print(\"Classification Report validation scratch model:\\n\", validation_report)\n",
        "  print(\"Classification Report testing scratch model:\\n\", testing_report)\n",
        "  print(\"Sklearn Multi layer Perceptron Model\")\n",
        "  print(f\"Training Accuracy fold{i+1} Sklearn model: {training_accuracy_sk:.3f}\")\n",
        "  print(f\"Validation Accuracy fold{i+1} Sklearn model: {validation_accuracy_sk:.3f}\")\n",
        "  print(f\"Testing Accuracy fold{i+1} Sklearn model: {testing_accuracy_sk:.3f}\")\n",
        "  print(\"Classification Report training Sklearn model:\\n\", training_report_sk)\n",
        "  print(\"Classification Report validation Sklearn model:\\n\", validation_report_sk)\n",
        "  print(\"Classification Report testing Sklearn model:\\n\", testing_report_sk)\n",
        "  print(\"**********************************************************************************************************************************************************************************\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX7Vv673mQOB"
      },
      "source": [
        "#Statistical Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "1i_gbv83oBVF",
        "outputId": "45773e31-19f3-403d-aecf-7a9a8bc436b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "2178/2178 [==============================] - 6s 2ms/step - loss: 0.5611 - accuracy: 0.7690 - val_loss: 0.4635 - val_accuracy: 0.8249\n",
            "Epoch 2/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4516 - accuracy: 0.8260 - val_loss: 0.4391 - val_accuracy: 0.8285\n",
            "Epoch 3/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4362 - accuracy: 0.8278 - val_loss: 0.4295 - val_accuracy: 0.8291\n",
            "Epoch 4/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4293 - accuracy: 0.8284 - val_loss: 0.4244 - val_accuracy: 0.8303\n",
            "Epoch 5/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4254 - accuracy: 0.8287 - val_loss: 0.4216 - val_accuracy: 0.8298\n",
            "Epoch 6/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4229 - accuracy: 0.8287 - val_loss: 0.4193 - val_accuracy: 0.8306\n",
            "Epoch 7/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4210 - accuracy: 0.8289 - val_loss: 0.4176 - val_accuracy: 0.8306\n",
            "Epoch 8/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4195 - accuracy: 0.8291 - val_loss: 0.4166 - val_accuracy: 0.8308\n",
            "Epoch 9/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4184 - accuracy: 0.8291 - val_loss: 0.4154 - val_accuracy: 0.8309\n",
            "Epoch 10/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4175 - accuracy: 0.8292 - val_loss: 0.4146 - val_accuracy: 0.8309\n",
            "Epoch 11/50\n",
            "2178/2178 [==============================] - 6s 3ms/step - loss: 0.4168 - accuracy: 0.8293 - val_loss: 0.4139 - val_accuracy: 0.8307\n",
            "Epoch 12/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4161 - accuracy: 0.8292 - val_loss: 0.4134 - val_accuracy: 0.8310\n",
            "Epoch 13/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4156 - accuracy: 0.8294 - val_loss: 0.4128 - val_accuracy: 0.8309\n",
            "Epoch 14/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4151 - accuracy: 0.8294 - val_loss: 0.4123 - val_accuracy: 0.8310\n",
            "Epoch 15/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4146 - accuracy: 0.8294 - val_loss: 0.4120 - val_accuracy: 0.8308\n",
            "Epoch 16/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4143 - accuracy: 0.8295 - val_loss: 0.4118 - val_accuracy: 0.8311\n",
            "Epoch 17/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4140 - accuracy: 0.8296 - val_loss: 0.4116 - val_accuracy: 0.8306\n",
            "Epoch 18/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4137 - accuracy: 0.8297 - val_loss: 0.4109 - val_accuracy: 0.8309\n",
            "Epoch 19/50\n",
            "2178/2178 [==============================] - 5s 2ms/step - loss: 0.4133 - accuracy: 0.8295 - val_loss: 0.4106 - val_accuracy: 0.8311\n",
            "Epoch 20/50\n",
            "2178/2178 [==============================] - 6s 3ms/step - loss: 0.4132 - accuracy: 0.8294 - val_loss: 0.4108 - val_accuracy: 0.8308\n",
            "Epoch 21/50\n",
            "2178/2178 [==============================] - 8s 4ms/step - loss: 0.4130 - accuracy: 0.8296 - val_loss: 0.4104 - val_accuracy: 0.8310\n",
            "Epoch 22/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4127 - accuracy: 0.8295 - val_loss: 0.4102 - val_accuracy: 0.8307\n",
            "Epoch 23/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4124 - accuracy: 0.8297 - val_loss: 0.4102 - val_accuracy: 0.8312\n",
            "Epoch 24/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4123 - accuracy: 0.8296 - val_loss: 0.4098 - val_accuracy: 0.8308\n",
            "Epoch 25/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4122 - accuracy: 0.8297 - val_loss: 0.4096 - val_accuracy: 0.8308\n",
            "Epoch 26/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4120 - accuracy: 0.8297 - val_loss: 0.4094 - val_accuracy: 0.8308\n",
            "Epoch 27/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4118 - accuracy: 0.8295 - val_loss: 0.4091 - val_accuracy: 0.8308\n",
            "Epoch 28/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4117 - accuracy: 0.8296 - val_loss: 0.4093 - val_accuracy: 0.8310\n",
            "Epoch 29/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4116 - accuracy: 0.8297 - val_loss: 0.4094 - val_accuracy: 0.8309\n",
            "Epoch 30/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4114 - accuracy: 0.8296 - val_loss: 0.4090 - val_accuracy: 0.8313\n",
            "Epoch 31/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4113 - accuracy: 0.8296 - val_loss: 0.4090 - val_accuracy: 0.8306\n",
            "Epoch 32/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4112 - accuracy: 0.8296 - val_loss: 0.4089 - val_accuracy: 0.8312\n",
            "Epoch 33/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4111 - accuracy: 0.8297 - val_loss: 0.4088 - val_accuracy: 0.8307\n",
            "Epoch 34/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4110 - accuracy: 0.8297 - val_loss: 0.4083 - val_accuracy: 0.8312\n",
            "Epoch 35/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4109 - accuracy: 0.8296 - val_loss: 0.4083 - val_accuracy: 0.8309\n",
            "Epoch 36/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4108 - accuracy: 0.8297 - val_loss: 0.4080 - val_accuracy: 0.8312\n",
            "Epoch 37/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4107 - accuracy: 0.8297 - val_loss: 0.4081 - val_accuracy: 0.8311\n",
            "Epoch 38/50\n",
            "2178/2178 [==============================] - 6s 3ms/step - loss: 0.4106 - accuracy: 0.8298 - val_loss: 0.4082 - val_accuracy: 0.8315\n",
            "Epoch 39/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4106 - accuracy: 0.8297 - val_loss: 0.4083 - val_accuracy: 0.8307\n",
            "Epoch 40/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4105 - accuracy: 0.8298 - val_loss: 0.4081 - val_accuracy: 0.8312\n",
            "Epoch 41/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4104 - accuracy: 0.8299 - val_loss: 0.4080 - val_accuracy: 0.8315\n",
            "Epoch 42/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4103 - accuracy: 0.8298 - val_loss: 0.4080 - val_accuracy: 0.8310\n",
            "Epoch 43/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4103 - accuracy: 0.8297 - val_loss: 0.4075 - val_accuracy: 0.8311\n",
            "Epoch 44/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4102 - accuracy: 0.8297 - val_loss: 0.4075 - val_accuracy: 0.8314\n",
            "Epoch 45/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4101 - accuracy: 0.8298 - val_loss: 0.4079 - val_accuracy: 0.8315\n",
            "Epoch 46/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4101 - accuracy: 0.8298 - val_loss: 0.4076 - val_accuracy: 0.8312\n",
            "Epoch 47/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4099 - accuracy: 0.8298 - val_loss: 0.4073 - val_accuracy: 0.8311\n",
            "Epoch 48/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4099 - accuracy: 0.8298 - val_loss: 0.4075 - val_accuracy: 0.8308\n",
            "Epoch 49/50\n",
            "2178/2178 [==============================] - 6s 3ms/step - loss: 0.4099 - accuracy: 0.8297 - val_loss: 0.4075 - val_accuracy: 0.8315\n",
            "Epoch 50/50\n",
            "2178/2178 [==============================] - 7s 3ms/step - loss: 0.4098 - accuracy: 0.8299 - val_loss: 0.4072 - val_accuracy: 0.8307\n",
            "7292/7292 [==============================] - 12s 2ms/step\n",
            "Scratch T-statistic with ground truth: [-57.34564652]\n",
            "Scratch P-value with ground truth: [0.]\n",
            "Iteration 1, loss = 0.41523111\n",
            "Validation score: 0.829293\n",
            "Iteration 2, loss = 0.40651111\n",
            "Validation score: 0.830128\n",
            "Iteration 3, loss = 0.40531375\n",
            "Validation score: 0.829862\n",
            "Iteration 4, loss = 0.40469527\n",
            "Validation score: 0.829807\n",
            "Iteration 5, loss = 0.40425038\n",
            "Validation score: 0.829853\n",
            "Iteration 6, loss = 0.40412947\n",
            "Validation score: 0.830275\n",
            "Iteration 7, loss = 0.40386203\n",
            "Validation score: 0.830119\n",
            "Iteration 8, loss = 0.40367762\n",
            "Validation score: 0.829081\n",
            "Iteration 9, loss = 0.40349845\n",
            "Validation score: 0.830312\n",
            "Iteration 10, loss = 0.40344156\n",
            "Validation score: 0.830753\n",
            "Iteration 11, loss = 0.40329769\n",
            "Validation score: 0.830551\n",
            "Iteration 12, loss = 0.40331912\n",
            "Validation score: 0.830349\n",
            "Iteration 13, loss = 0.40323335\n",
            "Validation score: 0.830275\n",
            "Iteration 14, loss = 0.40312011\n",
            "Validation score: 0.830110\n",
            "Iteration 15, loss = 0.40306140\n",
            "Validation score: 0.830358\n",
            "Iteration 16, loss = 0.40296763\n",
            "Validation score: 0.830018\n",
            "Iteration 17, loss = 0.40283011\n",
            "Validation score: 0.830358\n",
            "Iteration 18, loss = 0.40286200\n",
            "Validation score: 0.830367\n",
            "Iteration 19, loss = 0.40281870\n",
            "Validation score: 0.829715\n",
            "Iteration 20, loss = 0.40284933\n",
            "Validation score: 0.829945\n",
            "Iteration 21, loss = 0.40271538\n",
            "Validation score: 0.830514\n",
            "Validation score did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Scratch T-statistic with ground truth: -63.60629243357312\n",
            "Scratch P-value with ground truth: 0.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import regularizers\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "y_train = np.array(y_train).astype(int)\n",
        "y_test = np.array(y_test).astype(int)\n",
        "\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# From Scratch Model\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(23, kernel_regularizer=regularizers.l2(0.001), activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu'),\n",
        "    tf.keras.layers.Dense(8, kernel_regularizer=regularizers.l2(0.001), activation='relu'),\n",
        "    tf.keras.layers.Dense(4, kernel_regularizer=regularizers.l2(0.001), activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=200, validation_split=0.2)\n",
        "\n",
        "threshold = 0.5\n",
        "\n",
        "y_pred_scratch = model.predict(X_test)\n",
        "y_pred_scratch = np.array(y_pred_scratch) >= threshold\n",
        "\n",
        "t_statistic, p_value = ttest_ind(y_pred_scratch, y_test)\n",
        "\n",
        "print(\"Scratch T-statistic with ground truth:\", t_statistic)\n",
        "print(\"Scratch P-value with ground truth:\", p_value)\n",
        "\n",
        "# From Sklearn Model\n",
        "\n",
        "model_sk = MLPClassifier(hidden_layer_sizes=(16,8,4), tol=0.00001, alpha=0.0002, batch_size='auto', learning_rate='constant', learning_rate_init=0.004, validation_fraction=0.2, verbose = True, early_stopping=True, activation='relu', solver='adam', max_iter=200)\n",
        "model_history = model_sk.fit(X_train,y_train)\n",
        "\n",
        "y_pred_sk = model_sk.predict(X_test)\n",
        "\n",
        "t_statistic_sk, p_value_sk = ttest_ind(y_pred_sk, y_test)\n",
        "\n",
        "print(\"Scratch T-statistic with ground truth:\", t_statistic_sk)\n",
        "print(\"Scratch P-value with ground truth:\", p_value_sk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ET6OtQM8h0sL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Gg9gcaPSlTcs",
        "JOgeTsMRkkrU",
        "TX7Vv673mQOB"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}